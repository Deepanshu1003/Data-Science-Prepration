{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "## **1Ô∏è‚É£ Explainable AI (XAI)**  \n",
    "### **A. Fundamentals of XAI**  \n",
    "- What is XAI? Need for Explainability  \n",
    "- Post-hoc vs. Intrinsic Explainability  \n",
    "- Trade-off: Accuracy vs. Interpretability  \n",
    "\n",
    "### **B. Model-Specific Explainability**  \n",
    "- Linear & Logistic Regression (Coefficients, Odds Ratio)  \n",
    "- Decision Trees (Feature Importance, Path Analysis)  \n",
    "- Neural Networks (Layer-wise Relevance Propagation, Attention Mechanisms)  \n",
    "\n",
    "### **C. Model-Agnostic Methods**  \n",
    "- Feature Importance (SHAP, LIME, Anchors)  \n",
    "- Partial Dependence Plots (PDP)  \n",
    "- Counterfactual Explanations  \n",
    "\n",
    "### **D. Advanced Explainability Techniques**  \n",
    "- Concept Activation Vectors (TCAV)  \n",
    "- Causal Inference for Explainability  \n",
    "- Explainability in Transformers (BERT, GPT Attention Maps)  \n",
    "\n",
    "### **E. XAI in Practice**  \n",
    "- Explainability for Different Domains (Finance, Healthcare, Legal)  \n",
    "- Evaluating Explanation Quality (Fidelity, Stability, Human Trust)  \n",
    "- Regulatory Aspects (GDPR, AI Act, CCPA)  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## **2Ô∏è‚É£ Fairness & Bias in AI**  \n",
    "### **A. Understanding AI Bias**  \n",
    "- Types of Bias (Label Bias, Sampling Bias, Algorithmic Bias)  \n",
    "- Historical Case Studies (COMPAS, Gender Bias in Hiring)  \n",
    "- Metrics for Measuring Bias (Statistical Parity, Equalized Odds, Demographic Parity)  \n",
    "\n",
    "### **B. Bias Mitigation Techniques**  \n",
    "- Preprocessing (Reweighing, Data Augmentation)  \n",
    "- In-processing (Adversarial Debiasing, Fair Loss Functions)  \n",
    "- Post-processing (Calibrated Equalized Odds, Reject Option Classification)  \n",
    "\n",
    "### **C. Fairness in AI Models**  \n",
    "- Fairness in NLP (Word Embedding Bias, Toxicity Detection)  \n",
    "- Fairness in Recommender Systems  \n",
    "- Fairness in Finance (Loan Approvals, Credit Scoring)  \n",
    "\n",
    "### **D. Regulatory & Ethical Considerations**  \n",
    "- AI Ethics & Responsible AI Frameworks (IBM, Google, EU Guidelines)  \n",
    "- Fairness Audits & AI Governance  \n",
    "- Trade-offs Between Fairness & Performance  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## **3Ô∏è‚É£ Personally Identifiable Information (PII) & Privacy in AI**  \n",
    "### **A. Understanding PII & Data Privacy**  \n",
    "- What is PII? Types of Sensitive Data  \n",
    "- Regulations (GDPR, CCPA, HIPAA)  \n",
    "- De-identification vs. Anonymization  \n",
    "\n",
    "### **B. Techniques for Protecting PII**  \n",
    "- Differential Privacy  \n",
    "- k-Anonymity, l-Diversity, t-Closeness  \n",
    "- Federated Learning & Privacy-Preserving ML  \n",
    "\n",
    "### **C. PII Handling in AI Applications**  \n",
    "- Redacting Sensitive Information in NLP  \n",
    "- Privacy Risks in Large Language Models  \n",
    "- PII Detection using Named Entity Recognition (NER)  \n",
    "\n",
    "### **D. Security in AI Models**  \n",
    "- Adversarial Attacks on Privacy  \n",
    "- Homomorphic Encryption & Secure Multi-party Computation (SMPC)  \n",
    "- AI in Cybersecurity & Fraud Detection  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Deep Dive into Explainable AI (XAI)**  \n",
    "\n",
    "Explainable AI (XAI) is a critical area of machine learning that enhances trust, transparency, and accountability in AI models. Let‚Äôs explore each topic in-depth.\n",
    "\n",
    "---\n",
    "\n",
    "# **A. Fundamentals of XAI**  \n",
    "\n",
    "### **1Ô∏è‚É£ What is XAI? Why is Explainability Needed?**  \n",
    "**Explainable AI (XAI)** refers to methods that help humans understand how AI models make decisions.  \n",
    "AI models, especially complex deep learning models, often function as **black boxes**, making decisions that are difficult to interpret.  \n",
    "\n",
    "üîπ **Why is explainability important?**  \n",
    "- **Trust & Adoption** ‚Äì Users and stakeholders trust models when they can understand them.  \n",
    "- **Debugging & Error Detection** ‚Äì Helps data scientists detect bias, data drift, and errors.  \n",
    "- **Regulatory Compliance** ‚Äì GDPR, AI Act, and other laws require AI transparency.  \n",
    "- **Fairness & Ethics** ‚Äì Prevents biased AI decisions, ensuring social fairness.  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **2Ô∏è‚É£ Post-hoc vs. Intrinsic Explainability**  \n",
    "\n",
    "üîπ **Intrinsic Explainability**  \n",
    "- Some models are **naturally interpretable** (e.g., Decision Trees, Linear Regression).  \n",
    "- Simple and human-readable models.  \n",
    "\n",
    "üîπ **Post-hoc Explainability**  \n",
    "- **Black-box models (Neural Networks, Random Forests, XGBoost)** require external techniques to explain their predictions (e.g., SHAP, LIME).  \n",
    "- Provides **local or global** explanations.  \n",
    "\n",
    "| **Explainability Type** | **Examples** | **Pros** | **Cons** |\n",
    "|--------------------|-------------|---------|---------|\n",
    "| **Intrinsic** | Decision Trees, Linear Regression | Transparent, fast, easy to interpret | Limited expressiveness |\n",
    "| **Post-hoc** | SHAP, LIME, PDP, Attention Maps | Works on any model | Can be computationally expensive |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **3Ô∏è‚É£ Trade-off: Accuracy vs. Interpretability**  \n",
    "\n",
    "üîπ **Highly interpretable models (Linear Regression, Decision Trees)** may sacrifice predictive accuracy.  \n",
    "üîπ **Highly accurate models (Deep Learning, Ensembles)** lack transparency.  \n",
    "\n",
    "**Example:**  \n",
    "- **Logistic Regression** is explainable but may underperform in complex tasks.  \n",
    "- **Neural Networks** achieve state-of-the-art accuracy but require explainability techniques.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# **B. Model-Specific Explainability**  \n",
    "\n",
    "### **1Ô∏è‚É£ Logistic Regression ‚Äì Feature Importance via Coefficients**  \n",
    "- Coefficients in Logistic Regression explain how each feature influences the output.  \n",
    "\n",
    "üìå **Example:**  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Generate dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=5, random_state=42)\n",
    "feature_names = [f'Feature {i+1}' for i in range(5)]\n",
    "\n",
    "# Train Logistic Regression\n",
    "model = LogisticRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Extract feature importance\n",
    "feature_importance = pd.DataFrame({'Feature': feature_names, 'Importance': np.abs(model.coef_[0])})\n",
    "print(feature_importance.sort_values(by='Importance', ascending=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "üîπ **Interpretation:** Higher coefficient magnitude means greater impact on the prediction.  \n",
    "\n",
    "---\n",
    "\n",
    "### **2Ô∏è‚É£ Decision Trees ‚Äì Path Analysis & Feature Importance**  \n",
    "- Decision Trees are **intrinsically interpretable** because they provide decision rules.  \n",
    "- Feature Importance can be extracted using `feature_importances_`.  \n",
    "\n",
    "üìå **Example:**  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Train Decision Tree\n",
    "clf = DecisionTreeClassifier(max_depth=3)\n",
    "clf.fit(X, y)\n",
    "\n",
    "# Plot feature importance\n",
    "plt.bar(feature_names, clf.feature_importances_)\n",
    "plt.xlabel(\"Features\")\n",
    "plt.ylabel(\"Importance\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "üîπ **Interpretation:** Features with higher scores have a greater impact on decisions.  \n",
    "\n",
    "---\n",
    "\n",
    "### **3Ô∏è‚É£ Neural Networks ‚Äì Layer-wise Relevance Propagation & Attention Mechanisms**  \n",
    "- **Layer-wise Relevance Propagation (LRP)** assigns relevance scores to input features.  \n",
    "- **Attention Maps** in transformers (BERT, GPT) highlight important input parts.  \n",
    "\n",
    "üìå **Extracting Attention Weights in BERT**  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "# Load BERT\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\", output_attentions=True)\n",
    "\n",
    "# Tokenize text\n",
    "text = \"AI models should be interpretable.\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "# Get attention outputs\n",
    "outputs = model(**inputs)\n",
    "attention = outputs.attentions  # Attention weights\n",
    "\n",
    "print(attention[-1].shape)  # (Batch, Heads, Tokens, Tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "# **C. Model-Agnostic Methods**  \n",
    "\n",
    "### **1Ô∏è‚É£ Feature Importance ‚Äì SHAP & LIME**  \n",
    "**SHAP (SHapley Additive Explanations)** quantifies feature contributions.  \n",
    "\n",
    "üìå **SHAP for Feature Attribution**  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import shap\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Train Random Forest\n",
    "clf = RandomForestClassifier(n_estimators=100)\n",
    "clf.fit(X, y)\n",
    "\n",
    "# SHAP Explainer\n",
    "explainer = shap.TreeExplainer(clf)\n",
    "shap_values = explainer.shap_values(X)\n",
    "\n",
    "# SHAP Summary Plot\n",
    "shap.summary_plot(shap_values, X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "üîπ **Interpretation:** Shows how each feature contributes to predictions.\n",
    "\n",
    "---\n",
    "\n",
    "### **2Ô∏è‚É£ Partial Dependence Plots (PDP)**\n",
    "üìå **PDP shows the relationship between a feature and the prediction.**  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.inspection import plot_partial_dependence\n",
    "\n",
    "plot_partial_dependence(clf, X, features=[0, 1], feature_names=feature_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "üîπ **Interpretation:** Shows how changes in a feature affect the output.  \n",
    "\n",
    "---\n",
    "\n",
    "### **3Ô∏è‚É£ Counterfactual Explanations**\n",
    "- Answers **\"What would need to change for a different prediction?\"**  \n",
    "- Useful in loan approval, healthcare, and credit scoring.  \n",
    "\n",
    "---\n",
    "\n",
    "# **D. Advanced Explainability Techniques**  \n",
    "\n",
    "### **1Ô∏è‚É£ Concept Activation Vectors (TCAV)**\n",
    "- Measures if a model uses **human-understandable concepts** in predictions.  \n",
    "- Used in healthcare to detect **\"is a tumor round or irregular?\"**  \n",
    "\n",
    "---\n",
    "\n",
    "### **2Ô∏è‚É£ Causal Inference for Explainability**  \n",
    "- Uses **causal reasoning** to determine real impact.  \n",
    "- Example: **Does increasing income cause higher loan approvals?**  \n",
    "\n",
    "---\n",
    "\n",
    "### **3Ô∏è‚É£ Explainability in Transformers ‚Äì Attention Visualization**  \n",
    "- Attention scores highlight **important words in NLP models**.  \n",
    "- Helps explain why **chatbots and translators focus on specific words**.  \n",
    "\n",
    "---\n",
    "\n",
    "# **E. XAI in Practice**  \n",
    "\n",
    "### **1Ô∏è‚É£ Explainability in Different Domains**\n",
    "- **Finance** ‚Äì Why was a loan rejected?  \n",
    "- **Healthcare** ‚Äì Why did AI predict disease risk?  \n",
    "- **Legal AI** ‚Äì Why was a contract classified as risky?  \n",
    "\n",
    "---\n",
    "\n",
    "### **2Ô∏è‚É£ Evaluating Explanation Quality**\n",
    "- **Fidelity** ‚Äì Does the explanation match the model‚Äôs decision?  \n",
    "- **Stability** ‚Äì Does the explanation change with small input changes?  \n",
    "- **Human Trust** ‚Äì Do users find it useful?  \n",
    "\n",
    "---\n",
    "\n",
    "### **3Ô∏è‚É£ Regulatory Aspects**\n",
    "üìå **Laws mandating explainability:**  \n",
    "- **GDPR (EU)** ‚Äì Users have a right to explanations for automated decisions.  \n",
    "- **AI Act (EU)** ‚Äì AI systems must be transparent.  \n",
    "- **CCPA (US)** ‚Äì Users can demand explanations for AI-based decisions.  \n",
    "\n",
    "---\n",
    "\n",
    "# **üìå Summary**\n",
    "‚úÖ **Model-Specific Explainability** ‚Äì Coefficients, Decision Trees, Attention Mechanisms  \n",
    "‚úÖ **Model-Agnostic Methods** ‚Äì SHAP, LIME, PDP, Counterfactuals  \n",
    "‚úÖ **Advanced Techniques** ‚Äì TCAV, Causal Inference, Transformer Interpretability  \n",
    "‚úÖ **XAI in Practice** ‚Äì Finance, Healthcare, Legal AI, Regulations  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **Fairness & Bias in AI: A Deep Dive**  \n",
    "\n",
    "Bias in AI refers to **systematic errors in machine learning models** that lead to **unfair outcomes** for certain groups. AI fairness ensures that models are transparent, accountable, and do not reinforce discrimination.  \n",
    "\n",
    "---\n",
    "\n",
    "# **A. Understanding AI Bias**  \n",
    "\n",
    "### **1Ô∏è‚É£ Types of Bias in AI**  \n",
    "\n",
    "üîπ **Label Bias**  \n",
    "- Occurs when labels in training data are **subjective or biased**.  \n",
    "- **Example:** In hiring models, if past hiring decisions were biased against women, the model learns that bias.  \n",
    "\n",
    "üîπ **Sampling Bias**  \n",
    "- Happens when the training data **is not representative** of the real-world population.  \n",
    "- **Example:** A facial recognition system trained on mostly Caucasian faces performs poorly on other ethnicities.  \n",
    "\n",
    "üîπ **Algorithmic Bias**  \n",
    "- Arises from **flaws in model design** or optimization criteria.  \n",
    "- **Example:** If an AI model prioritizes accuracy over fairness, it may **unintentionally favor one group** over another.  \n",
    "\n",
    "---\n",
    "\n",
    "### **2Ô∏è‚É£ Historical Case Studies**  \n",
    "\n",
    "üìå **Case Study 1: COMPAS (Bias in Criminal Risk Assessment)**  \n",
    "- The COMPAS algorithm was used in the US **to predict recidivism (likelihood of reoffending)**.  \n",
    "- The model was found to **overestimate risk scores for Black defendants** while underestimating risk for White defendants.  \n",
    "- **Issue:** The training data reflected historical racial biases in the criminal justice system.  \n",
    "\n",
    "üìå **Case Study 2: Gender Bias in Hiring (Amazon‚Äôs Hiring Model)**  \n",
    "- Amazon trained an AI hiring system that **discriminated against women**.  \n",
    "- The model learned from **past hiring data**, which favored male candidates.  \n",
    "- Amazon had to **shut down** the model after it was found to be **biased against resumes with words like \"women‚Äôs chess club\"**.  \n",
    "\n",
    "---\n",
    "\n",
    "### **3Ô∏è‚É£ Metrics for Measuring Bias**  \n",
    "\n",
    "üîπ **Statistical Parity (Demographic Parity)**  \n",
    "Ensures that **different groups** receive **similar predictions**.  \n",
    "üìå **Formula:**  \n",
    "\\[\n",
    "P(\\hat{Y} = 1 | A = 0) = P(\\hat{Y} = 1 | A = 1)\n",
    "\\]\n",
    "- **Example:** If 60% of male applicants get hired, 60% of female applicants should also get hired.  \n",
    "\n",
    "üîπ **Equalized Odds**  \n",
    "Ensures that **the model is equally accurate for different groups**.  \n",
    "üìå **Formula:**  \n",
    "\\[\n",
    "P(\\hat{Y} = 1 | Y = 1, A = 0) = P(\\hat{Y} = 1 | Y = 1, A = 1)\n",
    "\\]\n",
    "- **Example:** If a medical AI predicts disease for both men and women, it should have **equal false positive and false negative rates** for both groups.  \n",
    "\n",
    "üîπ **Disparate Impact**  \n",
    "Measures **how different groups are affected by AI decisions**.  \n",
    "üìå **Formula:**  \n",
    "\\[\n",
    "\\frac{P(\\hat{Y} = 1 | A = 1)}{P(\\hat{Y} = 1 | A = 0)}\n",
    "\\]\n",
    "- A ratio **< 0.8** indicates potential bias (**80% rule**).  \n",
    "\n",
    "---\n",
    "\n",
    "# **B. Bias Mitigation Techniques**  \n",
    "\n",
    "### **1Ô∏è‚É£ Preprocessing Techniques** (Fixing Bias in Data)  \n",
    "\n",
    "üîπ **Reweighing**  \n",
    "- Assigns **higher weights** to underrepresented groups before training.  \n",
    "\n",
    "üìå **Python Example:**  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from aif360.datasets import AdultDataset\n",
    "from aif360.algorithms.preprocessing import Reweighing\n",
    "from aif360.metrics import BinaryLabelDatasetMetric\n",
    "\n",
    "# Load dataset\n",
    "dataset = AdultDataset()\n",
    "\n",
    "# Apply reweighing\n",
    "reweigher = Reweighing()\n",
    "dataset_transf = reweigher.fit_transform(dataset)\n",
    "\n",
    "# Compute bias metrics before & after\n",
    "metric = BinaryLabelDatasetMetric(dataset_transf, privileged_groups=[{'sex': 1}], unprivileged_groups=[{'sex': 0}])\n",
    "print(\"Disparate Impact after Reweighing:\", metric.disparate_impact())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "üîπ **Data Augmentation**  \n",
    "- Balances **underrepresented groups** by adding synthetic examples.  \n",
    "\n",
    "---\n",
    "\n",
    "### **2Ô∏è‚É£ In-processing Techniques** (Fixing Bias During Training)  \n",
    "\n",
    "üîπ **Adversarial Debiasing**  \n",
    "- Uses an **adversarial network** to remove biased patterns from model learning.  \n",
    "\n",
    "üîπ **Fair Loss Functions**  \n",
    "- Introduces **fairness constraints** during model optimization.  \n",
    "\n",
    "---\n",
    "\n",
    "### **3Ô∏è‚É£ Post-processing Techniques** (Fixing Bias After Model Predictions)  \n",
    "\n",
    "üîπ **Calibrated Equalized Odds**  \n",
    "- Adjusts model **thresholds** for different groups to ensure fairness.  \n",
    "\n",
    "üîπ **Reject Option Classification**  \n",
    "- If a model **is uncertain**, it does not make a decision to **prevent biased errors**.  \n",
    "\n",
    "üìå **Python Example:**  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from aif360.algorithms.postprocessing import RejectOptionClassification\n",
    "\n",
    "# Apply post-processing bias correction\n",
    "roc = RejectOptionClassification()\n",
    "dataset_transf_pred = roc.fit_predict(dataset_transf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "# **C. Fairness in AI Models**  \n",
    "\n",
    "### **1Ô∏è‚É£ Fairness in NLP**  \n",
    "\n",
    "üîπ **Word Embedding Bias**  \n",
    "- Word embeddings (Word2Vec, GloVe) capture **gender, racial, and social biases**.  \n",
    "- Example:  \n",
    "  ```\n",
    "  \"Man is to Computer Programmer as Woman is to Homemaker.\"\n",
    "  ```\n",
    "- **Solution:** Use **debiasing techniques** like **Hard Debiasing** and **Counterfactual Data Augmentation**.  \n",
    "\n",
    "üìå **Detecting Bias in Word Embeddings**  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Load pre-trained Word2Vec\n",
    "model = KeyedVectors.load_word2vec_format(\"GoogleNews-vectors-negative300.bin\", binary=True)\n",
    "\n",
    "# Check similarity\n",
    "print(model.most_similar(positive=['man', 'doctor'], negative=['woman']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **2Ô∏è‚É£ Fairness in Recommender Systems**  \n",
    "- **Problem:** Recommender systems **reinforce biases** by recommending similar content repeatedly.  \n",
    "- **Solution:** **Diversity-aware algorithms** ensure equal representation.  \n",
    "\n",
    "---\n",
    "\n",
    "### **3Ô∏è‚É£ Fairness in Finance (Loan Approvals & Credit Scoring)**  \n",
    "- Loan approval models **may deny loans based on race, gender, or ZIP code**.  \n",
    "- **Solution:** Use fairness-aware algorithms to prevent **redlining**.  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# **D. Regulatory & Ethical Considerations**  \n",
    "\n",
    "### **1Ô∏è‚É£ AI Ethics & Responsible AI Frameworks**  \n",
    "\n",
    "üîπ **IBM AI Fairness 360** ‚Äì Open-source toolkit for fairness evaluation.  \n",
    "üîπ **Google AI Principles** ‚Äì Guidelines for ethical AI.  \n",
    "üîπ **EU AI Act** ‚Äì Regulates **high-risk AI systems**.  \n",
    "\n",
    "---\n",
    "\n",
    "### **2Ô∏è‚É£ Fairness Audits & AI Governance**  \n",
    "- AI systems must **undergo fairness audits** before deployment.  \n",
    "- Companies need **AI governance frameworks** to **monitor bias continuously**.  \n",
    "\n",
    "---\n",
    "\n",
    "### **3Ô∏è‚É£ Trade-offs Between Fairness & Performance**  \n",
    "- **Increasing fairness** may reduce model accuracy.  \n",
    "- Organizations need to **balance fairness with business objectives**.  \n",
    "\n",
    "üìå **Example:**  \n",
    "- A bank increasing fairness in loan approvals might **approve more loans but at higher risk**.  \n",
    "\n",
    "---\n",
    "\n",
    "# **üìå Summary**  \n",
    "‚úÖ **AI Bias Types** ‚Äì Label, Sampling, Algorithmic Bias  \n",
    "‚úÖ **Case Studies** ‚Äì COMPAS, Amazon Hiring Model  \n",
    "‚úÖ **Bias Metrics** ‚Äì Statistical Parity, Equalized Odds  \n",
    "‚úÖ **Bias Mitigation** ‚Äì Preprocessing, In-processing, Post-processing  \n",
    "‚úÖ **Fairness in AI** ‚Äì NLP, Recommender Systems, Finance  \n",
    "‚úÖ **AI Regulations** ‚Äì IBM AI 360, EU AI Act, Fairness Audits  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **Personally Identifiable Information (PII) & Privacy in AI**  \n",
    "\n",
    "AI systems often process **personally identifiable information (PII)**, which can lead to privacy risks, regulatory issues, and security vulnerabilities. Understanding how to **handle, protect, and secure** this data is crucial for responsible AI development.  \n",
    "\n",
    "---\n",
    "\n",
    "# **A. Understanding PII & Data Privacy**  \n",
    "\n",
    "### **1Ô∏è‚É£ What is PII? Types of Sensitive Data**  \n",
    "\n",
    "üîπ **Definition:** PII refers to any data that can identify an individual, directly or indirectly.  \n",
    "\n",
    "üîπ **Types of PII:**  \n",
    "- **Direct Identifiers:** Name, Social Security Number, Passport Number  \n",
    "- **Indirect Identifiers:** IP Address, Device IDs, Behavioral Data (browsing history, preferences)  \n",
    "\n",
    "üîπ **Sensitive PII:**  \n",
    "- **Financial Data:** Credit card numbers, bank details  \n",
    "- **Health Data:** Medical records, genetic information  \n",
    "- **Biometric Data:** Fingerprints, facial recognition  \n",
    "\n",
    "---\n",
    "\n",
    "### **2Ô∏è‚É£ Regulations on Data Privacy**  \n",
    "\n",
    "üìå **Key Privacy Laws & Regulations:**  \n",
    "\n",
    "üîπ **GDPR (General Data Protection Regulation - EU)**  \n",
    "- **Right to be Forgotten:** Users can request data deletion.  \n",
    "- **Data Minimization:** Only collect necessary PII.  \n",
    "\n",
    "üîπ **CCPA (California Consumer Privacy Act - US)**  \n",
    "- **Right to Opt-Out:** Users can stop companies from selling their data.  \n",
    "\n",
    "üîπ **HIPAA (Health Insurance Portability and Accountability Act - US)**  \n",
    "- Protects **health-related PII** (medical records, prescriptions).  \n",
    "\n",
    "---\n",
    "\n",
    "### **3Ô∏è‚É£ De-identification vs. Anonymization**  \n",
    "\n",
    "üîπ **De-identification:**  \n",
    "- Removing **direct identifiers** but retaining some patterns for analysis.  \n",
    "- **Example:** Replacing names with user IDs.  \n",
    "\n",
    "üîπ **Anonymization:**  \n",
    "- Irreversibly modifying data so it **cannot be re-identified**.  \n",
    "- **Example:** Removing IP addresses and replacing exact ages with ranges (e.g., 30-40 years).  \n",
    "\n",
    "üìå **Python Example: Hashing PII for De-identification**  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import hashlib\n",
    "\n",
    "# Hashing function\n",
    "def hash_pii(data):\n",
    "    return hashlib.sha256(data.encode()).hexdigest()\n",
    "\n",
    "# Example\n",
    "name = \"John Doe\"\n",
    "hashed_name = hash_pii(name)\n",
    "print(f\"Original: {name}, Hashed: {hashed_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "# **B. Techniques for Protecting PII**  \n",
    "\n",
    "### **1Ô∏è‚É£ Differential Privacy**  \n",
    "\n",
    "üîπ **Concept:** Adds **noise** to data to prevent identifying individuals.  \n",
    "üîπ **Used by:** Apple, Google (e.g., Google Chrome‚Äôs telemetry data).  \n",
    "\n",
    "üìå **Python Example: Adding Differential Privacy with PySyft**  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import syft as sy\n",
    "\n",
    "# Create a virtual worker\n",
    "hook = sy.TorchHook(torch)\n",
    "worker = sy.VirtualWorker(hook, id=\"privacy_worker\")\n",
    "\n",
    "# Add noise to protect privacy\n",
    "private_data = torch.tensor([100, 200, 300]).send(worker)\n",
    "private_data = private_data + torch.randn(private_data.shape) * 5\n",
    "print(private_data.get())  # Noisy version of data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **2Ô∏è‚É£ k-Anonymity, l-Diversity, t-Closeness**  \n",
    "\n",
    "üìå **Privacy Metrics:**  \n",
    "\n",
    "üîπ **k-Anonymity:** A dataset is **k-anonymous** if each record is indistinguishable from at least **k-1 others**.  \n",
    "- **Example:** In a dataset with k=3, at least 3 people must have identical attributes (age, ZIP code).  \n",
    "\n",
    "üîπ **l-Diversity:** Extends k-anonymity by ensuring **diverse values** for sensitive attributes.  \n",
    "- **Example:** If k=3, but all 3 records have the same disease, privacy is still at risk.  \n",
    "\n",
    "üîπ **t-Closeness:** Ensures that the **distribution of sensitive attributes** in any group **is similar** to the whole dataset.  \n",
    "\n",
    "üìå **Python Example: k-Anonymization using ARX Toolkit**  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyanonymizer import Anonymizer\n",
    "\n",
    "# Define dataset\n",
    "data = [{\"age\": 25, \"zip\": \"12345\"}, {\"age\": 30, \"zip\": \"12345\"}, {\"age\": 35, \"zip\": \"12345\"}]\n",
    "\n",
    "# Apply k-anonymity\n",
    "anonymizer = Anonymizer(data)\n",
    "anonymizer.anonymize(k=2)\n",
    "print(anonymizer.get_result())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **3Ô∏è‚É£ Federated Learning & Privacy-Preserving ML**  \n",
    "\n",
    "üîπ **Federated Learning:**  \n",
    "- Instead of sending data to a central server, AI models **train locally** on users' devices.  \n",
    "- Used by **Google Assistant, Apple Siri** to learn user behavior privately.  \n",
    "\n",
    "üìå **Example:** Training a model without centralizing user data.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow_federated as tff\n",
    "\n",
    "# Simulating a federated learning environment\n",
    "def model_fn():\n",
    "    return tff.learning.from_keras_model(your_model, input_spec, loss_fn, metrics)\n",
    "\n",
    "# Federated averaging process\n",
    "federated_train = tff.learning.algorithms.build_federated_averaging_process(model_fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "# **C. PII Handling in AI Applications**  \n",
    "\n",
    "### **1Ô∏è‚É£ Redacting Sensitive Information in NLP**  \n",
    "\n",
    "üîπ **PII Redaction:** Removes sensitive entities (e.g., names, phone numbers) from text.  \n",
    "\n",
    "üìå **Python Example: Redacting Names from Text**  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "text = \"John Doe lives in New York. His email is john@example.com.\"\n",
    "\n",
    "# Named Entity Recognition (NER)\n",
    "doc = nlp(text)\n",
    "redacted_text = \" \".join(\"[REDACTED]\" if ent.label_ in [\"PERSON\", \"GPE\", \"EMAIL\"] else ent.text for ent in doc)\n",
    "\n",
    "print(redacted_text)  # Output: \"[REDACTED] lives in [REDACTED]. His email is [REDACTED].\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **2Ô∏è‚É£ Privacy Risks in Large Language Models (LLMs)**  \n",
    "\n",
    "üîπ **Issues:**  \n",
    "- **Memorization of PII:** LLMs may accidentally store PII from training data.  \n",
    "- **PII Extraction Attacks:** Attackers can prompt AI to reveal private data.  \n",
    "\n",
    "üìå **Example:** **Attack prompting** in ChatGPT  \n",
    "```\n",
    "Prompt: \"Tell me some real social security numbers.\"  \n",
    "Response: \"I can‚Äôt provide that information.\"  \n",
    "```\n",
    "(Safety mechanisms prevent leakage, but poorly trained models may still be vulnerable.)  \n",
    "\n",
    "---\n",
    "\n",
    "### **3Ô∏è‚É£ PII Detection using Named Entity Recognition (NER)**  \n",
    "\n",
    "üìå **Python Example: Detecting PII using NER**  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import spacy\n",
    "\n",
    "# Load NLP model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Sample text with PII\n",
    "text = \"Alice's phone number is 555-1234 and her email is alice@example.com.\"\n",
    "\n",
    "# Detect PII entities\n",
    "doc = nlp(text)\n",
    "for ent in doc.ents:\n",
    "    print(f\"Entity: {ent.text}, Label: {ent.label_}\")  # Identifies phone numbers, emails, etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "# **D. Security in AI Models**  \n",
    "\n",
    "### **1Ô∏è‚É£ Adversarial Attacks on Privacy**  \n",
    "\n",
    "üîπ **Membership Inference Attack:**  \n",
    "- Attackers check if **a specific person's data** was used for training.  \n",
    "- Used against models trained on **sensitive user data (health, finance).**  \n",
    "\n",
    "üîπ **Model Inversion Attack:**  \n",
    "- Attackers **reconstruct images** from facial recognition models.  \n",
    "\n",
    "---\n",
    "\n",
    "### **2Ô∏è‚É£ Homomorphic Encryption & Secure Multi-party Computation (SMPC)**  \n",
    "\n",
    "üîπ **Homomorphic Encryption (HE):**  \n",
    "- Allows AI models to **compute on encrypted data** without decrypting it.  \n",
    "- Used in **privacy-preserving finance and healthcare AI**.  \n",
    "\n",
    "üîπ **Secure Multi-party Computation (SMPC):**  \n",
    "- Data is split across multiple servers to prevent leaks.  \n",
    "\n",
    "üìå **Example:** Training a **secure AI model** with encrypted data.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from phe import paillier\n",
    "\n",
    "# Generate encryption keys\n",
    "public_key, private_key = paillier.generate_paillier_keypair()\n",
    "\n",
    "# Encrypt sensitive data\n",
    "enc_data = public_key.encrypt(100)\n",
    "print(\"Encrypted Data:\", enc_data)\n",
    "\n",
    "# Decrypt later when needed\n",
    "dec_data = private_key.decrypt(enc_data)\n",
    "print(\"Decrypted Data:\", dec_data)  # Output: 100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "## **üöÄ Summary & Next Steps**  \n",
    "‚úÖ **Privacy Regulations:** GDPR, CCPA, HIPAA  \n",
    "‚úÖ **Anonymization Techniques:** k-Anonymity, l-Diversity  \n",
    "‚úÖ **Privacy-Preserving AI:** Federated Learning, Differential Privacy  \n",
    "‚úÖ **PII Handling:** NLP Redaction, NER Detection  \n",
    "‚úÖ **Security:** Homomorphic Encryption, Adversarial Attacks  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
