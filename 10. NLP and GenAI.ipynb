{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìå **NLP & Generative AI - Topics & Subtopics**  \n",
    "\n",
    "## üîπ **1. Introduction to NLP & GenAI**  \n",
    "- What is NLP? Applications & Use Cases  \n",
    "- Traditional NLP vs. Deep Learning-based NLP  \n",
    "- Overview of GenAI in NLP (Chatbots, Summarization, Content Generation)  \n",
    "\n",
    "---\n",
    "\n",
    "## üîπ **2. Text Preprocessing Techniques**  \n",
    "### **üëâ Basic Preprocessing**\n",
    "- Tokenization (Word, Sentence)  \n",
    "- Stopword Removal  \n",
    "- Stemming & Lemmatization  \n",
    "- Text Normalization (Lowercasing, Punctuation Removal)  \n",
    "\n",
    "### **üëâ Feature Extraction**\n",
    "- **TF-IDF (Term Frequency - Inverse Document Frequency)**  \n",
    "- **Bag of Words (BoW)**  \n",
    "- **Word Embeddings (Word2Vec, GloVe, FastText)**  \n",
    "\n",
    "---\n",
    "\n",
    "## üîπ **3. Deep Learning for NLP**  \n",
    "### **üëâ Recurrent Neural Networks (RNNs) & LSTMs**\n",
    "- Why RNNs for NLP?  \n",
    "- Long Short-Term Memory (LSTM) & Gated Recurrent Units (GRUs)  \n",
    "\n",
    "### **üëâ Attention Mechanism & Transformers**\n",
    "- Problems with RNNs (Long-Term Dependencies)  \n",
    "- Self-Attention & Multi-Head Attention  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## üîπ **4. Transformer Models (BERT, GPT, T5, etc.)**  \n",
    "### **üëâ BERT (Bidirectional Encoder Representations from Transformers)**\n",
    "- Understanding BERT‚Äôs architecture  \n",
    "- Pretraining & Fine-tuning BERT for NLP Tasks (Classification, QA, Summarization)  \n",
    "\n",
    "### **üëâ GPT (Generative Pretrained Transformer)**\n",
    "- GPT-2, GPT-3, GPT-4 Overview  \n",
    "- How GPT generates text  \n",
    "- Fine-Tuning GPT for Specific Applications  \n",
    "\n",
    "### **üëâ Other Transformer Models**\n",
    "- T5 (Text-to-Text Transfer Transformer)  \n",
    "- DistilBERT (Efficient BERT)  \n",
    "- LLaMA, Falcon, and Open Source Models  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## üîπ **5. Advanced NLP & GenAI Applications**  \n",
    "### **üëâ NLP in Action**\n",
    "- Named Entity Recognition (NER)  \n",
    "- Sentiment Analysis  \n",
    "- Question Answering  \n",
    "- Summarization  \n",
    "\n",
    "### **üëâ GenAI for Text Generation**\n",
    "- Chatbots & Conversational AI  \n",
    "- AI-powered Content Creation  \n",
    "- Code Generation (Codex, StarCoder)  \n",
    "\n",
    "---\n",
    "\n",
    "## üîπ **6. Fine-Tuning & Deployment**  \n",
    "- Fine-Tuning Transformers with Hugging Face  \n",
    "- Model Deployment (Streamlit, FastAPI, LangChain)  \n",
    "- Optimizing LLMs for Efficiency  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìå **Introduction to NLP & Generative AI**  \n",
    "\n",
    "## üîπ **What is NLP?**  \n",
    "**Natural Language Processing (NLP)** is a field of artificial intelligence (AI) that enables machines to understand, interpret, and generate human language.  \n",
    "\n",
    "### **üí° Key Tasks in NLP**  \n",
    "‚úî Text Processing (Tokenization, Lemmatization)  \n",
    "‚úî Text Classification (Spam Detection, Sentiment Analysis)  \n",
    "‚úî Machine Translation (Google Translate)  \n",
    "‚úî Speech Recognition (Siri, Google Assistant)  \n",
    "‚úî Named Entity Recognition (NER)  \n",
    "\n",
    "### **üîç Example: Text Processing in Python**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.data.path.append('/Users/deepanshu/nltk_data')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Tokens: ['Natural', 'Language', 'Processing', 'enables', 'machines', 'to', 'understand', 'human', 'language', '!']\n",
      "Filtered Tokens: ['Natural', 'Language', 'Processing', 'enables', 'machines', 'understand', 'human', 'language', '!']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "text = \"Natural Language Processing enables machines to understand human language!\"\n",
    "\n",
    "# Tokenization\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Removing Stopwords\n",
    "filtered_tokens = [word for word in tokens if word.lower() not in stopwords.words('english')]\n",
    "\n",
    "print(\"Original Tokens:\", tokens)\n",
    "print(\"Filtered Tokens:\", filtered_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## üîπ **Applications & Use Cases of NLP**  \n",
    "### **üìå Text Classification**  \n",
    "- Sentiment Analysis (Positive/Negative Reviews)  \n",
    "- Spam Detection (Email Filters)  \n",
    "\n",
    "**Example: Sentiment Analysis using NLTK**  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment Score: 0.6125\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "text = \"I love using ChatGPT! It's amazing.\"\n",
    "sentiment = TextBlob(text).sentiment.polarity  # -1 (Negative) to +1 (Positive)\n",
    "\n",
    "print(\"Sentiment Score:\", sentiment)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### **üìå Machine Translation**\n",
    "- Google Translate  \n",
    "- DeepL Translator  \n",
    "\n",
    "**Example: Translate English to French using Google Translator API**  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated Text: Bonjour comment allez-vous?\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from deep_translator import GoogleTranslator\n",
    "\n",
    "translator = GoogleTranslator(source=\"en\", target=\"fr\")\n",
    "translated_text = translator.translate(\"Hello, how are you?\")\n",
    "print(\"Translated Text:\", translated_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### **üìå Named Entity Recognition (NER)**\n",
    "- Identifying proper names (Person, Location, Organization)  \n",
    "- Used in search engines, chatbots, and legal document analysis  \n",
    "\n",
    "**Example: Named Entity Recognition using SpaCy**  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading model...\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.8.0\n",
      "\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;3m‚ö† Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.cli import download\n",
    "\n",
    "# Manually download and load the model\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "except OSError:\n",
    "    print(\"Downloading model...\")\n",
    "    download(\"en_core_web_sm\")\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple -> ORG\n",
      "U.K. -> GPE\n",
      "$1 billion -> MONEY\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion.\")\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(f\"{ent.text} -> {ent.label_}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ **Traditional NLP vs. Deep Learning-based NLP**  \n",
    "\n",
    "| **Aspect**          | **Traditional NLP**               | **Deep Learning-based NLP** |\n",
    "|---------------------|---------------------------------|-----------------------------|\n",
    "| **Approach**       | Rule-based, Statistical Models | Neural Networks (LSTMs, Transformers) |\n",
    "| **Feature Engineering** | Manual (TF-IDF, BoW) | Automatic (Embeddings) |\n",
    "| **Performance**     | Decent for structured text | Superior for complex tasks |\n",
    "| **Examples**       | NLTK, SpaCy | BERT, GPT, T5 |\n",
    "\n",
    "### **üí° Example: Traditional vs. Deep Learning for Text Classification**  \n",
    "**1Ô∏è‚É£ Traditional (TF-IDF + Naive Bayes)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: [1]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "corpus = [\"This is a great movie\", \"This movie is bad\", \"I love this film\"]\n",
    "labels = [1, 0, 1]  # 1: Positive, 0: Negative\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(X, labels)\n",
    "\n",
    "test_text = [\"I hate this movie\"]\n",
    "X_test = vectorizer.transform(test_text)\n",
    "prediction = model.predict(X_test)\n",
    "\n",
    "print(\"Prediction:\", prediction)  # 0 (Negative)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**2Ô∏è‚É£ Deep Learning (BERT Fine-Tuning - Overview, No Code Here)**\n",
    "- Uses **pre-trained word embeddings**  \n",
    "- Fine-tunes on a **large dataset** for better accuracy  \n",
    "- Requires **more computational power**  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üîπ **Overview of GenAI in NLP**  \n",
    "**Generative AI (GenAI)** refers to models that can **generate human-like text** based on learned patterns.  \n",
    "\n",
    "### **üîç Key GenAI Applications in NLP**  \n",
    "‚úî **Chatbots & Virtual Assistants** (ChatGPT, Google Bard)  \n",
    "‚úî **Text Summarization** (TLDR, AI-powered news apps)  \n",
    "‚úî **Content Generation** (Blog Writing, Code Generation)  \n",
    "\n",
    "### **üí° Example: Generating Text with GPT-3.5**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "openai.api_key = \"your-api-key\"\n",
    "\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Write a short poem about AI\"}]\n",
    ")\n",
    "\n",
    "print(response[\"choices\"][0][\"message\"][\"content\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ **Text Preprocessing Techniques in NLP**  \n",
    "\n",
    "Text preprocessing is the first step in Natural Language Processing (NLP), transforming raw text into a structured format that can be effectively used by machine learning models.  \n",
    "\n",
    "---\n",
    "\n",
    "## üîπ **1. Basic Preprocessing Techniques**  \n",
    "\n",
    "### **üëâ 1.1 Tokenization (Word, Sentence)**\n",
    "Tokenization is the process of splitting a text into individual words (word tokenization) or sentences (sentence tokenization).  \n",
    "\n",
    "üìå **Why is it important?**  \n",
    "‚úî Helps in analyzing individual words/sentences.  \n",
    "‚úî Essential for NLP tasks like text classification and sentiment analysis.  \n",
    "\n",
    "üîπ **Example: Word & Sentence Tokenization in Python**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Tokenization: ['Natural Language Processing enables machines to understand human language.', 'It is widely used in AI applications.']\n",
      "Word Tokenization: ['Natural', 'Language', 'Processing', 'enables', 'machines', 'to', 'understand', 'human', 'language', '.', 'It', 'is', 'widely', 'used', 'in', 'AI', 'applications', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1000)>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "#nltk.download('punkt')\n",
    "\n",
    "text = \"Natural Language Processing enables machines to understand human language. It is widely used in AI applications.\"\n",
    "\n",
    "# Sentence Tokenization\n",
    "sentences = sent_tokenize(text)\n",
    "print(\"Sentence Tokenization:\", sentences)\n",
    "\n",
    "# Word Tokenization\n",
    "words = word_tokenize(text)\n",
    "print(\"Word Tokenization:\", words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **üëâ 1.2 Stopword Removal**\n",
    "Stopwords are common words (like *is, the, a, an*) that do not add significant meaning to the text.  \n",
    "\n",
    "üìå **Why remove stopwords?**  \n",
    "‚úî Reduces the dimensionality of the dataset.  \n",
    "‚úî Improves model efficiency by focusing on meaningful words.  \n",
    "\n",
    "üîπ **Example: Removing Stopwords in Python**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Words: ['sample', 'text']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "words = [\"this\", \"is\", \"a\", \"sample\", \"text\"]\n",
    "filtered_words = [word for word in words if word.lower() not in stopwords.words('english')]\n",
    "\n",
    "print(\"Filtered Words:\", filtered_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **üëâ 1.3 Stemming & Lemmatization**\n",
    "Both techniques reduce words to their root form.  \n",
    "\n",
    "| **Technique** | **Example** |\n",
    "|--------------|------------|\n",
    "| **Stemming** | \"running\" ‚Üí \"run\", \"studies\" ‚Üí \"studi\" |\n",
    "| **Lemmatization** | \"running\" ‚Üí \"run\", \"studies\" ‚Üí \"study\" |\n",
    "\n",
    "üìå **Difference:**  \n",
    "‚úî **Stemming** is faster but less accurate (removes suffixes).  \n",
    "‚úî **Lemmatization** considers the meaning of words using a dictionary.  \n",
    "\n",
    "üîπ **Example: Stemming in Python**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed Words: ['run', 'fli', 'studi', 'better']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "words = [\"running\", \"flies\", \"studies\", \"better\"]\n",
    "\n",
    "stemmed_words = [stemmer.stem(word) for word in words]\n",
    "print(\"Stemmed Words:\", stemmed_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "üîπ **Example: Lemmatization in Python**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized Words: ['run', 'fly', 'study', 'better']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "#nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "words = [\"running\", \"flies\", \"studies\", \"better\"]\n",
    "\n",
    "lemmatized_words = [lemmatizer.lemmatize(word, pos=\"v\") for word in words]\n",
    "print(\"Lemmatized Words:\", lemmatized_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **üëâ 1.4 Text Normalization**\n",
    "üìå **What is it?**  \n",
    "‚úî Converts text into a consistent format.  \n",
    "‚úî Removes unnecessary variations in text.  \n",
    "\n",
    "üìå **Common Techniques:**  \n",
    "- **Lowercasing** ‚Üí Converts text to lowercase.  \n",
    "- **Removing Punctuation** ‚Üí Removes symbols like ‚Äú.‚Äù, ‚Äú?‚Äù, ‚Äú!‚Äù.  \n",
    "\n",
    "üîπ **Example: Text Normalization in Python**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized Text: hello world nlp is amazing\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import re\n",
    "\n",
    "text = \"Hello WORLD! NLP is AMAZING!!!\"\n",
    "normalized_text = re.sub(r'[^\\w\\s]', '', text.lower())  # Remove punctuation and lowercase\n",
    "print(\"Normalized Text:\", normalized_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ **2. Feature Extraction Techniques**\n",
    "After preprocessing, text needs to be converted into numerical features for machine learning models.\n",
    "\n",
    "---\n",
    "\n",
    "### **üëâ 2.1 TF-IDF (Term Frequency - Inverse Document Frequency)**\n",
    "üìå **What is TF-IDF?**  \n",
    "‚úî **TF (Term Frequency)** ‚Üí Measures how often a word appears in a document.  \n",
    "‚úî **IDF (Inverse Document Frequency)** ‚Üí Gives higher importance to rare words.  \n",
    "\n",
    "üìå **Why use TF-IDF?**  \n",
    "‚úî Captures word importance in a document.  \n",
    "‚úî Better than simple word counts (Bag of Words).  \n",
    "\n",
    "üîπ **Example: TF-IDF in Python**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Feature Names: ['amazing' 'is' 'learning' 'love' 'machine' 'nlp' 'powerful']\n",
      "TF-IDF Vectors:\n",
      " [[0.68091856 0.51785612 0.         0.         0.         0.51785612\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.79596054 0.         0.60534851\n",
      "  0.        ]\n",
      " [0.         0.40204024 0.52863461 0.         0.52863461 0.\n",
      "  0.52863461]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "documents = [\"NLP is amazing\", \"I love NLP\", \"Machine learning is powerful\"]\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(documents)\n",
    "\n",
    "print(\"TF-IDF Feature Names:\", vectorizer.get_feature_names_out())\n",
    "print(\"TF-IDF Vectors:\\n\", X.toarray())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **üëâ 2.2 Bag of Words (BoW)**\n",
    "üìå **What is BoW?**  \n",
    "‚úî Converts text into a vector of word occurrences.  \n",
    "‚úî Represents text numerically but ignores word meaning.  \n",
    "\n",
    "üîπ **Example: BoW in Python**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW Feature Names: ['ai' 'fun' 'future' 'is' 'love' 'nlp' 'the']\n",
      "BoW Vectors:\n",
      " [[0 0 0 0 1 1 0]\n",
      " [0 1 0 1 0 1 0]\n",
      " [1 0 1 1 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "documents = [\"I love NLP\", \"NLP is fun\", \"AI is the future\"]\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(documents)\n",
    "\n",
    "print(\"BoW Feature Names:\", vectorizer.get_feature_names_out())\n",
    "print(\"BoW Vectors:\\n\", X.toarray())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **üëâ 2.3 Word Embeddings (Word2Vec, GloVe, FastText)**\n",
    "üìå **What are Word Embeddings?**  \n",
    "‚úî Represents words as dense vectors in a high-dimensional space.  \n",
    "‚úî Captures word meaning & relationships.  \n",
    "\n",
    "#### **1Ô∏è‚É£ Word2Vec (Google)**\n",
    "- Uses **CBOW (Continuous Bag of Words)** or **Skip-gram** to learn word representations.  \n",
    "- Example: **\"King - Man + Woman = Queen\"**  \n",
    "\n",
    "üîπ **Example: Train Word2Vec using `gensim`**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Vector for 'learning': [-1.0724545e-03  4.7286271e-04  1.0206699e-02  1.8018546e-02\n",
      " -1.8605899e-02 -1.4233618e-02  1.2917745e-02  1.7945977e-02\n",
      " -1.0030856e-02 -7.5267432e-03  1.4761009e-02 -3.0669428e-03\n",
      " -9.0732267e-03  1.3108104e-02 -9.7203208e-03 -3.6320353e-03\n",
      "  5.7531595e-03  1.9837476e-03 -1.6570430e-02 -1.8897636e-02\n",
      "  1.4623532e-02  1.0140524e-02  1.3515387e-02  1.5257311e-03\n",
      "  1.2701781e-02 -6.8107317e-03 -1.8928028e-03  1.1537147e-02\n",
      " -1.5043275e-02 -7.8722071e-03 -1.5023164e-02 -1.8600845e-03\n",
      "  1.9076237e-02 -1.4638334e-02 -4.6675373e-03 -3.8754821e-03\n",
      "  1.6154874e-02 -1.1861792e-02  9.0324880e-05 -9.5074680e-03\n",
      " -1.9207101e-02  1.0014586e-02 -1.7519170e-02 -8.7836506e-03\n",
      " -7.0199967e-05 -5.9236289e-04 -1.5322480e-02  1.9229487e-02\n",
      "  9.9641159e-03  1.8466286e-02]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "sentences = [\n",
    "    word_tokenize(\"I love natural language processing\"),\n",
    "    word_tokenize(\"Machine learning is powerful\"),\n",
    "    word_tokenize(\"Deep learning enables AI breakthroughs\")\n",
    "]\n",
    "\n",
    "model = Word2Vec(sentences, vector_size=50, min_count=1, workers=4)\n",
    "\n",
    "print(\"Word Vector for 'learning':\", model.wv['learning'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "#### **2Ô∏è‚É£ GloVe (Global Vectors - Stanford)**\n",
    "- Learns word embeddings based on word co-occurrence.  \n",
    "- Pre-trained embeddings available (50D, 100D, 200D).  \n",
    "\n",
    "üîπ **Example: Load Pre-trained GloVe Embeddings**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "import os\n",
    "\n",
    "gensim_data_path = os.path.expanduser(\"~/gensim-data\")\n",
    "os.makedirs(gensim_data_path, exist_ok=True)\n",
    "api.BASE_DIR = gensim_data_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "unable to read local cache '/Users/deepanshu/gensim-data/information.json' during fallback, connect to the Internet and retry",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/gensim/downloader.py:219\u001b[0m, in \u001b[0;36m_load_info\u001b[0;34m(url, encoding)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# We need io.open here because Py2 open doesn't support encoding keyword\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m--> 219\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcache_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m fin:\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m json\u001b[38;5;241m.\u001b[39mload(fin)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/deepanshu/gensim-data/information.json'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdownloader\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mapi\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m glove_model \u001b[38;5;241m=\u001b[39m \u001b[43mapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mglove-wiki-gigaword-50\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Load 50D GloVe embeddings\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVector for \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapple\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m, glove_model[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapple\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/gensim/downloader.py:490\u001b[0m, in \u001b[0;36mload\u001b[0;34m(name, return_path)\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Download (if needed) dataset/model and load it to memory (unless `return_path` is set).\u001b[39;00m\n\u001b[1;32m    437\u001b[0m \n\u001b[1;32m    438\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    487\u001b[0m \n\u001b[1;32m    488\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    489\u001b[0m _create_base_dir()\n\u001b[0;32m--> 490\u001b[0m file_name \u001b[38;5;241m=\u001b[39m \u001b[43m_get_filename\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    492\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncorrect model/corpus name\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/gensim/downloader.py:426\u001b[0m, in \u001b[0;36m_get_filename\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_filename\u001b[39m(name):\n\u001b[1;32m    413\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Retrieve the filename of the dataset/model.\u001b[39;00m\n\u001b[1;32m    414\u001b[0m \n\u001b[1;32m    415\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    424\u001b[0m \n\u001b[1;32m    425\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 426\u001b[0m     information \u001b[38;5;241m=\u001b[39m \u001b[43minfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    427\u001b[0m     corpora \u001b[38;5;241m=\u001b[39m information[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcorpora\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    428\u001b[0m     models \u001b[38;5;241m=\u001b[39m information[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodels\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/gensim/downloader.py:268\u001b[0m, in \u001b[0;36minfo\u001b[0;34m(name, show_only_latest, name_only)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minfo\u001b[39m(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, show_only_latest\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, name_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    229\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Provide the information related to model/dataset.\u001b[39;00m\n\u001b[1;32m    230\u001b[0m \n\u001b[1;32m    231\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    266\u001b[0m \n\u001b[1;32m    267\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 268\u001b[0m     information \u001b[38;5;241m=\u001b[39m \u001b[43m_load_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    271\u001b[0m         corpora \u001b[38;5;241m=\u001b[39m information[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcorpora\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/gensim/downloader.py:222\u001b[0m, in \u001b[0;36m_load_info\u001b[0;34m(url, encoding)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m json\u001b[38;5;241m.\u001b[39mload(fin)\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m:\n\u001b[0;32m--> 222\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124munable to read local cache \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m during fallback, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    224\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconnect to the Internet and retry\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m cache_path\n\u001b[1;32m    225\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: unable to read local cache '/Users/deepanshu/gensim-data/information.json' during fallback, connect to the Internet and retry"
     ]
    }
   ],
   "source": [
    "\n",
    "import gensim.downloader as api\n",
    "\n",
    "glove_model = api.load(\"glove-wiki-gigaword-50\")  # Load 50D GloVe embeddings\n",
    "print(\"Vector for 'apple':\", glove_model['apple'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "#### **3Ô∏è‚É£ FastText (Facebook)**\n",
    "- Like Word2Vec but captures subword information.  \n",
    "- Better for handling rare words or misspellings.  \n",
    "\n",
    "üîπ **Example: Load Pre-trained FastText Embeddings**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fasttext_model = api.load(\"fasttext-wiki-news-subwords-300\")\n",
    "print(\"Vector for 'machine':\", fasttext_model['machine'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "# ‚úÖ **Summary**\n",
    "| **Technique** | **Purpose** | **Example Output** |\n",
    "|--------------|------------|---------------------|\n",
    "| **Tokenization** | Splits text into words/sentences | [\"NLP\", \"is\", \"fun\"] |\n",
    "| **Stopword Removal** | Removes common words | [\"NLP\", \"fun\"] |\n",
    "| **Stemming** | Converts words to root form | [\"run\", \"studi\"] |\n",
    "| **Lemmatization** | Converts words to base form | [\"run\", \"study\"] |\n",
    "| **TF-IDF** | Assigns importance to words | `[0.5, 0.8, 0.3]` |\n",
    "| **BoW** | Word frequency count | `[1, 2, 0]` |\n",
    "| **Word Embeddings** | Context-aware word representation | `[0.25, -0.08, 0.67]` |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ **Deep Learning for NLP**  \n",
    "\n",
    "Deep Learning has revolutionized NLP by allowing models to capture complex patterns in text data. Before transformers, **Recurrent Neural Networks (RNNs)** and their variants like **Long Short-Term Memory (LSTM)** and **Gated Recurrent Units (GRU)** were the backbone of deep NLP models. However, these architectures struggled with long-range dependencies, which led to the development of the **Attention Mechanism** and ultimately **Transformers (BERT, GPT, etc.)**.\n",
    "\n",
    "---\n",
    "\n",
    "## **üîπ 1. Recurrent Neural Networks (RNNs) & LSTMs**\n",
    "\n",
    "### **1.1 Why RNNs for NLP?**\n",
    "- Traditional feedforward neural networks **do not handle sequential data well** because they process inputs independently.  \n",
    "- **Recurrent Neural Networks (RNNs)** were designed to handle sequential data, making them ideal for NLP tasks like text classification, machine translation, and speech recognition.  \n",
    "\n",
    "### **1.2 Understanding RNNs**\n",
    "An **RNN** processes a sequence of words step-by-step while **maintaining a hidden state** that captures past information.\n",
    "\n",
    "#### **Mathematical Formulation:**\n",
    "At each time step $ t $:\n",
    "$$\n",
    "h_t = \\tanh(W_h h_{t-1} + W_x x_t + b)\n",
    "$$\n",
    "where:  \n",
    "- $ h_t $ = hidden state at time step $ t $  \n",
    "- $ x_t $ = input at time step $ t $ (word embedding)  \n",
    "- $ W_h, W_x, b $ = learnable weights  \n",
    "\n",
    "However, RNNs struggle with **long-term dependencies** because of the **vanishing gradient problem**, meaning they forget past information as sequences get longer.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **1.3 Long Short-Term Memory (LSTM)**\n",
    "LSTMs solve the **vanishing gradient problem** by introducing a **cell state** that selectively remembers and forgets information.  \n",
    "\n",
    "#### **LSTM Architecture**\n",
    "An LSTM consists of three key gates:  \n",
    "1. **Forget Gate** $ f_t $ ‚Äì Decides what to forget  \n",
    "   $$\n",
    "   f_t = \\sigma(W_f [h_{t-1}, x_t] + b_f)\n",
    "   $$\n",
    "2. **Input Gate** $ i_t $ ‚Äì Decides what new information to store  \n",
    "   $$\n",
    "   i_t = \\sigma(W_i [h_{t-1}, x_t] + b_i)\n",
    "   $$\n",
    "3. **Output Gate** $ o_t $ ‚Äì Decides what to output  \n",
    "   $$\n",
    "   o_t = \\sigma(W_o [h_{t-1}, x_t] + b_o)\n",
    "   $$\n",
    "4. **Cell State Update**  \n",
    "   $$\n",
    "   C_t = f_t * C_{t-1} + i_t * \\tilde{C_t}\n",
    "   $$\n",
    "   where $ \\tilde{C_t} $ is the candidate cell state.\n",
    "\n",
    "#### **üîπ Python Implementation of LSTM for NLP**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.fc(out[:, -1, :])  # Get the last time step output\n",
    "        return out\n",
    "\n",
    "# Example: LSTM for text classification\n",
    "lstm_model = LSTMModel(input_size=50, hidden_size=128, output_size=2)  # Binary classification\n",
    "print(lstm_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### **1.4 Gated Recurrent Units (GRUs)**\n",
    "- GRUs are a simplified version of LSTMs with fewer parameters.  \n",
    "- They **combine the forget and input gates** into a single **update gate**.  \n",
    "- GRUs perform similarly to LSTMs but are computationally faster.  \n",
    "\n",
    "#### **GRU Update Equations**\n",
    "$$\n",
    "z_t = \\sigma(W_z [h_{t-1}, x_t])\n",
    "$$\n",
    "$$\n",
    "r_t = \\sigma(W_r [h_{t-1}, x_t])\n",
    "$$\n",
    "$$\n",
    "\\tilde{h_t} = \\tanh(W_h [r_t * h_{t-1}, x_t])\n",
    "$$\n",
    "$$\n",
    "h_t = (1 - z_t) * h_{t-1} + z_t * \\tilde{h_t}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## **üîπ 2. Attention Mechanism & Transformers**\n",
    "Although LSTMs and GRUs improved memory retention, they still **process words sequentially**, which limits parallelization. **Attention Mechanisms** solve this issue.\n",
    "\n",
    "### **2.1 Problems with RNNs**\n",
    "- **Long-Term Dependencies**: Even LSTMs struggle with extremely long sequences.  \n",
    "- **Sequential Processing**: Cannot process words in parallel, slowing training.  \n",
    "\n",
    "### **2.2 Introduction to Attention**\n",
    "The **attention mechanism** allows models to focus on the most relevant words in a sentence rather than treating all words equally.\n",
    "\n",
    "#### **üîπ Self-Attention Mechanism**\n",
    "Self-Attention computes relationships between words in a sentence.  \n",
    "\n",
    "Given an input sequence of words, we define:\n",
    "1. **Query (Q)** ‚Äì What are we looking for?  \n",
    "2. **Key (K)** ‚Äì What do we have?  \n",
    "3. **Value (V)** ‚Äì What information do we use?  \n",
    "\n",
    "$$\n",
    "\\text{Attention(Q, K, V)} = \\text{softmax} \\left( \\frac{QK^T}{\\sqrt{d_k}} \\right) V\n",
    "$$\n",
    "where $ d_k $ is the scaling factor.\n",
    "\n",
    "### **2.3 Multi-Head Attention**\n",
    "Instead of computing a single attention score, we compute multiple **attention heads** in parallel, capturing different types of relationships.\n",
    "\n",
    "$$\n",
    "\\text{MultiHead(Q, K, V)} = \\text{Concat}(\\text{head}_1, ..., \\text{head}_h) W^O\n",
    "$$\n",
    "\n",
    "### **2.4 Transformer Architecture**\n",
    "Transformers (BERT, GPT) replace RNNs with attention layers.\n",
    "\n",
    "#### **üîπ Transformer Encoder (BERT)**\n",
    "- **Multi-Head Attention** ‚Üí Focuses on relevant words  \n",
    "- **Feedforward Network** ‚Üí Learns representations  \n",
    "- **Layer Normalization** ‚Üí Stabilizes learning  \n",
    "\n",
    "#### **üîπ Transformer Decoder (GPT)**\n",
    "- Uses **causal attention** (cannot see future words)  \n",
    "- Generates text token by token  \n",
    "\n",
    "---\n",
    "\n",
    "### **üîπ Python Implementation of Self-Attention**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.head_dim = embed_size // heads\n",
    "\n",
    "        self.values = nn.Linear(embed_size, embed_size, bias=False)\n",
    "        self.keys = nn.Linear(embed_size, embed_size, bias=False)\n",
    "        self.queries = nn.Linear(embed_size, embed_size, bias=False)\n",
    "        self.fc_out = nn.Linear(embed_size, embed_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        N, seq_length, embed_size = x.shape\n",
    "        values = self.values(x)\n",
    "        keys = self.keys(x)\n",
    "        queries = self.queries(x)\n",
    "\n",
    "        attention = torch.softmax(torch.matmul(queries, keys.transpose(-2, -1)) / self.head_dim**0.5, dim=-1)\n",
    "        out = torch.matmul(attention, values)\n",
    "        out = self.fc_out(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "# Example usage\n",
    "embed_size = 128  # Size of word embeddings\n",
    "heads = 8  # Multi-head attention\n",
    "self_attention = SelfAttention(embed_size, heads)\n",
    "print(self_attention)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "## **‚úÖ Summary of What We Covered**\n",
    "‚úî **RNNs, LSTMs, GRUs** ‚Üí Sequential models for NLP  \n",
    "‚úî **Attention Mechanism** ‚Üí Allows models to focus on relevant words  \n",
    "‚úî **Self-Attention & Multi-Head Attention** ‚Üí Core of transformers  \n",
    "‚úî **Transformers (BERT, GPT) Overview** ‚Üí Modern NLP models  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ **Deep Dive into Transformer Models**  \n",
    "\n",
    "Transformers have revolutionized NLP by enabling powerful models like **BERT, GPT, T5, and others**. Unlike traditional sequence models (RNNs, LSTMs), transformers leverage **self-attention** to process input **in parallel**, leading to **faster and more accurate text understanding and generation**.\n",
    "\n",
    "---\n",
    "\n",
    "# **üîπ 1. Introduction to Transformers**  \n",
    "\n",
    "### **1.1 Why Transformers?**\n",
    "Traditional models (RNNs, LSTMs) process text sequentially, leading to:\n",
    "- **Long-term dependency issues** (forgetting earlier words in long texts).  \n",
    "- **Slow training** due to sequential computations.  \n",
    "\n",
    "**Transformers solve these problems** by using:  \n",
    "‚úÖ **Self-Attention** (allows the model to focus on important words)  \n",
    "‚úÖ **Parallel Processing** (faster than RNNs)  \n",
    "‚úÖ **Positional Encoding** (captures word order)  \n",
    "\n",
    "### **1.2 Transformer Architecture**\n",
    "A **transformer** consists of an **encoder** and a **decoder**:  \n",
    "- **Encoder (Used in BERT)** ‚Üí Processes input text into representations.  \n",
    "- **Decoder (Used in GPT)** ‚Üí Generates text token by token.  \n",
    "\n",
    "Each block in a transformer contains:  \n",
    "‚úî **Multi-Head Self-Attention** ‚Äì Helps the model focus on important words.  \n",
    "‚úî **Feedforward Network** ‚Äì Adds non-linearity for better learning.  \n",
    "‚úî **Layer Normalization & Residual Connections** ‚Äì Stabilizes training.  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# **üîπ 2. BERT (Bidirectional Encoder Representations from Transformers)**  \n",
    "\n",
    "BERT is **an encoder-only transformer model** that **understands text bidirectionally**. It was introduced by Google in 2018 and became the foundation of many NLP applications.\n",
    "\n",
    "### **2.1 Understanding BERT‚Äôs Architecture**\n",
    "- **BERT uses only the encoder** part of the transformer.  \n",
    "- It **reads text in both directions (left to right & right to left)**.  \n",
    "- Uses **self-attention** to capture relationships between words.  \n",
    "\n",
    "**Example**:  \n",
    "‚û°Ô∏è In the sentence:  \n",
    "  _\"The **bank** of the river was beautiful.\"_  \n",
    "‚û°Ô∏è A **bidirectional model** understands that \"bank\" means \"riverbank\" and not \"financial institution.\"\n",
    "\n",
    "### **2.2 Pretraining BERT**\n",
    "BERT is **pretrained** on a large corpus using two tasks:  \n",
    "‚úî **Masked Language Modeling (MLM)** ‚Äì Randomly masks words and predicts them.  \n",
    "‚úî **Next Sentence Prediction (NSP)** ‚Äì Predicts if sentence B follows sentence A.\n",
    "\n",
    "#### **üîπ BERT Pretraining Code in Python**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "import torch\n",
    "\n",
    "# Load pre-trained BERT tokenizer & model\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Example sentence with masked word\n",
    "sentence = \"The capital of France is [MASK].\"\n",
    "tokens = tokenizer(sentence, return_tensors=\"pt\")\n",
    "\n",
    "# Predict the masked word\n",
    "with torch.no_grad():\n",
    "    outputs = model(**tokens)\n",
    "    predictions = outputs.logits\n",
    "\n",
    "# Decode the predicted word\n",
    "predicted_token = torch.argmax(predictions[0, 5]).item()\n",
    "predicted_word = tokenizer.decode([predicted_token])\n",
    "print(f\"Predicted word: {predicted_word}\")  # Expected output: Paris\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### **2.3 Fine-Tuning BERT for NLP Tasks**\n",
    "BERT can be fine-tuned for:\n",
    "‚úÖ **Text Classification (Sentiment Analysis, Spam Detection)**  \n",
    "‚úÖ **Question Answering (QA Models like SQuAD)**  \n",
    "‚úÖ **Named Entity Recognition (NER)**  \n",
    "‚úÖ **Summarization, Translation**  \n",
    "\n",
    "#### **üîπ Fine-Tuning BERT for Sentiment Analysis**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Load pre-trained BERT model for classification\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "\n",
    "# Example sentences\n",
    "texts = [\"I love this movie!\", \"This product is terrible.\"]\n",
    "\n",
    "# Tokenize and convert to tensor\n",
    "tokens = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Predict sentiment\n",
    "with torch.no_grad():\n",
    "    outputs = model(**tokens)\n",
    "    predictions = torch.argmax(outputs.logits, dim=1)\n",
    "\n",
    "print(predictions)  # 1 for positive, 0 for negative\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# **üîπ 3. GPT (Generative Pretrained Transformer)**  \n",
    "\n",
    "### **3.1 Overview of GPT-2, GPT-3, and GPT-4**\n",
    "- **GPT is a decoder-only transformer model** designed for text generation.  \n",
    "- It **reads text left to right** (unidirectional).  \n",
    "- **Larger models (GPT-3, GPT-4)** improve text coherence and reasoning.\n",
    "\n",
    "| Model | Parameters | Key Feature |\n",
    "|--------|-----------|-------------|\n",
    "| **GPT-2** | 1.5B | Coherent text generation |\n",
    "| **GPT-3** | 175B | Few-shot learning |\n",
    "| **GPT-4** | >1T? | Multimodal, reasoning |\n",
    "\n",
    "### **3.2 How GPT Generates Text**\n",
    "GPT generates text **token by token**, predicting the **next most probable word**.  \n",
    "Example:  \n",
    "‚úÖ Input: \"Once upon a time\"  \n",
    "‚úÖ GPT predicts: \"there was a king who ruled a vast kingdom.\"\n",
    "\n",
    "#### **üîπ GPT-2 Text Generation in Python**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load GPT-2 model & tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Input text\n",
    "input_text = \"Once upon a time\"\n",
    "input_tokens = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# Generate text\n",
    "output = model.generate(input_tokens, max_length=50, temperature=0.7)\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **3.3 Fine-Tuning GPT for Specific Tasks**\n",
    "GPT can be fine-tuned for:\n",
    "‚úÖ **Chatbots (Customer Support, AI Assistants)**  \n",
    "‚úÖ **Creative Writing (Story Generation, Code Generation)**  \n",
    "‚úÖ **Financial Reports, Medical Summarization**  \n",
    "\n",
    "---\n",
    "\n",
    "# **üîπ 4. Other Transformer Models**\n",
    "\n",
    "### **4.1 T5 (Text-to-Text Transfer Transformer)**\n",
    "- Unlike BERT/GPT, **T5 treats every NLP task as text generation**.\n",
    "- **Example Tasks:**\n",
    "  - Summarization: **\"Summarize: The book is about...\"**  \n",
    "  - Translation: **\"Translate to French: Hello!\"**  \n",
    "\n",
    "#### **üîπ T5 Summarization Example**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "\n",
    "text = \"Summarize: This article explains transformers in deep learning.\"\n",
    "tokens = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "summary = model.generate(tokens)\n",
    "print(tokenizer.decode(summary[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **4.2 DistilBERT (Efficient BERT)**\n",
    "- A **lighter version of BERT** that retains 97% accuracy but is **60% smaller**.\n",
    "- Used in mobile and real-time applications.\n",
    "\n",
    "---\n",
    "\n",
    "### **4.3 Open-Source Transformer Models**\n",
    "‚úî **LLaMA (Meta AI‚Äôs model)** ‚Äì Open-source GPT alternative.  \n",
    "‚úî **Falcon (Hugging Face Model)** ‚Äì Efficient and optimized for inference.  \n",
    "‚úî **Mistral, Bloom** ‚Äì Used for research and enterprise applications.  \n",
    "\n",
    "---\n",
    "\n",
    "# **‚úÖ Summary of What We Covered**\n",
    "‚úî **Transformer Basics** ‚Üí Self-Attention, Multi-Head Attention  \n",
    "‚úî **BERT** ‚Üí Bidirectional, Used for Understanding Text (NER, QA, Classification)  \n",
    "‚úî **GPT** ‚Üí Unidirectional, Used for Text Generation (Chatbots, Summarization)  \n",
    "‚úî **T5** ‚Üí Text-to-Text Model for Translation, Summarization  \n",
    "‚úî **DistilBERT, LLaMA, Falcon** ‚Üí Optimized transformer models  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ **Advanced NLP & GenAI Applications**  \n",
    "\n",
    "After covering the foundations of NLP and transformers, let's now **explore real-world applications** like **NER, Sentiment Analysis, Question Answering, Summarization, Chatbots, AI Content Generation, and Deployment**.  \n",
    "\n",
    "---\n",
    "\n",
    "# **üîπ 1. NLP in Action (Real-World Applications)**  \n",
    "\n",
    "## **üëâ 1.1. Named Entity Recognition (NER)**  \n",
    "**NER extracts entities like names, dates, locations, and organizations from text.**  \n",
    "It is widely used in:\n",
    "- **Finance** (Extracting company names from news)\n",
    "- **Healthcare** (Identifying medical terms in documents)\n",
    "- **Legal Industry** (Extracting case details)\n",
    "\n",
    "### **üìù Example: Extracting Named Entities Using SpaCy**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elon Musk - PERSON\n",
      "Tesla - ORG\n",
      "Berlin - GPE\n",
      "March 5, 2023 - DATE\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import spacy\n",
    "\n",
    "# Load pre-trained NLP model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "text = \"Elon Musk, the CEO of Tesla, visited Berlin on March 5, 2023.\"\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(text)\n",
    "\n",
    "# Print Named Entities\n",
    "for ent in doc.ents:\n",
    "    print(f\"{ent.text} - {ent.label_}\")\n",
    "\n",
    "# Expected Output:\n",
    "# Elon Musk - PERSON\n",
    "# Tesla - ORG\n",
    "# Berlin - GPE\n",
    "# March 5, 2023 - DATE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "## **üëâ 1.2 Sentiment Analysis**  \n",
    "Sentiment Analysis determines whether a text is **positive, negative, or neutral**.  \n",
    "It is widely used in:\n",
    "- **Social Media Monitoring** (Twitter sentiment)\n",
    "- **Customer Reviews Analysis**\n",
    "- **Stock Market Predictions** (Based on news sentiment)\n",
    "\n",
    "### **üìù Sentiment Analysis Using Transformers**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Load sentiment analysis pipeline\u001b[39;00m\n\u001b[1;32m      4\u001b[0m sentiment_pipeline \u001b[38;5;241m=\u001b[39m pipeline(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentiment-analysis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load sentiment analysis pipeline\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "text = \"I love the new Tesla model, it's amazing!\"\n",
    "result = sentiment_pipeline(text)\n",
    "print(result)\n",
    "\n",
    "# Expected Output: [{'label': 'POSITIVE', 'score': 0.99}]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "## **üëâ 1.3 Question Answering (QA)**  \n",
    "QA models extract answers from a given passage.  \n",
    "Use Cases:\n",
    "- **Customer Support Chatbots**\n",
    "- **Document Search**\n",
    "- **AI Assistants like ChatGPT, Siri, Alexa**\n",
    "\n",
    "### **üìù Example: Question Answering with BERT**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load the QA model\n",
    "qa_pipeline = pipeline(\"question-answering\")\n",
    "\n",
    "context = \"Elon Musk is the CEO of Tesla and SpaceX.\"\n",
    "question = \"Who is the CEO of Tesla?\"\n",
    "\n",
    "# Get the answer\n",
    "answer = qa_pipeline(question=question, context=context)\n",
    "print(answer[\"answer\"])\n",
    "\n",
    "# Expected Output: \"Elon Musk\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "## **üëâ 1.4 Text Summarization**  \n",
    "Summarization reduces long text into concise summaries.  \n",
    "Use Cases:\n",
    "- **News Summarization**\n",
    "- **Legal & Financial Document Summaries**\n",
    "- **YouTube Video Transcripts Summarization**\n",
    "\n",
    "### **üìù Example: Summarization with T5**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load summarization model\n",
    "summarizer = pipeline(\"summarization\")\n",
    "\n",
    "text = \"Transformers have revolutionized NLP by enabling state-of-the-art performance on various tasks. They use self-attention mechanisms to process text efficiently.\"\n",
    "\n",
    "# Summarize the text\n",
    "summary = summarizer(text, max_length=50, min_length=10, do_sample=False)\n",
    "print(summary[0][\"summary_text\"])\n",
    "\n",
    "# Expected Output: \"Transformers revolutionized NLP using self-attention.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "# **üîπ 2. GenAI for Text Generation**  \n",
    "\n",
    "## **üëâ 2.1 Chatbots & Conversational AI**  \n",
    "AI-powered chatbots use **LLMs (Large Language Models)** like **GPT-4** to interact with users.  \n",
    "Use Cases:\n",
    "- **Customer Support Bots**\n",
    "- **Healthcare Assistants**\n",
    "- **Personal AI Assistants**\n",
    "\n",
    "### **üìù Example: Chatbot using OpenAI API**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import openai\n",
    "\n",
    "# Set your API key (Replace 'your-api-key' with actual OpenAI API key)\n",
    "openai.api_key = \"your-api-key\"\n",
    "\n",
    "def chatbot(prompt):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    return response[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "# Example Chatbot Conversation\n",
    "print(chatbot(\"How does a transformer model work?\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "## **üëâ 2.2 AI-Powered Content Generation**  \n",
    "Generative AI can write **blog posts, marketing content, and social media captions**.  \n",
    "Use Cases:\n",
    "- **Automated Content Writing**\n",
    "- **Email Drafting**\n",
    "- **Creative Writing (Stories, Poetry)**\n",
    "\n",
    "### **üìù Example: AI-generated Blog Post Using GPT**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prompt = \"Write a short blog post about the future of AI in finance.\"\n",
    "\n",
    "print(chatbot(prompt))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## **üëâ 2.3 Code Generation (Codex, StarCoder, Code Llama)**  \n",
    "AI can generate **code snippets, fix bugs, and suggest optimizations**.  \n",
    "Use Cases:\n",
    "- **Automated Code Completion** (GitHub Copilot)\n",
    "- **AI-assisted Debugging**\n",
    "- **Code Translation (Python ‚Üí Java, etc.)**\n",
    "\n",
    "### **üìù Example: Code Generation Using OpenAI Codex**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prompt = \"Write a Python function to check if a number is prime.\"\n",
    "\n",
    "print(chatbot(prompt))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "# **üîπ 3. Fine-Tuning & Deployment**  \n",
    "\n",
    "## **üëâ 3.1 Fine-Tuning Transformers with Hugging Face**  \n",
    "Fine-tuning allows us to train transformer models on **custom datasets** for specific use cases.\n",
    "\n",
    "### **üìù Fine-Tuning BERT for Custom Text Classification**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import BertForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "# Load pre-trained BERT model\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8\n",
    ")\n",
    "\n",
    "# Define trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,  # Custom dataset\n",
    "    eval_dataset=eval_dataset     # Validation dataset\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "## **üëâ 3.2 Deploying AI Models (Streamlit, FastAPI, LangChain)**  \n",
    "\n",
    "After fine-tuning a model, we need to deploy it for real-world use.  \n",
    "**Deployment Options:**\n",
    "‚úÖ **Streamlit** ‚Äì Build an interactive UI for NLP applications.  \n",
    "‚úÖ **FastAPI** ‚Äì Deploy models as REST APIs.  \n",
    "‚úÖ **LangChain** ‚Äì Create AI-powered applications using LLMs.\n",
    "\n",
    "### **üìù Example: Deploying a Chatbot Using Streamlit**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import streamlit as st\n",
    "import openai\n",
    "\n",
    "# Set API key\n",
    "openai.api_key = \"your-api-key\"\n",
    "\n",
    "st.title(\"AI Chatbot\")\n",
    "\n",
    "user_input = st.text_input(\"Ask a question:\")\n",
    "\n",
    "if st.button(\"Send\"):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[{\"role\": \"user\", \"content\": user_input}]\n",
    "    )\n",
    "    st.write(response[\"choices\"][0][\"message\"][\"content\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "## **üëâ 3.3 Optimizing LLMs for Efficiency**  \n",
    "LLMs can be **memory-intensive** and **expensive to run**.  \n",
    "‚úÖ **Quantization** (Reduces model size)  \n",
    "‚úÖ **Distillation** (Uses smaller models like DistilBERT)  \n",
    "‚úÖ **Model Caching** (Speeds up inference)\n",
    "\n",
    "---\n",
    "\n",
    "# **‚úÖ Summary of What We Covered**\n",
    "‚úî **NER, Sentiment Analysis, QA, Summarization**  \n",
    "‚úî **Chatbots, Content Creation, Code Generation**  \n",
    "‚úî **Fine-Tuning Transformers for Custom Tasks**  \n",
    "‚úî **Model Deployment with Streamlit, FastAPI, LangChain**  \n",
    "‚úî **Optimizing Large Language Models (LLMs)**  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Here's a structured **flowchart of NLP & GenAI evolution**, starting from traditional text processing to modern Large Language Models (LLMs).  \n",
    "\n",
    "---\n",
    "\n",
    "## **üåç Evolution of NLP & GenAI: From Basics to Cutting-Edge AI**  \n",
    "\n",
    "### **1Ô∏è‚É£ Traditional NLP (Rule-Based & Classical ML Approaches)**\n",
    "üìå **Early NLP focused on rule-based methods and statistical techniques.**  \n",
    "\n",
    "- **Text Preprocessing** üõ†Ô∏è  \n",
    "  - Tokenization, Stopword Removal, Lemmatization  \n",
    "  - TF-IDF, Bag of Words (BoW), N-grams  \n",
    "\n",
    "- **Machine Learning for NLP** ü§ñ  \n",
    "  - Logistic Regression, Naive Bayes for Text Classification  \n",
    "  - Hidden Markov Models (HMM) & Conditional Random Fields (CRF) for NER  \n",
    "  - Latent Dirichlet Allocation (LDA) for Topic Modeling  \n",
    "\n",
    "üì¢ **Limitations**  \n",
    "‚ùå Feature engineering required  \n",
    "‚ùå Poor handling of complex grammar & context  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **2Ô∏è‚É£ Deep Learning for NLP (2010s - Rise of Neural Networks)**\n",
    "üìå **DL-based NLP improved text understanding using neural networks.**  \n",
    "\n",
    "- **Word Embeddings üåç** (Understanding word meanings)  \n",
    "  - Word2Vec, GloVe, FastText  \n",
    "\n",
    "- **Recurrent Neural Networks (RNNs) & Variants üîÅ**  \n",
    "  - Vanilla RNNs (Context-dependent text processing)  \n",
    "  - Long Short-Term Memory (LSTM) & Gated Recurrent Units (GRU) (Long-term memory)  \n",
    "\n",
    "- **CNNs for Text Processing üìú**  \n",
    "  - Text classification, Sentence embeddings  \n",
    "\n",
    "üì¢ **Limitations**  \n",
    "‚ùå RNNs struggle with long-range dependencies  \n",
    "‚ùå Sequential processing is slow  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **3Ô∏è‚É£ Attention Mechanism & Transformers (Breakthrough in NLP)**\n",
    "üìå **Transformers introduced parallel processing and self-attention.**  \n",
    "\n",
    "- **Self-Attention Mechanism** (Key innovation üèÜ)  \n",
    "  - Helps model **relationships between all words** in a sentence  \n",
    "  - Improves context understanding  \n",
    "\n",
    "- **Transformer Model (Vaswani et al., 2017) ‚ö°**  \n",
    "  - Fully parallelizable ‚Üí Faster training  \n",
    "  - Scales well for long sequences  \n",
    "\n",
    "üì¢ **Transformers replaced RNNs & LSTMs in NLP** üéâ  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **4Ô∏è‚É£ Transformer-Based Models (2020s - Rise of LLMs)**\n",
    "üìå **State-of-the-art NLP powered by massive LLMs.**  \n",
    "\n",
    "- **BERT (Bidirectional Encoder Representations from Transformers) üîÑ**  \n",
    "  - Context-aware embeddings (understands both left & right context)  \n",
    "  - Used for classification, sentiment analysis, question answering  \n",
    "\n",
    "- **GPT (Generative Pretrained Transformer) üî•**  \n",
    "  - **GPT-2, GPT-3, GPT-4** ‚Üí Autoregressive text generation  \n",
    "  - Chatbots, Storytelling, Code Generation  \n",
    "\n",
    "- **T5 (Text-to-Text Transfer Transformer) üìñ**  \n",
    "  - Converts all NLP tasks into text generation problems  \n",
    "\n",
    "- **DistilBERT, ALBERT (Optimized Transformers) üöÄ**  \n",
    "  - Smaller, faster versions of BERT for deployment  \n",
    "\n",
    "üì¢ **Transformers dominate modern NLP applications**  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### **5Ô∏è‚É£ Generative AI & LLM Applications (Today & Beyond)**\n",
    "üìå **LLMs revolutionized AI applications beyond NLP.**  \n",
    "\n",
    "- **Conversational AI & Chatbots ü§ñ**  \n",
    "  - OpenAI's ChatGPT, Google's Gemini, Meta‚Äôs LLaMA  \n",
    "  - AI-powered personal assistants, enterprise chatbots  \n",
    "\n",
    "- **AI-Powered Content Creation üìù**  \n",
    "  - Blog writing, Social media content, AI-assisted coding (Codex, StarCoder)  \n",
    "\n",
    "- **Multimodal Models (Text, Image, Video, Audio) üé•**  \n",
    "  - GPT-4o, Gemini 1.5, LLaMA 3 ‚Üí Vision-Language models  \n",
    "\n",
    "üì¢ **Next frontier**: **AGI (Artificial General Intelligence)**? üöÄ  \n",
    "\n",
    "---\n",
    "\n",
    "## **üó∫Ô∏è Complete Flowchart: Evolution of NLP & GenAI**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```plaintext\n",
    "1Ô∏è‚É£ Traditional NLP (Rule-Based & ML)  \n",
    "   ‚îú‚îÄ‚îÄ Text Preprocessing (Tokenization, Stopwords, TF-IDF)  \n",
    "   ‚îú‚îÄ‚îÄ Machine Learning for NLP (Naive Bayes, SVM, CRF)  \n",
    "   ‚îî‚îÄ‚îÄ Topic Modeling (LDA)  \n",
    "\n",
    "2Ô∏è‚É£ Deep Learning for NLP  \n",
    "   ‚îú‚îÄ‚îÄ Word Embeddings (Word2Vec, GloVe, FastText)  \n",
    "   ‚îú‚îÄ‚îÄ RNNs & LSTMs (Context Learning)  \n",
    "   ‚îú‚îÄ‚îÄ CNNs for NLP (Text Classification)  \n",
    "   ‚îî‚îÄ‚îÄ Seq2Seq Models (Early Translation & Chatbots)  \n",
    "\n",
    "3Ô∏è‚É£ Attention & Transformers  \n",
    "   ‚îú‚îÄ‚îÄ Self-Attention Mechanism  \n",
    "   ‚îú‚îÄ‚îÄ Transformer Architecture (Vaswani et al.)  \n",
    "   ‚îî‚îÄ‚îÄ Faster & More Scalable NLP  \n",
    "\n",
    "4Ô∏è‚É£ Transformer-Based Models  \n",
    "   ‚îú‚îÄ‚îÄ **BERT** (Contextual Representation)  \n",
    "   ‚îú‚îÄ‚îÄ **GPT** (Autoregressive Text Generation)  \n",
    "   ‚îú‚îÄ‚îÄ **T5** (Text-to-Text Learning)  \n",
    "   ‚îú‚îÄ‚îÄ **DistilBERT & ALBERT** (Efficient Transformers)  \n",
    "   ‚îî‚îÄ‚îÄ **Multilingual NLP Models**  \n",
    "\n",
    "5Ô∏è‚É£ Generative AI (LLMs)  \n",
    "   ‚îú‚îÄ‚îÄ Conversational AI (ChatGPT, Bard, LLaMA)  \n",
    "   ‚îú‚îÄ‚îÄ AI Content Generation (Blogs, Code, Marketing)  \n",
    "   ‚îú‚îÄ‚îÄ Multimodal AI (Text + Image + Video)  \n",
    "   ‚îî‚îÄ‚îÄ Future Trends (AGI & Beyond)  \n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## **üåü Summary**\n",
    "‚úÖ **Traditional NLP** ‚Üí Rule-based & Statistical ML  \n",
    "‚úÖ **Deep Learning for NLP** ‚Üí RNNs, LSTMs, CNNs  \n",
    "‚úÖ **Transformers** ‚Üí Self-Attention, BERT, GPT  \n",
    "‚úÖ **Generative AI** ‚Üí Chatbots, LLMs, Multimodal AI  \n",
    "\n",
    "üöÄ **Future Focus**: AGI, Multimodal Models, Optimized LLMs for Edge Devices  \n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
