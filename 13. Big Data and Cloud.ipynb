{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🚀 **Big Data & Cloud ML: Topics & Subtopics**  \n",
    "\n",
    "This will cover **Big Data processing**, **scalable machine learning**, and **cloud-based AI services** to handle large-scale data and deploy ML models effectively.  \n",
    "\n",
    "---\n",
    "\n",
    "## **🔹 1. Big Data Processing Frameworks**  \n",
    "These tools help handle large datasets efficiently.  \n",
    "\n",
    "### ✅ **Apache Spark** (Distributed Data Processing)  \n",
    "- Spark Architecture (RDD, DAG, Lazy Evaluation)  \n",
    "- Spark DataFrames & SQL  \n",
    "- Spark MLlib (Machine Learning on Spark)  \n",
    "- Optimizations (Partitioning, Caching, Broadcasting)  \n",
    "\n",
    "### ✅ **Dask** (Parallel Computing in Python)  \n",
    "- Dask vs. Spark  \n",
    "- Dask DataFrames & Arrays  \n",
    "- Scaling Pandas Workflows  \n",
    "- Parallel ML Training with Dask-ML  \n",
    "\n",
    "### ✅ **Other Big Data Tools**  \n",
    "- **Apache Flink** – Real-time stream processing  \n",
    "- **Apache Kafka** – Message streaming for Big Data  \n",
    "- **Hadoop & HDFS** – Distributed storage & processing  \n",
    "\n",
    "🛠 **Hands-on:**  \n",
    "- Process large datasets with **Spark & Dask**  \n",
    "- Build a **real-time stream processing pipeline with Kafka**  \n",
    "\n",
    "---\n",
    "\n",
    "## **🔹 2. Scalable Machine Learning (ML) on Big Data**  \n",
    "How to train & deploy ML models efficiently on large datasets.  \n",
    "\n",
    "### ✅ **Machine Learning with Spark MLlib**  \n",
    "- Feature Engineering at Scale  \n",
    "- Training ML Models (Regression, Classification)  \n",
    "- Hyperparameter Tuning in Spark  \n",
    "- Deploying Spark ML Models  \n",
    "\n",
    "### ✅ **Dask-ML for Scalable ML**  \n",
    "- Parallel Hyperparameter Tuning  \n",
    "- Scaling Scikit-Learn with Dask  \n",
    "- Model Training on Large Datasets  \n",
    "\n",
    "### ✅ **Online & Incremental Learning**  \n",
    "- Streaming ML with Spark Streaming  \n",
    "- Incremental Learning (River, Vowpal Wabbit)  \n",
    "- Federated Learning (Google’s FedAvg, PySyft)  \n",
    "\n",
    "🛠 **Hands-on:**  \n",
    "- Train a **large-scale ML model using Spark MLlib**  \n",
    "- Implement **incremental learning for streaming data**  \n",
    "\n",
    "---\n",
    "\n",
    "## **🔹 3. Cloud ML: AWS, GCP, Azure**  \n",
    "Deploying ML models in the cloud using **managed AI services**.  \n",
    "\n",
    "### ✅ **AWS Machine Learning**  \n",
    "- **SageMaker** – Train & deploy ML models  \n",
    "- **AWS Lambda & API Gateway** – Serverless ML  \n",
    "- **EMR (Elastic MapReduce)** – Run Spark on AWS  \n",
    "\n",
    "### ✅ **Google Cloud AI**  \n",
    "- **Vertex AI** – End-to-end ML platform  \n",
    "- **BigQuery ML** – Train ML models inside BigQuery  \n",
    "- **Dataflow & Dataproc** – Serverless Spark & Hadoop  \n",
    "\n",
    "### ✅ **Microsoft Azure AI**  \n",
    "- **Azure Machine Learning Studio**  \n",
    "- **Azure Databricks** (Cloud-based Spark)  \n",
    "- **ML Pipelines & MLOps in Azure**  \n",
    "\n",
    "🛠 **Hands-on:**  \n",
    "- Deploy an **ML model using AWS SageMaker**  \n",
    "- Train an **ML model in BigQuery ML**  \n",
    "\n",
    "---\n",
    "\n",
    "## **🔹 4. MLOps & Model Deployment at Scale**  \n",
    "How to deploy and manage ML models in production.  \n",
    "\n",
    "### ✅ **MLOps for Scalable ML**  \n",
    "- **CI/CD for ML** – Automate ML pipelines  \n",
    "- **Model Versioning & Monitoring** (MLflow, Kubeflow)  \n",
    "- **A/B Testing & Model Drift Detection**  \n",
    "\n",
    "### ✅ **Deploying ML Models at Scale**  \n",
    "- **Serverless Deployment** (FastAPI, AWS Lambda)  \n",
    "- **Kubernetes for ML (Kubeflow)**  \n",
    "- **Inference Optimization** (ONNX, TensorRT)  \n",
    "\n",
    "🛠 **Hands-on:**  \n",
    "- Deploy a **serverless ML model on AWS Lambda**  \n",
    "- Set up **MLOps with MLflow**  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's break down **Big Data Processing Frameworks** in detail, covering **Apache Spark, Dask, and other tools** like **Kafka, Flink, and Hadoop**, along with **Python code snippets** to demonstrate their usage.  \n",
    "\n",
    "---\n",
    "\n",
    "# **🚀 Big Data Processing Frameworks**\n",
    "\n",
    "## **🔹 1. Apache Spark (Distributed Data Processing)**\n",
    "Apache Spark is an open-source, distributed computing system that processes large datasets in parallel across multiple nodes.  \n",
    "\n",
    "### ✅ **Spark Architecture**\n",
    "Spark operates using a **Resilient Distributed Dataset (RDD)** and a **DAG (Directed Acyclic Graph)** execution model.\n",
    "\n",
    "- **RDD (Resilient Distributed Dataset)** → Immutable, distributed collection of data processed in parallel.  \n",
    "- **DAG (Directed Acyclic Graph)** → Logical execution plan Spark follows for transformations.  \n",
    "- **Lazy Evaluation** → Computation is not executed until an action (like `.collect()`) is called.  \n",
    "\n",
    "**🔹 Example: Creating an RDD in PySpark**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder.appName(\"BigDataProcessing\").getOrCreate()\n",
    "\n",
    "# Create an RDD\n",
    "data = [(\"Alice\", 30), (\"Bob\", 25), (\"Charlie\", 35)]\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "\n",
    "# Perform transformation\n",
    "filtered_rdd = rdd.filter(lambda x: x[1] > 28)\n",
    "\n",
    "# Trigger execution with an action\n",
    "print(filtered_rdd.collect())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ **Spark DataFrames & SQL**\n",
    "Spark DataFrames are optimized, distributed versions of pandas DataFrames. SparkSQL allows querying using SQL syntax.\n",
    "\n",
    "**🔹 Example: Working with DataFrames & SQL in Spark**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder.appName(\"SparkSQLExample\").getOrCreate()\n",
    "\n",
    "# Create DataFrame\n",
    "data = [(\"Alice\", 30), (\"Bob\", 25), (\"Charlie\", 35)]\n",
    "columns = [\"Name\", \"Age\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# SQL Queries\n",
    "df.createOrReplaceTempView(\"people\")\n",
    "result = spark.sql(\"SELECT * FROM people WHERE Age > 28\")\n",
    "result.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ **Spark MLlib (Machine Learning on Spark)**\n",
    "MLlib is Spark’s built-in machine learning library that supports large-scale ML.\n",
    "\n",
    "**🔹 Example: Linear Regression using Spark MLlib**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Sample data\n",
    "data = [(1, 2.0), (2, 2.5), (3, 3.0), (4, 3.5), (5, 4.0)]\n",
    "df = spark.createDataFrame(data, [\"Feature\", \"Label\"])\n",
    "\n",
    "# Convert features into vector\n",
    "assembler = VectorAssembler(inputCols=[\"Feature\"], outputCol=\"Features\")\n",
    "df = assembler.transform(df)\n",
    "\n",
    "# Train Linear Regression model\n",
    "lr = LinearRegression(featuresCol=\"Features\", labelCol=\"Label\")\n",
    "model = lr.fit(df)\n",
    "\n",
    "# Predict\n",
    "predictions = model.transform(df)\n",
    "predictions.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ **Optimizations in Spark (Partitioning, Caching, Broadcasting)**\n",
    "- **Partitioning** → Distributes data across nodes to balance load.  \n",
    "- **Caching** → Stores data in memory for faster access.  \n",
    "- **Broadcasting** → Distributes small datasets efficiently across nodes.\n",
    "\n",
    "**🔹 Example: Caching a DataFrame**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.cache()\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "## **🔹 2. Dask (Parallel Computing in Python)**\n",
    "Dask is a parallel computing framework that scales Pandas, NumPy, and Scikit-Learn workflows.\n",
    "\n",
    "### ✅ **Dask vs. Spark**\n",
    "| Feature | Apache Spark | Dask |\n",
    "|---------|-------------|------|\n",
    "| Language | Scala, Java, Python | Python |\n",
    "| Parallelism | Distributed Cluster | Single/Multi-core & Cluster |\n",
    "| Data Structures | RDD, DataFrame | Dask DataFrame, Array |\n",
    "| Use Case | Big Data Processing | Scaling Python workflows |\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ **Dask DataFrames & Arrays**\n",
    "Dask DataFrames are parallel versions of Pandas DataFrames.\n",
    "\n",
    "**🔹 Example: Dask DataFrame**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import dask.dataframe as dd\n",
    "\n",
    "# Load large dataset\n",
    "df = dd.read_csv(\"large_dataset.csv\")\n",
    "\n",
    "# Perform operations\n",
    "df_filtered = df[df[\"column_name\"] > 100]\n",
    "df_filtered.compute()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**🔹 Example: Dask Arrays (Scaling NumPy)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import dask.array as da\n",
    "\n",
    "# Create large array\n",
    "arr = da.random.random((10000, 10000), chunks=(1000, 1000))\n",
    "\n",
    "# Compute mean\n",
    "print(arr.mean().compute())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ **Parallel ML Training with Dask-ML**\n",
    "Dask-ML scales Scikit-Learn models across multiple cores.\n",
    "\n",
    "**🔹 Example: Parallel Random Forest with Dask-ML**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from dask_ml.ensemble import RandomForestClassifier\n",
    "import dask.dataframe as dd\n",
    "\n",
    "# Load dataset\n",
    "df = dd.read_csv(\"large_data.csv\")\n",
    "\n",
    "# Train model\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(df.iloc[:, :-1], df.iloc[:, -1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "## **🔹 3. Other Big Data Tools**\n",
    "\n",
    "### ✅ **Apache Flink – Real-time stream processing**\n",
    "Flink is a distributed stream processing framework.\n",
    "\n",
    "### ✅ **Apache Kafka – Message Streaming for Big Data**\n",
    "Kafka is a message broker for real-time data pipelines.\n",
    "\n",
    "**🔹 Example: Producing and Consuming Kafka Messages**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from kafka import KafkaProducer, KafkaConsumer\n",
    "\n",
    "# Producer (Sending Data)\n",
    "producer = KafkaProducer(bootstrap_servers='localhost:9092')\n",
    "producer.send('test_topic', b'Hello Kafka!')\n",
    "\n",
    "# Consumer (Receiving Data)\n",
    "consumer = KafkaConsumer('test_topic', bootstrap_servers='localhost:9092')\n",
    "for msg in consumer:\n",
    "    print(msg.value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ **Hadoop & HDFS – Distributed Storage & Processing**\n",
    "Hadoop provides a distributed file system (HDFS) for storing large datasets.\n",
    "\n",
    "**🔹 Example: Accessing HDFS from Python**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from hdfs import InsecureClient\n",
    "\n",
    "client = InsecureClient('http://localhost:50070', user='hdfs')\n",
    "client.list('/')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# **🛠 Hands-on Projects**\n",
    "1️⃣ **Process large datasets using Spark & Dask**  \n",
    "2️⃣ **Build a real-time data pipeline using Kafka**  \n",
    "3️⃣ **Train a scalable ML model using Spark MLlib**  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **🚀 Scalable Machine Learning (ML) on Big Data**  \n",
    "When dealing with **large-scale ML**, standard tools like **Scikit-Learn** often fail due to **memory constraints**. This is where **Spark MLlib, Dask-ML, and Streaming ML** come in, enabling **parallel, distributed, and incremental learning** for handling large datasets.\n",
    "\n",
    "---\n",
    "\n",
    "# **1️⃣ Machine Learning with Spark MLlib**\n",
    "**Spark MLlib** is a **distributed machine learning library** built on Apache Spark, allowing **large-scale ML training** across clusters.\n",
    "\n",
    "### **✅ Feature Engineering at Scale**\n",
    "Spark MLlib provides **feature transformations, scaling, and vectorization** for large datasets.\n",
    "\n",
    "**🔹 Example: Feature Engineering in Spark**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "\n",
    "# Start Spark Session\n",
    "spark = SparkSession.builder.appName(\"FeatureEngineering\").getOrCreate()\n",
    "\n",
    "# Sample Data\n",
    "data = [(1, 2.0, 3.0), (2, 5.0, 6.0), (3, 10.0, 15.0)]\n",
    "columns = [\"ID\", \"Feature1\", \"Feature2\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Combine features into a single vector\n",
    "assembler = VectorAssembler(inputCols=[\"Feature1\", \"Feature2\"], outputCol=\"features\")\n",
    "df_vectorized = assembler.transform(df)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\")\n",
    "scaler_model = scaler.fit(df_vectorized)\n",
    "df_scaled = scaler_model.transform(df_vectorized)\n",
    "\n",
    "df_scaled.select(\"scaled_features\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **✅ Training ML Models (Regression, Classification)**\n",
    "Spark MLlib supports **linear regression, decision trees, random forests, and gradient boosting**.\n",
    "\n",
    "**🔹 Example: Train a Logistic Regression Model**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Training Data\n",
    "training_data = spark.createDataFrame([(0.0, [0.0, 1.1, 0.1]),\n",
    "                                       (1.0, [2.0, 1.0, -1.0]),\n",
    "                                       (0.0, [2.0, 1.3, 1.0]),\n",
    "                                       (1.0, [0.0, 1.2, -0.5])],\n",
    "                                      [\"label\", \"features\"])\n",
    "\n",
    "# Train Model\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.01)\n",
    "model = lr.fit(training_data)\n",
    "\n",
    "# Predict\n",
    "predictions = model.transform(training_data)\n",
    "predictions.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **✅ Hyperparameter Tuning in Spark**\n",
    "**Grid Search** and **Cross-validation** can be done efficiently using Spark ML’s `ParamGridBuilder` and `CrossValidator`.\n",
    "\n",
    "**🔹 Example: Hyperparameter Tuning in Spark**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Define hyperparameter grid\n",
    "param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.01, 0.1, 1.0]) \\\n",
    "    .build()\n",
    "\n",
    "# Cross-validation\n",
    "cv = CrossValidator(estimator=lr,\n",
    "                    estimatorParamMaps=param_grid,\n",
    "                    evaluator=BinaryClassificationEvaluator(),\n",
    "                    numFolds=3)\n",
    "\n",
    "cv_model = cv.fit(training_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **✅ Deploying Spark ML Models**\n",
    "Spark ML models can be **saved** and **deployed** for predictions at scale.\n",
    "\n",
    "**🔹 Example: Save & Load Spark Model**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save model\n",
    "model.save(\"logistic_regression_model\")\n",
    "\n",
    "# Load model\n",
    "from pyspark.ml.classification import LogisticRegressionModel\n",
    "loaded_model = LogisticRegressionModel.load(\"logistic_regression_model\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# **2️⃣ Scalable ML with Dask-ML**\n",
    "Dask-ML scales **Scikit-Learn models** to large datasets by distributing computations.\n",
    "\n",
    "### **✅ Parallel Hyperparameter Tuning**\n",
    "Dask enables **parallel search** for hyperparameter tuning.\n",
    "\n",
    "**🔹 Example: Parallel Grid Search with Dask**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from dask_ml.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import dask.array as da\n",
    "\n",
    "# Generate large dataset\n",
    "X = da.random.random((10000, 20), chunks=(1000, 20))\n",
    "y = da.random.randint(0, 2, size=10000, chunks=1000)\n",
    "\n",
    "# Define Model & Hyperparameters\n",
    "clf = RandomForestClassifier()\n",
    "param_grid = {\"n_estimators\": [10, 50, 100], \"max_depth\": [5, 10, None]}\n",
    "\n",
    "# Perform Grid Search\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=3)\n",
    "grid_search.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **✅ Scaling Scikit-Learn with Dask**\n",
    "Dask extends Scikit-Learn to large datasets without loading everything into memory.\n",
    "\n",
    "**🔹 Example: Large Dataset ML Training with Dask**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import dask.dataframe as dd\n",
    "from dask_ml.linear_model import LogisticRegression\n",
    "\n",
    "# Load large dataset\n",
    "df = dd.read_csv(\"large_dataset.csv\")\n",
    "\n",
    "# Train model\n",
    "lr = LogisticRegression()\n",
    "lr.fit(df.iloc[:, :-1], df.iloc[:, -1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "# **3️⃣ Online & Incremental Learning**\n",
    "Instead of training models on **static data**, **incremental learning** allows updating models **dynamically**.\n",
    "\n",
    "## **✅ Streaming ML with Spark Streaming**\n",
    "Spark Streaming enables **real-time model updates** with incoming data.\n",
    "\n",
    "**🔹 Example: Streaming ML with Spark**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "# Streaming DataFrame\n",
    "df_stream = spark.readStream.format(\"csv\").option(\"header\", \"true\").load(\"streaming_data/\")\n",
    "\n",
    "# Train Model on Incoming Data\n",
    "dt = DecisionTreeClassifier(featuresCol=\"features\", labelCol=\"label\")\n",
    "model = dt.fit(df_stream)\n",
    "\n",
    "# Predict on Stream\n",
    "predictions = model.transform(df_stream)\n",
    "predictions.writeStream.outputMode(\"append\").format(\"console\").start().awaitTermination()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "## **✅ Incremental Learning (River, Vowpal Wabbit)**\n",
    "River is a Python library for **online learning** (updating ML models incrementally).\n",
    "\n",
    "**🔹 Example: Online Learning with River**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from river import linear_model, preprocessing\n",
    "\n",
    "# Initialize model\n",
    "model = preprocessing.StandardScaler() | linear_model.LogisticRegression()\n",
    "\n",
    "# Stream new data and update model\n",
    "for x, y in stream:  # stream is incoming data\n",
    "    model.learn_one(x, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "## **✅ Federated Learning (Google’s FedAvg, PySyft)**\n",
    "Federated Learning trains models **across multiple devices** without sending data to a central server.\n",
    "\n",
    "**🔹 Example: Federated Learning with PySyft**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import syft as sy\n",
    "\n",
    "# Create virtual workers\n",
    "alice = sy.VirtualWorker(hook, id=\"alice\")\n",
    "bob = sy.VirtualWorker(hook, id=\"bob\")\n",
    "\n",
    "# Train model across distributed nodes\n",
    "model.send(alice)\n",
    "output = model(input_data)\n",
    "model.get()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# **🛠 Hands-on Projects**\n",
    "✅ **Train a large-scale ML model using Spark MLlib**  \n",
    "✅ **Parallel hyperparameter tuning with Dask-ML**  \n",
    "✅ **Implement incremental learning for streaming data**  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **🚀 Cloud Machine Learning: AWS, GCP, Azure**  \n",
    "Cloud platforms provide **scalable, managed ML services** that allow **training, deployment, and monitoring of ML models** without managing infrastructure.  \n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "# **1️⃣ AWS Machine Learning**  \n",
    "\n",
    "## ✅ **Amazon SageMaker** – Train & Deploy ML Models  \n",
    "Amazon SageMaker is a **fully managed** ML platform for training, tuning, and deploying ML models.  \n",
    "\n",
    "### **🔹 Features of SageMaker**  \n",
    "- **Managed Jupyter Notebooks** – Develop models interactively.  \n",
    "- **Built-in Algorithms** – Prebuilt ML models (XGBoost, Linear Regression, etc.).  \n",
    "- **AutoML** – Automatic model tuning with SageMaker Autopilot.  \n",
    "- **Hyperparameter Optimization (HPO)** – Optimize models using Bayesian search.  \n",
    "- **Model Deployment** – Deploy models as REST APIs with **SageMaker Endpoints**.  \n",
    "\n",
    "### **🔹 Example: Training a Model in SageMaker**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import boto3  \n",
    "import sagemaker  \n",
    "from sagemaker import get_execution_role  \n",
    "\n",
    "# Initialize SageMaker session  \n",
    "sagemaker_session = sagemaker.Session()  \n",
    "role = get_execution_role()  \n",
    "\n",
    "# Define training parameters  \n",
    "container = sagemaker.image_uris.retrieve(\"xgboost\", boto3.Session().region_name, \"latest\")  \n",
    "\n",
    "# Create an estimator  \n",
    "xgb = sagemaker.estimator.Estimator(container,  \n",
    "                                    role,  \n",
    "                                    instance_count=1,  \n",
    "                                    instance_type=\"ml.m5.large\",  \n",
    "                                    output_path=\"s3://your-bucket/output\",  \n",
    "                                    sagemaker_session=sagemaker_session)  \n",
    "\n",
    "# Set hyperparameters  \n",
    "xgb.set_hyperparameters(objective=\"binary:logistic\", num_round=100)  \n",
    "\n",
    "# Train the model  \n",
    "xgb.fit({\"train\": \"s3://your-bucket/train.csv\"})  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ **AWS Lambda & API Gateway** – Serverless ML  \n",
    "AWS Lambda lets you **run ML inference serverlessly** without managing servers.  \n",
    "\n",
    "### **🔹 Deploy ML Model as Serverless API**\n",
    "1. Train an ML model in **SageMaker** or **Scikit-Learn**.  \n",
    "2. Save the model to **Amazon S3**.  \n",
    "3. Deploy as **Lambda Function** with **API Gateway**.  \n",
    "\n",
    "### **🔹 Example: Deploy ML Model with Lambda**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "import boto3\n",
    "import pickle\n",
    "\n",
    "# Load model from S3\n",
    "s3 = boto3.client('s3')\n",
    "bucket = \"your-bucket\"\n",
    "model_key = \"model.pkl\"\n",
    "response = s3.get_object(Bucket=bucket, Key=model_key)\n",
    "model = pickle.loads(response['Body'].read())\n",
    "\n",
    "# Define Lambda function\n",
    "def lambda_handler(event, context):\n",
    "    data = json.loads(event['body'])\n",
    "    prediction = model.predict([data['features']])\n",
    "    return {\"statusCode\": 200, \"body\": json.dumps({\"prediction\": prediction.tolist()})}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ **AWS EMR (Elastic MapReduce)** – Running Spark on AWS  \n",
    "**EMR** allows running **Apache Spark, Hadoop, and Presto** on AWS.  \n",
    "\n",
    "### **🔹 Example: Running a Spark Job on EMR**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark on EMR\n",
    "spark = SparkSession.builder.appName(\"AWS_EMR_Spark\").getOrCreate()\n",
    "\n",
    "# Load Data\n",
    "df = spark.read.csv(\"s3://your-bucket/data.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Perform Transformations\n",
    "df.groupBy(\"category\").count().show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "# **2️⃣ Google Cloud AI**  \n",
    "\n",
    "## ✅ **Vertex AI** – End-to-End ML Platform  \n",
    "**Vertex AI** provides **AutoML, custom training, model deployment, and monitoring** in a single platform.\n",
    "\n",
    "### **🔹 Example: Training a Model in Vertex AI**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "# Initialize Vertex AI client\n",
    "aiplatform.init(project=\"your-project-id\", location=\"us-central1\")\n",
    "\n",
    "# Train model\n",
    "model = aiplatform.CustomTrainingJob(\n",
    "    display_name=\"my-model\",\n",
    "    script_path=\"train.py\",\n",
    "    container_uri=\"gcr.io/cloud-aiplatform/training/tf-cpu.2-2\",\n",
    "    model_serving_container_image_uri=\"gcr.io/cloud-aiplatform/prediction/tf2-cpu.2-2\"\n",
    ")\n",
    "\n",
    "model.run(replica_count=1, machine_type=\"n1-standard-4\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ **BigQuery ML** – Train ML Models Inside BigQuery  \n",
    "**BigQuery ML** enables SQL-based ML model training **directly inside BigQuery** without exporting data.\n",
    "\n",
    "### **🔹 Example: Train a Regression Model in BigQuery**\n",
    "```sql\n",
    "CREATE OR REPLACE MODEL my_project.my_dataset.model_name  \n",
    "OPTIONS(model_type='linear_reg') AS  \n",
    "SELECT feature1, feature2, target FROM my_project.my_dataset.train_data;\n",
    "```\n",
    "\n",
    "### **🔹 Example: Make Predictions with BigQuery ML**\n",
    "```sql\n",
    "SELECT *  \n",
    "FROM ML.PREDICT(MODEL my_project.my_dataset.model_name,  \n",
    "                (SELECT feature1, feature2 FROM my_project.my_dataset.test_data));\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## ✅ **Dataflow & Dataproc** – Serverless Spark & Hadoop  \n",
    "- **Dataflow**: Managed **Apache Beam** for real-time and batch processing.  \n",
    "- **Dataproc**: Fully managed **Apache Spark & Hadoop** on Google Cloud.  \n",
    "\n",
    "### **🔹 Example: Running a Data Pipeline on Dataflow**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import apache_beam as beam\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "\n",
    "options = PipelineOptions()\n",
    "p = beam.Pipeline(options=options)\n",
    "\n",
    "(p | \"Read Data\" >> beam.io.ReadFromText(\"gs://your-bucket/data.csv\")\n",
    "   | \"Transform Data\" >> beam.Map(lambda x: x.upper())\n",
    "   | \"Write Output\" >> beam.io.WriteToText(\"gs://your-bucket/output.txt\"))\n",
    "\n",
    "p.run()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# **3️⃣ Microsoft Azure AI**  \n",
    "\n",
    "## ✅ **Azure Machine Learning Studio**  \n",
    "Azure ML Studio provides a **drag-and-drop interface** and **Python SDK** for ML training and deployment.\n",
    "\n",
    "### **🔹 Example: Train Model in Azure ML**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from azureml.core import Workspace, Experiment, ScriptRunConfig\n",
    "\n",
    "# Connect to Azure ML Workspace\n",
    "ws = Workspace.from_config()\n",
    "\n",
    "# Define experiment\n",
    "experiment = Experiment(ws, name=\"my_experiment\")\n",
    "config = ScriptRunConfig(source_directory=\".\", script=\"train.py\")\n",
    "\n",
    "# Run experiment\n",
    "run = experiment.submit(config)\n",
    "run.wait_for_completion(show_output=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## ✅ **Azure Databricks** – Cloud-Based Spark  \n",
    "Azure Databricks is a **managed Spark environment** on Azure for **Big Data & ML**.\n",
    "\n",
    "### **🔹 Example: Train a Spark ML Model in Azure Databricks**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Azure_Databricks\").getOrCreate()\n",
    "df = spark.read.csv(\"dbfs:/mnt/data.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Train Decision Tree\n",
    "dt = DecisionTreeClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
    "model = dt.fit(df)\n",
    "\n",
    "# Predict\n",
    "predictions = model.transform(df)\n",
    "predictions.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ **ML Pipelines & MLOps in Azure**  \n",
    "Azure provides **MLOps capabilities** using **Azure DevOps, ML Pipelines, and Model Monitoring**.\n",
    "\n",
    "---\n",
    "\n",
    "# **🛠 Hands-on Projects**  \n",
    "✅ **Deploy an ML model using AWS SageMaker**  \n",
    "✅ **Train an ML model in BigQuery ML**  \n",
    "✅ **Build an MLOps pipeline using Azure ML**  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's break down **MLOps & Model Deployment at Scale** into detailed sections with explanations and code snippets.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 **1. MLOps for Scalable ML**\n",
    "### ✅ **1.1 CI/CD for ML (Automating ML Pipelines)**\n",
    "MLOps integrates DevOps practices into ML workflows to automate and streamline model training, validation, deployment, and monitoring.\n",
    "\n",
    "📌 **Key Steps in ML CI/CD Pipeline**  \n",
    "1. **Data Versioning** – Store datasets with DVC or MLflow.  \n",
    "2. **Model Training & Validation** – Train models using automated pipelines.  \n",
    "3. **Model Packaging** – Convert models to deployable formats (Pickle, ONNX).  \n",
    "4. **Model Deployment** – Deploy as APIs or in production environments.  \n",
    "5. **Continuous Monitoring** – Track model drift and retrain models when needed.  \n",
    "\n",
    "🛠 **Example: CI/CD with GitHub Actions & MLflow**  \n",
    "```yaml\n",
    "name: ML Pipeline\n",
    "\n",
    "on: [push]\n",
    "\n",
    "jobs:\n",
    "  train:\n",
    "    runs-on: ubuntu-latest\n",
    "    steps:\n",
    "      - name: Checkout Repository\n",
    "        uses: actions/checkout@v2\n",
    "      \n",
    "      - name: Set up Python\n",
    "        uses: actions/setup-python@v2\n",
    "        with:\n",
    "          python-version: '3.8'\n",
    "      \n",
    "      - name: Install Dependencies\n",
    "        run: pip install -r requirements.txt\n",
    "      \n",
    "      - name: Train Model\n",
    "        run: python train.py\n",
    "\n",
    "      - name: Log Model with MLflow\n",
    "        run: python log_model.py\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ **1.2 Model Versioning & Monitoring (MLflow, Kubeflow)**\n",
    "#### 📌 **Model Versioning with MLflow**\n",
    "MLflow helps track experiments, store models, and deploy them.\n",
    "\n",
    "🛠 **Example: Logging a Model in MLflow**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "mlflow.set_experiment(\"RandomForest_Experiment\")\n",
    "\n",
    "with mlflow.start_run():\n",
    "    model = RandomForestClassifier(n_estimators=100)\n",
    "    mlflow.sklearn.log_model(model, \"random_forest_model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### 📌 **Model Monitoring & Drift Detection**\n",
    "- **MLflow Tracking** logs metrics, parameters, and artifacts.  \n",
    "- **Prometheus + Grafana** can track model latency and performance.  \n",
    "- **Evidently AI** detects model drift.  \n",
    "\n",
    "🛠 **Example: Monitor Data Drift with Evidently**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from evidently.test_suite import TestSuite\n",
    "from evidently.tests import TestDataDrift\n",
    "\n",
    "suite = TestSuite(tests=[TestDataDrift()])\n",
    "suite.run(reference_data=ref_df, current_data=curr_df)\n",
    "suite.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ **1.3 A/B Testing & Model Drift Detection**\n",
    "- **A/B Testing** helps compare model versions in production.\n",
    "- **Model Drift** happens when data distributions change over time.\n",
    "\n",
    "🛠 **Example: A/B Testing API with FastAPI**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from fastapi import FastAPI\n",
    "import random\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "@app.get(\"/predict\")\n",
    "def predict():\n",
    "    model_version = random.choice([\"v1\", \"v2\"])\n",
    "    return {\"model_version\": model_version, \"prediction\": 0.85}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 **2. Deploying ML Models at Scale**\n",
    "### ✅ **2.1 Serverless Deployment (FastAPI, AWS Lambda)**\n",
    "Serverless deployment makes ML models accessible via APIs.\n",
    "\n",
    "🛠 **Example: Deploying a FastAPI Model**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "import pickle\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "model = pickle.load(open(\"model.pkl\", \"rb\"))\n",
    "\n",
    "@app.get(\"/predict\")\n",
    "def predict(x: float):\n",
    "    return {\"prediction\": model.predict([[x]])[0]}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "🛠 **Example: Deploy Model on AWS Lambda**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "import joblib\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    model = joblib.load(\"/tmp/model.pkl\")\n",
    "    input_data = json.loads(event['body'])\n",
    "    prediction = model.predict([input_data['features']])\n",
    "    return {\"statusCode\": 200, \"body\": json.dumps({\"prediction\": prediction.tolist()})}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ **2.2 Kubernetes for ML (Kubeflow)**\n",
    "Kubernetes (K8s) helps in scaling ML models.\n",
    "\n",
    "📌 **Kubeflow Features**\n",
    "- **Pipeline Orchestration** – Automate workflows.  \n",
    "- **Model Serving** – Deploy models using KFServing.  \n",
    "- **Hyperparameter Tuning** – Use Katib for tuning.  \n",
    "\n",
    "🛠 **Example: Deploy Model using KFServing**\n",
    "```yaml\n",
    "apiVersion: serving.kubeflow.org/v1alpha2\n",
    "kind: InferenceService\n",
    "metadata:\n",
    "  name: my-model\n",
    "spec:\n",
    "  default:\n",
    "    predictor:\n",
    "      tensorflow:\n",
    "        storageUri: \"gs://my-bucket/my-model/\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ **2.3 Inference Optimization (ONNX, TensorRT)**\n",
    "Optimizing models improves speed and efficiency.\n",
    "\n",
    "#### 📌 **ONNX for Model Optimization**\n",
    "ONNX enables cross-platform inference with optimized performance.\n",
    "\n",
    "🛠 **Convert a PyTorch Model to ONNX**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "\n",
    "model = models.resnet18(pretrained=True)\n",
    "dummy_input = torch.randn(1, 3, 224, 224)\n",
    "torch.onnx.export(model, dummy_input, \"resnet18.onnx\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### 📌 **Optimize Inference with TensorRT**\n",
    "NVIDIA TensorRT accelerates deep learning inference.\n",
    "\n",
    "🛠 **Convert Model to TensorRT**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorrt as trt\n",
    "\n",
    "TRT_LOGGER = trt.Logger(trt.Logger.WARNING)\n",
    "builder = trt.Builder(TRT_LOGGER)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
