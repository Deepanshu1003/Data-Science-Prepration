{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üöÄ **Big Data & Cloud ML: Topics & Subtopics**  \n",
    "\n",
    "This will cover **Big Data processing**, **scalable machine learning**, and **cloud-based AI services** to handle large-scale data and deploy ML models effectively.  \n",
    "\n",
    "---\n",
    "\n",
    "## **üîπ 1. Big Data Processing Frameworks**  \n",
    "These tools help handle large datasets efficiently.  \n",
    "\n",
    "### ‚úÖ **Apache Spark** (Distributed Data Processing)  \n",
    "- Spark Architecture (RDD, DAG, Lazy Evaluation)  \n",
    "- Spark DataFrames & SQL  \n",
    "- Spark MLlib (Machine Learning on Spark)  \n",
    "- Optimizations (Partitioning, Caching, Broadcasting)  \n",
    "\n",
    "### ‚úÖ **Dask** (Parallel Computing in Python)  \n",
    "- Dask vs. Spark  \n",
    "- Dask DataFrames & Arrays  \n",
    "- Scaling Pandas Workflows  \n",
    "- Parallel ML Training with Dask-ML  \n",
    "\n",
    "### ‚úÖ **Other Big Data Tools**  \n",
    "- **Apache Flink** ‚Äì Real-time stream processing  \n",
    "- **Apache Kafka** ‚Äì Message streaming for Big Data  \n",
    "- **Hadoop & HDFS** ‚Äì Distributed storage & processing  \n",
    "\n",
    "üõ† **Hands-on:**  \n",
    "- Process large datasets with **Spark & Dask**  \n",
    "- Build a **real-time stream processing pipeline with Kafka**  \n",
    "\n",
    "---\n",
    "\n",
    "## **üîπ 2. Scalable Machine Learning (ML) on Big Data**  \n",
    "How to train & deploy ML models efficiently on large datasets.  \n",
    "\n",
    "### ‚úÖ **Machine Learning with Spark MLlib**  \n",
    "- Feature Engineering at Scale  \n",
    "- Training ML Models (Regression, Classification)  \n",
    "- Hyperparameter Tuning in Spark  \n",
    "- Deploying Spark ML Models  \n",
    "\n",
    "### ‚úÖ **Dask-ML for Scalable ML**  \n",
    "- Parallel Hyperparameter Tuning  \n",
    "- Scaling Scikit-Learn with Dask  \n",
    "- Model Training on Large Datasets  \n",
    "\n",
    "### ‚úÖ **Online & Incremental Learning**  \n",
    "- Streaming ML with Spark Streaming  \n",
    "- Incremental Learning (River, Vowpal Wabbit)  \n",
    "- Federated Learning (Google‚Äôs FedAvg, PySyft)  \n",
    "\n",
    "üõ† **Hands-on:**  \n",
    "- Train a **large-scale ML model using Spark MLlib**  \n",
    "- Implement **incremental learning for streaming data**  \n",
    "\n",
    "---\n",
    "\n",
    "## **üîπ 3. Cloud ML: AWS, GCP, Azure**  \n",
    "Deploying ML models in the cloud using **managed AI services**.  \n",
    "\n",
    "### ‚úÖ **AWS Machine Learning**  \n",
    "- **SageMaker** ‚Äì Train & deploy ML models  \n",
    "- **AWS Lambda & API Gateway** ‚Äì Serverless ML  \n",
    "- **EMR (Elastic MapReduce)** ‚Äì Run Spark on AWS  \n",
    "\n",
    "### ‚úÖ **Google Cloud AI**  \n",
    "- **Vertex AI** ‚Äì End-to-end ML platform  \n",
    "- **BigQuery ML** ‚Äì Train ML models inside BigQuery  \n",
    "- **Dataflow & Dataproc** ‚Äì Serverless Spark & Hadoop  \n",
    "\n",
    "### ‚úÖ **Microsoft Azure AI**  \n",
    "- **Azure Machine Learning Studio**  \n",
    "- **Azure Databricks** (Cloud-based Spark)  \n",
    "- **ML Pipelines & MLOps in Azure**  \n",
    "\n",
    "üõ† **Hands-on:**  \n",
    "- Deploy an **ML model using AWS SageMaker**  \n",
    "- Train an **ML model in BigQuery ML**  \n",
    "\n",
    "---\n",
    "\n",
    "## **üîπ 4. MLOps & Model Deployment at Scale**  \n",
    "How to deploy and manage ML models in production.  \n",
    "\n",
    "### ‚úÖ **MLOps for Scalable ML**  \n",
    "- **CI/CD for ML** ‚Äì Automate ML pipelines  \n",
    "- **Model Versioning & Monitoring** (MLflow, Kubeflow)  \n",
    "- **A/B Testing & Model Drift Detection**  \n",
    "\n",
    "### ‚úÖ **Deploying ML Models at Scale**  \n",
    "- **Serverless Deployment** (FastAPI, AWS Lambda)  \n",
    "- **Kubernetes for ML (Kubeflow)**  \n",
    "- **Inference Optimization** (ONNX, TensorRT)  \n",
    "\n",
    "üõ† **Hands-on:**  \n",
    "- Deploy a **serverless ML model on AWS Lambda**  \n",
    "- Set up **MLOps with MLflow**  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's break down **Big Data Processing Frameworks** in detail, covering **Apache Spark, Dask, and other tools** like **Kafka, Flink, and Hadoop**, along with **Python code snippets** to demonstrate their usage.  \n",
    "\n",
    "---\n",
    "\n",
    "# **üöÄ Big Data Processing Frameworks**\n",
    "\n",
    "## **üîπ 1. Apache Spark (Distributed Data Processing)**\n",
    "Apache Spark is an open-source, distributed computing system that processes large datasets in parallel across multiple nodes.  \n",
    "\n",
    "### ‚úÖ **Spark Architecture**\n",
    "Spark operates using a **Resilient Distributed Dataset (RDD)** and a **DAG (Directed Acyclic Graph)** execution model.\n",
    "\n",
    "- **RDD (Resilient Distributed Dataset)** ‚Üí Immutable, distributed collection of data processed in parallel.  \n",
    "- **DAG (Directed Acyclic Graph)** ‚Üí Logical execution plan Spark follows for transformations.  \n",
    "- **Lazy Evaluation** ‚Üí Computation is not executed until an action (like `.collect()`) is called.  \n",
    "\n",
    "**üîπ Example: Creating an RDD in PySpark**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder.appName(\"BigDataProcessing\").getOrCreate()\n",
    "\n",
    "# Create an RDD\n",
    "data = [(\"Alice\", 30), (\"Bob\", 25), (\"Charlie\", 35)]\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "\n",
    "# Perform transformation\n",
    "filtered_rdd = rdd.filter(lambda x: x[1] > 28)\n",
    "\n",
    "# Trigger execution with an action\n",
    "print(filtered_rdd.collect())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ **Spark DataFrames & SQL**\n",
    "Spark DataFrames are optimized, distributed versions of pandas DataFrames. SparkSQL allows querying using SQL syntax.\n",
    "\n",
    "**üîπ Example: Working with DataFrames & SQL in Spark**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder.appName(\"SparkSQLExample\").getOrCreate()\n",
    "\n",
    "# Create DataFrame\n",
    "data = [(\"Alice\", 30), (\"Bob\", 25), (\"Charlie\", 35)]\n",
    "columns = [\"Name\", \"Age\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# SQL Queries\n",
    "df.createOrReplaceTempView(\"people\")\n",
    "result = spark.sql(\"SELECT * FROM people WHERE Age > 28\")\n",
    "result.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ **Spark MLlib (Machine Learning on Spark)**\n",
    "MLlib is Spark‚Äôs built-in machine learning library that supports large-scale ML.\n",
    "\n",
    "**üîπ Example: Linear Regression using Spark MLlib**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Sample data\n",
    "data = [(1, 2.0), (2, 2.5), (3, 3.0), (4, 3.5), (5, 4.0)]\n",
    "df = spark.createDataFrame(data, [\"Feature\", \"Label\"])\n",
    "\n",
    "# Convert features into vector\n",
    "assembler = VectorAssembler(inputCols=[\"Feature\"], outputCol=\"Features\")\n",
    "df = assembler.transform(df)\n",
    "\n",
    "# Train Linear Regression model\n",
    "lr = LinearRegression(featuresCol=\"Features\", labelCol=\"Label\")\n",
    "model = lr.fit(df)\n",
    "\n",
    "# Predict\n",
    "predictions = model.transform(df)\n",
    "predictions.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ **Optimizations in Spark (Partitioning, Caching, Broadcasting)**\n",
    "- **Partitioning** ‚Üí Distributes data across nodes to balance load.  \n",
    "- **Caching** ‚Üí Stores data in memory for faster access.  \n",
    "- **Broadcasting** ‚Üí Distributes small datasets efficiently across nodes.\n",
    "\n",
    "**üîπ Example: Caching a DataFrame**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.cache()\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "## **üîπ 2. Dask (Parallel Computing in Python)**\n",
    "Dask is a parallel computing framework that scales Pandas, NumPy, and Scikit-Learn workflows.\n",
    "\n",
    "### ‚úÖ **Dask vs. Spark**\n",
    "| Feature | Apache Spark | Dask |\n",
    "|---------|-------------|------|\n",
    "| Language | Scala, Java, Python | Python |\n",
    "| Parallelism | Distributed Cluster | Single/Multi-core & Cluster |\n",
    "| Data Structures | RDD, DataFrame | Dask DataFrame, Array |\n",
    "| Use Case | Big Data Processing | Scaling Python workflows |\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ **Dask DataFrames & Arrays**\n",
    "Dask DataFrames are parallel versions of Pandas DataFrames.\n",
    "\n",
    "**üîπ Example: Dask DataFrame**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import dask.dataframe as dd\n",
    "\n",
    "# Load large dataset\n",
    "df = dd.read_csv(\"large_dataset.csv\")\n",
    "\n",
    "# Perform operations\n",
    "df_filtered = df[df[\"column_name\"] > 100]\n",
    "df_filtered.compute()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**üîπ Example: Dask Arrays (Scaling NumPy)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import dask.array as da\n",
    "\n",
    "# Create large array\n",
    "arr = da.random.random((10000, 10000), chunks=(1000, 1000))\n",
    "\n",
    "# Compute mean\n",
    "print(arr.mean().compute())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ **Parallel ML Training with Dask-ML**\n",
    "Dask-ML scales Scikit-Learn models across multiple cores.\n",
    "\n",
    "**üîπ Example: Parallel Random Forest with Dask-ML**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from dask_ml.ensemble import RandomForestClassifier\n",
    "import dask.dataframe as dd\n",
    "\n",
    "# Load dataset\n",
    "df = dd.read_csv(\"large_data.csv\")\n",
    "\n",
    "# Train model\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(df.iloc[:, :-1], df.iloc[:, -1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "## **üîπ 3. Other Big Data Tools**\n",
    "\n",
    "### ‚úÖ **Apache Flink ‚Äì Real-time stream processing**\n",
    "Flink is a distributed stream processing framework.\n",
    "\n",
    "### ‚úÖ **Apache Kafka ‚Äì Message Streaming for Big Data**\n",
    "Kafka is a message broker for real-time data pipelines.\n",
    "\n",
    "**üîπ Example: Producing and Consuming Kafka Messages**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from kafka import KafkaProducer, KafkaConsumer\n",
    "\n",
    "# Producer (Sending Data)\n",
    "producer = KafkaProducer(bootstrap_servers='localhost:9092')\n",
    "producer.send('test_topic', b'Hello Kafka!')\n",
    "\n",
    "# Consumer (Receiving Data)\n",
    "consumer = KafkaConsumer('test_topic', bootstrap_servers='localhost:9092')\n",
    "for msg in consumer:\n",
    "    print(msg.value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ **Hadoop & HDFS ‚Äì Distributed Storage & Processing**\n",
    "Hadoop provides a distributed file system (HDFS) for storing large datasets.\n",
    "\n",
    "**üîπ Example: Accessing HDFS from Python**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from hdfs import InsecureClient\n",
    "\n",
    "client = InsecureClient('http://localhost:50070', user='hdfs')\n",
    "client.list('/')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# **üõ† Hands-on Projects**\n",
    "1Ô∏è‚É£ **Process large datasets using Spark & Dask**  \n",
    "2Ô∏è‚É£ **Build a real-time data pipeline using Kafka**  \n",
    "3Ô∏è‚É£ **Train a scalable ML model using Spark MLlib**  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **üöÄ Scalable Machine Learning (ML) on Big Data**  \n",
    "When dealing with **large-scale ML**, standard tools like **Scikit-Learn** often fail due to **memory constraints**. This is where **Spark MLlib, Dask-ML, and Streaming ML** come in, enabling **parallel, distributed, and incremental learning** for handling large datasets.\n",
    "\n",
    "---\n",
    "\n",
    "# **1Ô∏è‚É£ Machine Learning with Spark MLlib**\n",
    "**Spark MLlib** is a **distributed machine learning library** built on Apache Spark, allowing **large-scale ML training** across clusters.\n",
    "\n",
    "### **‚úÖ Feature Engineering at Scale**\n",
    "Spark MLlib provides **feature transformations, scaling, and vectorization** for large datasets.\n",
    "\n",
    "**üîπ Example: Feature Engineering in Spark**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "\n",
    "# Start Spark Session\n",
    "spark = SparkSession.builder.appName(\"FeatureEngineering\").getOrCreate()\n",
    "\n",
    "# Sample Data\n",
    "data = [(1, 2.0, 3.0), (2, 5.0, 6.0), (3, 10.0, 15.0)]\n",
    "columns = [\"ID\", \"Feature1\", \"Feature2\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Combine features into a single vector\n",
    "assembler = VectorAssembler(inputCols=[\"Feature1\", \"Feature2\"], outputCol=\"features\")\n",
    "df_vectorized = assembler.transform(df)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\")\n",
    "scaler_model = scaler.fit(df_vectorized)\n",
    "df_scaled = scaler_model.transform(df_vectorized)\n",
    "\n",
    "df_scaled.select(\"scaled_features\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **‚úÖ Training ML Models (Regression, Classification)**\n",
    "Spark MLlib supports **linear regression, decision trees, random forests, and gradient boosting**.\n",
    "\n",
    "**üîπ Example: Train a Logistic Regression Model**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Training Data\n",
    "training_data = spark.createDataFrame([(0.0, [0.0, 1.1, 0.1]),\n",
    "                                       (1.0, [2.0, 1.0, -1.0]),\n",
    "                                       (0.0, [2.0, 1.3, 1.0]),\n",
    "                                       (1.0, [0.0, 1.2, -0.5])],\n",
    "                                      [\"label\", \"features\"])\n",
    "\n",
    "# Train Model\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.01)\n",
    "model = lr.fit(training_data)\n",
    "\n",
    "# Predict\n",
    "predictions = model.transform(training_data)\n",
    "predictions.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **‚úÖ Hyperparameter Tuning in Spark**\n",
    "**Grid Search** and **Cross-validation** can be done efficiently using Spark ML‚Äôs `ParamGridBuilder` and `CrossValidator`.\n",
    "\n",
    "**üîπ Example: Hyperparameter Tuning in Spark**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Define hyperparameter grid\n",
    "param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.01, 0.1, 1.0]) \\\n",
    "    .build()\n",
    "\n",
    "# Cross-validation\n",
    "cv = CrossValidator(estimator=lr,\n",
    "                    estimatorParamMaps=param_grid,\n",
    "                    evaluator=BinaryClassificationEvaluator(),\n",
    "                    numFolds=3)\n",
    "\n",
    "cv_model = cv.fit(training_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **‚úÖ Deploying Spark ML Models**\n",
    "Spark ML models can be **saved** and **deployed** for predictions at scale.\n",
    "\n",
    "**üîπ Example: Save & Load Spark Model**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save model\n",
    "model.save(\"logistic_regression_model\")\n",
    "\n",
    "# Load model\n",
    "from pyspark.ml.classification import LogisticRegressionModel\n",
    "loaded_model = LogisticRegressionModel.load(\"logistic_regression_model\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# **2Ô∏è‚É£ Scalable ML with Dask-ML**\n",
    "Dask-ML scales **Scikit-Learn models** to large datasets by distributing computations.\n",
    "\n",
    "### **‚úÖ Parallel Hyperparameter Tuning**\n",
    "Dask enables **parallel search** for hyperparameter tuning.\n",
    "\n",
    "**üîπ Example: Parallel Grid Search with Dask**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from dask_ml.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import dask.array as da\n",
    "\n",
    "# Generate large dataset\n",
    "X = da.random.random((10000, 20), chunks=(1000, 20))\n",
    "y = da.random.randint(0, 2, size=10000, chunks=1000)\n",
    "\n",
    "# Define Model & Hyperparameters\n",
    "clf = RandomForestClassifier()\n",
    "param_grid = {\"n_estimators\": [10, 50, 100], \"max_depth\": [5, 10, None]}\n",
    "\n",
    "# Perform Grid Search\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=3)\n",
    "grid_search.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **‚úÖ Scaling Scikit-Learn with Dask**\n",
    "Dask extends Scikit-Learn to large datasets without loading everything into memory.\n",
    "\n",
    "**üîπ Example: Large Dataset ML Training with Dask**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import dask.dataframe as dd\n",
    "from dask_ml.linear_model import LogisticRegression\n",
    "\n",
    "# Load large dataset\n",
    "df = dd.read_csv(\"large_dataset.csv\")\n",
    "\n",
    "# Train model\n",
    "lr = LogisticRegression()\n",
    "lr.fit(df.iloc[:, :-1], df.iloc[:, -1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "# **3Ô∏è‚É£ Online & Incremental Learning**\n",
    "Instead of training models on **static data**, **incremental learning** allows updating models **dynamically**.\n",
    "\n",
    "## **‚úÖ Streaming ML with Spark Streaming**\n",
    "Spark Streaming enables **real-time model updates** with incoming data.\n",
    "\n",
    "**üîπ Example: Streaming ML with Spark**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "# Streaming DataFrame\n",
    "df_stream = spark.readStream.format(\"csv\").option(\"header\", \"true\").load(\"streaming_data/\")\n",
    "\n",
    "# Train Model on Incoming Data\n",
    "dt = DecisionTreeClassifier(featuresCol=\"features\", labelCol=\"label\")\n",
    "model = dt.fit(df_stream)\n",
    "\n",
    "# Predict on Stream\n",
    "predictions = model.transform(df_stream)\n",
    "predictions.writeStream.outputMode(\"append\").format(\"console\").start().awaitTermination()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "## **‚úÖ Incremental Learning (River, Vowpal Wabbit)**\n",
    "River is a Python library for **online learning** (updating ML models incrementally).\n",
    "\n",
    "**üîπ Example: Online Learning with River**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from river import linear_model, preprocessing\n",
    "\n",
    "# Initialize model\n",
    "model = preprocessing.StandardScaler() | linear_model.LogisticRegression()\n",
    "\n",
    "# Stream new data and update model\n",
    "for x, y in stream:  # stream is incoming data\n",
    "    model.learn_one(x, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "## **‚úÖ Federated Learning (Google‚Äôs FedAvg, PySyft)**\n",
    "Federated Learning trains models **across multiple devices** without sending data to a central server.\n",
    "\n",
    "**üîπ Example: Federated Learning with PySyft**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import syft as sy\n",
    "\n",
    "# Create virtual workers\n",
    "alice = sy.VirtualWorker(hook, id=\"alice\")\n",
    "bob = sy.VirtualWorker(hook, id=\"bob\")\n",
    "\n",
    "# Train model across distributed nodes\n",
    "model.send(alice)\n",
    "output = model(input_data)\n",
    "model.get()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# **üõ† Hands-on Projects**\n",
    "‚úÖ **Train a large-scale ML model using Spark MLlib**  \n",
    "‚úÖ **Parallel hyperparameter tuning with Dask-ML**  \n",
    "‚úÖ **Implement incremental learning for streaming data**  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **üöÄ Cloud Machine Learning: AWS, GCP, Azure**  \n",
    "Cloud platforms provide **scalable, managed ML services** that allow **training, deployment, and monitoring of ML models** without managing infrastructure.  \n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "# **1Ô∏è‚É£ AWS Machine Learning**  \n",
    "\n",
    "## ‚úÖ **Amazon SageMaker** ‚Äì Train & Deploy ML Models  \n",
    "Amazon SageMaker is a **fully managed** ML platform for training, tuning, and deploying ML models.  \n",
    "\n",
    "### **üîπ Features of SageMaker**  \n",
    "- **Managed Jupyter Notebooks** ‚Äì Develop models interactively.  \n",
    "- **Built-in Algorithms** ‚Äì Prebuilt ML models (XGBoost, Linear Regression, etc.).  \n",
    "- **AutoML** ‚Äì Automatic model tuning with SageMaker Autopilot.  \n",
    "- **Hyperparameter Optimization (HPO)** ‚Äì Optimize models using Bayesian search.  \n",
    "- **Model Deployment** ‚Äì Deploy models as REST APIs with **SageMaker Endpoints**.  \n",
    "\n",
    "### **üîπ Example: Training a Model in SageMaker**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import boto3  \n",
    "import sagemaker  \n",
    "from sagemaker import get_execution_role  \n",
    "\n",
    "# Initialize SageMaker session  \n",
    "sagemaker_session = sagemaker.Session()  \n",
    "role = get_execution_role()  \n",
    "\n",
    "# Define training parameters  \n",
    "container = sagemaker.image_uris.retrieve(\"xgboost\", boto3.Session().region_name, \"latest\")  \n",
    "\n",
    "# Create an estimator  \n",
    "xgb = sagemaker.estimator.Estimator(container,  \n",
    "                                    role,  \n",
    "                                    instance_count=1,  \n",
    "                                    instance_type=\"ml.m5.large\",  \n",
    "                                    output_path=\"s3://your-bucket/output\",  \n",
    "                                    sagemaker_session=sagemaker_session)  \n",
    "\n",
    "# Set hyperparameters  \n",
    "xgb.set_hyperparameters(objective=\"binary:logistic\", num_round=100)  \n",
    "\n",
    "# Train the model  \n",
    "xgb.fit({\"train\": \"s3://your-bucket/train.csv\"})  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ **AWS Lambda & API Gateway** ‚Äì Serverless ML  \n",
    "AWS Lambda lets you **run ML inference serverlessly** without managing servers.  \n",
    "\n",
    "### **üîπ Deploy ML Model as Serverless API**\n",
    "1. Train an ML model in **SageMaker** or **Scikit-Learn**.  \n",
    "2. Save the model to **Amazon S3**.  \n",
    "3. Deploy as **Lambda Function** with **API Gateway**.  \n",
    "\n",
    "### **üîπ Example: Deploy ML Model with Lambda**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "import boto3\n",
    "import pickle\n",
    "\n",
    "# Load model from S3\n",
    "s3 = boto3.client('s3')\n",
    "bucket = \"your-bucket\"\n",
    "model_key = \"model.pkl\"\n",
    "response = s3.get_object(Bucket=bucket, Key=model_key)\n",
    "model = pickle.loads(response['Body'].read())\n",
    "\n",
    "# Define Lambda function\n",
    "def lambda_handler(event, context):\n",
    "    data = json.loads(event['body'])\n",
    "    prediction = model.predict([data['features']])\n",
    "    return {\"statusCode\": 200, \"body\": json.dumps({\"prediction\": prediction.tolist()})}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ **AWS EMR (Elastic MapReduce)** ‚Äì Running Spark on AWS  \n",
    "**EMR** allows running **Apache Spark, Hadoop, and Presto** on AWS.  \n",
    "\n",
    "### **üîπ Example: Running a Spark Job on EMR**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark on EMR\n",
    "spark = SparkSession.builder.appName(\"AWS_EMR_Spark\").getOrCreate()\n",
    "\n",
    "# Load Data\n",
    "df = spark.read.csv(\"s3://your-bucket/data.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Perform Transformations\n",
    "df.groupBy(\"category\").count().show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "# **2Ô∏è‚É£ Google Cloud AI**  \n",
    "\n",
    "## ‚úÖ **Vertex AI** ‚Äì End-to-End ML Platform  \n",
    "**Vertex AI** provides **AutoML, custom training, model deployment, and monitoring** in a single platform.\n",
    "\n",
    "### **üîπ Example: Training a Model in Vertex AI**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "# Initialize Vertex AI client\n",
    "aiplatform.init(project=\"your-project-id\", location=\"us-central1\")\n",
    "\n",
    "# Train model\n",
    "model = aiplatform.CustomTrainingJob(\n",
    "    display_name=\"my-model\",\n",
    "    script_path=\"train.py\",\n",
    "    container_uri=\"gcr.io/cloud-aiplatform/training/tf-cpu.2-2\",\n",
    "    model_serving_container_image_uri=\"gcr.io/cloud-aiplatform/prediction/tf2-cpu.2-2\"\n",
    ")\n",
    "\n",
    "model.run(replica_count=1, machine_type=\"n1-standard-4\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ **BigQuery ML** ‚Äì Train ML Models Inside BigQuery  \n",
    "**BigQuery ML** enables SQL-based ML model training **directly inside BigQuery** without exporting data.\n",
    "\n",
    "### **üîπ Example: Train a Regression Model in BigQuery**\n",
    "```sql\n",
    "CREATE OR REPLACE MODEL my_project.my_dataset.model_name  \n",
    "OPTIONS(model_type='linear_reg') AS  \n",
    "SELECT feature1, feature2, target FROM my_project.my_dataset.train_data;\n",
    "```\n",
    "\n",
    "### **üîπ Example: Make Predictions with BigQuery ML**\n",
    "```sql\n",
    "SELECT *  \n",
    "FROM ML.PREDICT(MODEL my_project.my_dataset.model_name,  \n",
    "                (SELECT feature1, feature2 FROM my_project.my_dataset.test_data));\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ **Dataflow & Dataproc** ‚Äì Serverless Spark & Hadoop  \n",
    "- **Dataflow**: Managed **Apache Beam** for real-time and batch processing.  \n",
    "- **Dataproc**: Fully managed **Apache Spark & Hadoop** on Google Cloud.  \n",
    "\n",
    "### **üîπ Example: Running a Data Pipeline on Dataflow**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import apache_beam as beam\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "\n",
    "options = PipelineOptions()\n",
    "p = beam.Pipeline(options=options)\n",
    "\n",
    "(p | \"Read Data\" >> beam.io.ReadFromText(\"gs://your-bucket/data.csv\")\n",
    "   | \"Transform Data\" >> beam.Map(lambda x: x.upper())\n",
    "   | \"Write Output\" >> beam.io.WriteToText(\"gs://your-bucket/output.txt\"))\n",
    "\n",
    "p.run()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# **3Ô∏è‚É£ Microsoft Azure AI**  \n",
    "\n",
    "## ‚úÖ **Azure Machine Learning Studio**  \n",
    "Azure ML Studio provides a **drag-and-drop interface** and **Python SDK** for ML training and deployment.\n",
    "\n",
    "### **üîπ Example: Train Model in Azure ML**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from azureml.core import Workspace, Experiment, ScriptRunConfig\n",
    "\n",
    "# Connect to Azure ML Workspace\n",
    "ws = Workspace.from_config()\n",
    "\n",
    "# Define experiment\n",
    "experiment = Experiment(ws, name=\"my_experiment\")\n",
    "config = ScriptRunConfig(source_directory=\".\", script=\"train.py\")\n",
    "\n",
    "# Run experiment\n",
    "run = experiment.submit(config)\n",
    "run.wait_for_completion(show_output=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ **Azure Databricks** ‚Äì Cloud-Based Spark  \n",
    "Azure Databricks is a **managed Spark environment** on Azure for **Big Data & ML**.\n",
    "\n",
    "### **üîπ Example: Train a Spark ML Model in Azure Databricks**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Azure_Databricks\").getOrCreate()\n",
    "df = spark.read.csv(\"dbfs:/mnt/data.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Train Decision Tree\n",
    "dt = DecisionTreeClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
    "model = dt.fit(df)\n",
    "\n",
    "# Predict\n",
    "predictions = model.transform(df)\n",
    "predictions.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ **ML Pipelines & MLOps in Azure**  \n",
    "Azure provides **MLOps capabilities** using **Azure DevOps, ML Pipelines, and Model Monitoring**.\n",
    "\n",
    "---\n",
    "\n",
    "# **üõ† Hands-on Projects**  \n",
    "‚úÖ **Deploy an ML model using AWS SageMaker**  \n",
    "‚úÖ **Train an ML model in BigQuery ML**  \n",
    "‚úÖ **Build an MLOps pipeline using Azure ML**  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's break down **MLOps & Model Deployment at Scale** into detailed sections with explanations and code snippets.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ **1. MLOps for Scalable ML**\n",
    "### ‚úÖ **1.1 CI/CD for ML (Automating ML Pipelines)**\n",
    "MLOps integrates DevOps practices into ML workflows to automate and streamline model training, validation, deployment, and monitoring.\n",
    "\n",
    "üìå **Key Steps in ML CI/CD Pipeline**  \n",
    "1. **Data Versioning** ‚Äì Store datasets with DVC or MLflow.  \n",
    "2. **Model Training & Validation** ‚Äì Train models using automated pipelines.  \n",
    "3. **Model Packaging** ‚Äì Convert models to deployable formats (Pickle, ONNX).  \n",
    "4. **Model Deployment** ‚Äì Deploy as APIs or in production environments.  \n",
    "5. **Continuous Monitoring** ‚Äì Track model drift and retrain models when needed.  \n",
    "\n",
    "üõ† **Example: CI/CD with GitHub Actions & MLflow**  \n",
    "```yaml\n",
    "name: ML Pipeline\n",
    "\n",
    "on: [push]\n",
    "\n",
    "jobs:\n",
    "  train:\n",
    "    runs-on: ubuntu-latest\n",
    "    steps:\n",
    "      - name: Checkout Repository\n",
    "        uses: actions/checkout@v2\n",
    "      \n",
    "      - name: Set up Python\n",
    "        uses: actions/setup-python@v2\n",
    "        with:\n",
    "          python-version: '3.8'\n",
    "      \n",
    "      - name: Install Dependencies\n",
    "        run: pip install -r requirements.txt\n",
    "      \n",
    "      - name: Train Model\n",
    "        run: python train.py\n",
    "\n",
    "      - name: Log Model with MLflow\n",
    "        run: python log_model.py\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ **1.2 Model Versioning & Monitoring (MLflow, Kubeflow)**\n",
    "#### üìå **Model Versioning with MLflow**\n",
    "MLflow helps track experiments, store models, and deploy them.\n",
    "\n",
    "üõ† **Example: Logging a Model in MLflow**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "mlflow.set_experiment(\"RandomForest_Experiment\")\n",
    "\n",
    "with mlflow.start_run():\n",
    "    model = RandomForestClassifier(n_estimators=100)\n",
    "    mlflow.sklearn.log_model(model, \"random_forest_model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### üìå **Model Monitoring & Drift Detection**\n",
    "- **MLflow Tracking** logs metrics, parameters, and artifacts.  \n",
    "- **Prometheus + Grafana** can track model latency and performance.  \n",
    "- **Evidently AI** detects model drift.  \n",
    "\n",
    "üõ† **Example: Monitor Data Drift with Evidently**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from evidently.test_suite import TestSuite\n",
    "from evidently.tests import TestDataDrift\n",
    "\n",
    "suite = TestSuite(tests=[TestDataDrift()])\n",
    "suite.run(reference_data=ref_df, current_data=curr_df)\n",
    "suite.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ **1.3 A/B Testing & Model Drift Detection**\n",
    "- **A/B Testing** helps compare model versions in production.\n",
    "- **Model Drift** happens when data distributions change over time.\n",
    "\n",
    "üõ† **Example: A/B Testing API with FastAPI**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from fastapi import FastAPI\n",
    "import random\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "@app.get(\"/predict\")\n",
    "def predict():\n",
    "    model_version = random.choice([\"v1\", \"v2\"])\n",
    "    return {\"model_version\": model_version, \"prediction\": 0.85}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ **2. Deploying ML Models at Scale**\n",
    "### ‚úÖ **2.1 Serverless Deployment (FastAPI, AWS Lambda)**\n",
    "Serverless deployment makes ML models accessible via APIs.\n",
    "\n",
    "üõ† **Example: Deploying a FastAPI Model**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "import pickle\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "model = pickle.load(open(\"model.pkl\", \"rb\"))\n",
    "\n",
    "@app.get(\"/predict\")\n",
    "def predict(x: float):\n",
    "    return {\"prediction\": model.predict([[x]])[0]}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "üõ† **Example: Deploy Model on AWS Lambda**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "import joblib\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    model = joblib.load(\"/tmp/model.pkl\")\n",
    "    input_data = json.loads(event['body'])\n",
    "    prediction = model.predict([input_data['features']])\n",
    "    return {\"statusCode\": 200, \"body\": json.dumps({\"prediction\": prediction.tolist()})}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ **2.2 Kubernetes for ML (Kubeflow)**\n",
    "Kubernetes (K8s) helps in scaling ML models.\n",
    "\n",
    "üìå **Kubeflow Features**\n",
    "- **Pipeline Orchestration** ‚Äì Automate workflows.  \n",
    "- **Model Serving** ‚Äì Deploy models using KFServing.  \n",
    "- **Hyperparameter Tuning** ‚Äì Use Katib for tuning.  \n",
    "\n",
    "üõ† **Example: Deploy Model using KFServing**\n",
    "```yaml\n",
    "apiVersion: serving.kubeflow.org/v1alpha2\n",
    "kind: InferenceService\n",
    "metadata:\n",
    "  name: my-model\n",
    "spec:\n",
    "  default:\n",
    "    predictor:\n",
    "      tensorflow:\n",
    "        storageUri: \"gs://my-bucket/my-model/\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ **2.3 Inference Optimization (ONNX, TensorRT)**\n",
    "Optimizing models improves speed and efficiency.\n",
    "\n",
    "#### üìå **ONNX for Model Optimization**\n",
    "ONNX enables cross-platform inference with optimized performance.\n",
    "\n",
    "üõ† **Convert a PyTorch Model to ONNX**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "\n",
    "model = models.resnet18(pretrained=True)\n",
    "dummy_input = torch.randn(1, 3, 224, 224)\n",
    "torch.onnx.export(model, dummy_input, \"resnet18.onnx\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### üìå **Optimize Inference with TensorRT**\n",
    "NVIDIA TensorRT accelerates deep learning inference.\n",
    "\n",
    "üõ† **Convert Model to TensorRT**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorrt as trt\n",
    "\n",
    "TRT_LOGGER = trt.Logger(trt.Logger.WARNING)\n",
    "builder = trt.Builder(TRT_LOGGER)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
