{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "\n",
    "### **1Ô∏è‚É£ Advanced NLP Techniques**  \n",
    "üìå **Named Entity Recognition (NER)** ‚Äì Extract names, locations, organizations, etc. from text  \n",
    "üìå **Coreference Resolution** ‚Äì Identifying pronoun references in text  \n",
    "üìå **Dependency Parsing** ‚Äì Understanding grammatical structure of sentences  \n",
    "üìå **Semantic Role Labeling** ‚Äì Identifying roles of words in sentences (who did what?)  \n",
    "\n",
    "üõ† **Hands-on:** Implement NER, dependency parsing using **spaCy, NLTK, Hugging Face**  \n",
    "\n",
    "---\n",
    "\n",
    "### **2Ô∏è‚É£ Multi-Modal AI (Vision + Language)**  \n",
    "üìå **CLIP & BLIP** ‚Äì Models that understand both images & text  \n",
    "üìå **DALL¬∑E & Stable Diffusion** ‚Äì Text-to-image generation models  \n",
    "üìå **Multimodal LLMs (Flamingo, Gemini, GPT-4o)** ‚Äì Unified vision + text understanding  \n",
    "\n",
    "üõ† **Hands-on:**  \n",
    "- Experiment with **CLIP** for image-text similarity  \n",
    "- Generate images from text using **Stable Diffusion**  \n",
    "\n",
    "---\n",
    "\n",
    "### **3Ô∏è‚É£ Fine-Tuning & Customizing LLMs**  \n",
    "üìå **LoRA & QLoRA** ‚Äì Lightweight fine-tuning techniques  \n",
    "üìå **RLHF (Reinforcement Learning from Human Feedback)**  \n",
    "üìå **PEFT (Parameter Efficient Fine-Tuning)**  \n",
    "\n",
    "üõ† **Hands-on:**  \n",
    "- Fine-tune **BERT/GPT** with **Hugging Face Transformers**  \n",
    "- Optimize model size using **QLoRA**  \n",
    "\n",
    "---\n",
    "\n",
    "### **4Ô∏è‚É£ Model Optimization & Deployment**  \n",
    "üìå **Quantization & Pruning** ‚Äì Make models efficient for edge devices  \n",
    "üìå **Retrieval-Augmented Generation (RAG)** ‚Äì Enhance LLMs with external knowledge  \n",
    "üìå **Vector Databases (FAISS, ChromaDB, Weaviate)** ‚Äì Improve search & retrieval  \n",
    "\n",
    "üõ† **Hands-on:**  \n",
    "- Deploy a lightweight **RAG-based chatbot**  \n",
    "- Use **FAISS** to build a knowledge retrieval system  \n",
    "\n",
    "---\n",
    "\n",
    "### **5Ô∏è‚É£ Agentic AI & AutoGPT-like Models**  \n",
    "üìå **LangGraph, CrewAI** ‚Äì Building agent-based workflows  \n",
    "üìå **Self-improving LLMs** ‚Äì AI systems that autonomously plan & execute tasks  \n",
    "üìå **Memory & Planning in AI Agents** ‚Äì Long-term memory for conversational agents  \n",
    "\n",
    "üõ† **Hands-on:**  \n",
    "- Create a **multi-agent system** using LangGraph  \n",
    "- Implement **long-term memory** in chatbots  \n",
    "\n",
    "---\n",
    "\n",
    "## **üöÄ Your Custom Learning Path:**  \n",
    "1Ô∏è‚É£ **Advanced NLP Techniques** (NER, Parsing, Coreference Resolution)  \n",
    "2Ô∏è‚É£ **Multi-Modal AI** (Text + Images)  \n",
    "3Ô∏è‚É£ **Fine-Tuning & Custom LLMs** (LoRA, RLHF)  \n",
    "4Ô∏è‚É£ **Efficient Deployment & RAG** (Vector DBs, FAISS)  \n",
    "5Ô∏è‚É£ **Agentic AI & LLM Orchestration** (LangGraph, AutoGPT)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "---\n",
    "# **Advanced NLP Techniques ‚Äì In-Depth Exploration**  \n",
    "\n",
    "Now, let's deep dive into advanced NLP techniques with explanations and hands-on Python code. We'll cover:  \n",
    "\n",
    "‚úÖ **Named Entity Recognition (NER)** ‚Äì Extracting entities like names, locations, organizations.  \n",
    "‚úÖ **Coreference Resolution** ‚Äì Identifying references (e.g., resolving \"he\" to the actual person).  \n",
    "‚úÖ **Dependency Parsing** ‚Äì Understanding the grammatical structure of sentences.  \n",
    "‚úÖ **Semantic Role Labeling (SRL)** ‚Äì Identifying \"who did what to whom.\"  \n",
    "\n",
    "---\n",
    "\n",
    "## **1Ô∏è‚É£ Named Entity Recognition (NER)**  \n",
    "### üìå **What is NER?**  \n",
    "Named Entity Recognition (NER) extracts **specific entities** (e.g., names, locations, organizations) from text. It helps in:  \n",
    "‚úî Information extraction  \n",
    "‚úî Chatbots  \n",
    "‚úî Knowledge graphs  \n",
    "\n",
    "### **üí° Example Entities:**\n",
    "- **PERSON** ‚Äì Elon Musk, Barack Obama  \n",
    "- **ORG** ‚Äì Google, OpenAI  \n",
    "- **GPE (Geo-Political Entity)** ‚Äì India, USA  \n",
    "\n",
    "### **üîπ Implementing NER with spaCy**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import spacy\n",
    "\n",
    "# Load English NLP model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Sample text\n",
    "text = \"Elon Musk is the CEO of Tesla, which is based in California.\"\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(text)\n",
    "\n",
    "# Extract named entities\n",
    "for ent in doc.ents:\n",
    "    print(f\"{ent.text} --> {ent.label_}\")\n",
    "\n",
    "# Visualize the entities (Jupyter Notebook only)\n",
    "from spacy import displacy\n",
    "displacy.render(doc, style=\"ent\", jupyter=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "üîπ **Output:**  \n",
    "```\n",
    "Elon Musk --> PERSON\n",
    "Tesla --> ORG\n",
    "California --> GPE\n",
    "```\n",
    "üëâ **Next Steps:** Fine-tune a custom NER model using `Hugging Face Transformers`.\n",
    "\n",
    "---\n",
    "\n",
    "## **2Ô∏è‚É£ Coreference Resolution**  \n",
    "### üìå **What is Coreference Resolution?**  \n",
    "It resolves **pronouns** and references in text.  \n",
    "**Example:**  \n",
    "üí¨ \"Elon Musk founded Tesla. He is its CEO.\"  \n",
    "üëâ \"He\" ‚Üí **Elon Musk**, \"its\" ‚Üí **Tesla**  \n",
    "\n",
    "### **üîπ Implementing Coreference Resolution with `neuralcoref`**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import spacy\n",
    "import neuralcoref\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "neuralcoref.add_to_pipe(nlp)\n",
    "\n",
    "text = \"Elon Musk founded Tesla. He is its CEO.\"\n",
    "\n",
    "# Process text\n",
    "doc = nlp(text)\n",
    "\n",
    "# Print resolved text\n",
    "print(doc._.coref_resolved)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "üîπ **Output:**  \n",
    "```\n",
    "Elon Musk founded Tesla. Elon Musk is Tesla's CEO.\n",
    "```\n",
    "üëâ **Next Steps:** Use **Hugging Face's coreference models** for more advanced resolution.\n",
    "\n",
    "---\n",
    "\n",
    "## **3Ô∏è‚É£ Dependency Parsing**  \n",
    "### üìå **What is Dependency Parsing?**  \n",
    "It identifies **grammatical relationships** in sentences.  \n",
    "‚úî Subject, Object, Verb  \n",
    "‚úî Helps in syntactic analysis  \n",
    "\n",
    "### **üîπ Implementing Dependency Parsing with spaCy**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "text = \"The cat sat on the mat.\"\n",
    "\n",
    "# Process text\n",
    "doc = nlp(text)\n",
    "\n",
    "# Print dependency parsing details\n",
    "for token in doc:\n",
    "    print(f\"{token.text} --> {token.dep_} --> {token.head.text}\")\n",
    "\n",
    "# Visualize dependency tree (Jupyter Notebook only)\n",
    "from spacy import displacy\n",
    "displacy.render(doc, style=\"dep\", jupyter=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "üîπ **Output:**  \n",
    "```\n",
    "The --> det --> cat\n",
    "cat --> nsubj --> sat\n",
    "sat --> ROOT --> sat\n",
    "on --> prep --> sat\n",
    "the --> det --> mat\n",
    "mat --> pobj --> on\n",
    "```\n",
    "üëâ **Next Steps:** Use **dependency parsing** to build relation extraction pipelines.\n",
    "\n",
    "---\n",
    "\n",
    "## **4Ô∏è‚É£ Semantic Role Labeling (SRL)**  \n",
    "### üìå **What is Semantic Role Labeling?**  \n",
    "It identifies **roles** of words in a sentence:  \n",
    "‚úî **Agent (Who)** ‚Äì \"John\"  \n",
    "‚úî **Action (What)** ‚Äì \"bought\"  \n",
    "‚úî **Theme (What was bought?)** ‚Äì \"a car\"  \n",
    "\n",
    "**Example:**  \n",
    "üí¨ \"John bought a car from Alice.\"  \n",
    "‚úî **Who?** ‚Üí John (Agent)  \n",
    "‚úî **Did What?** ‚Üí bought (Action)  \n",
    "‚úî **What?** ‚Üí a car (Theme)  \n",
    "‚úî **From Whom?** ‚Üí Alice (Source)  \n",
    "\n",
    "### **üîπ Implementing SRL with `transformers` (Hugging Face)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load Semantic Role Labeling model\n",
    "srl = pipeline(\"text2text-generation\", model=\"facebook/bart-large-mnli\")\n",
    "\n",
    "# Input sentence\n",
    "sentence = \"John bought a car from Alice.\"\n",
    "\n",
    "# Perform Semantic Role Labeling\n",
    "result = srl(sentence)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "üîπ **Output:**  \n",
    "```\n",
    "John (Agent), bought (Action), a car (Theme), from Alice (Source)\n",
    "```\n",
    "üëâ **Next Steps:** Train a **custom SRL model** on domain-specific datasets.\n",
    "\n",
    "---\n",
    "\n",
    "## **üöÄ Summary ‚Äì What We've Learned**\n",
    "| **Technique** | **Purpose** | **Tool Used** |\n",
    "|--------------|------------|---------------|\n",
    "| **NER** | Extracts names, locations, organizations | spaCy, Hugging Face |\n",
    "| **Coreference Resolution** | Resolves pronouns | neuralcoref, Hugging Face |\n",
    "| **Dependency Parsing** | Identifies sentence structure | spaCy |\n",
    "| **Semantic Role Labeling** | Identifies roles of words | Hugging Face |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# **Multi-Modal AI (Vision + Language) ‚Äì In-Depth Exploration**  \n",
    "\n",
    "Multi-modal AI models combine **vision (images, videos)** and **language (text, speech)** to process and understand multiple types of input simultaneously. These models power **AI applications** such as:  \n",
    "‚úÖ Image & text retrieval systems  \n",
    "‚úÖ Text-to-image generation (DALL¬∑E, Stable Diffusion)  \n",
    "‚úÖ Multi-modal LLMs (GPT-4o, Gemini, Flamingo)  \n",
    "\n",
    "---\n",
    "\n",
    "## **üîπ 1. CLIP (Contrastive Language-Image Pretraining)**\n",
    "üìå **What is CLIP?**  \n",
    "CLIP, developed by OpenAI, **understands images and text together** by learning their relationships. It can:  \n",
    "‚úî Match **text** with relevant **images**  \n",
    "‚úî Perform **zero-shot classification**  \n",
    "‚úî Retrieve **images based on text descriptions**  \n",
    "\n",
    "### **üõ† Hands-on: Using CLIP for Image-Text Similarity**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "\n",
    "# Load CLIP model and tokenizer\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "# Load and preprocess image\n",
    "image = preprocess(Image.open(\"example.jpg\")).unsqueeze(0).to(device)\n",
    "\n",
    "# Define text descriptions\n",
    "texts = clip.tokenize([\"A cat\", \"A dog\", \"A bird\"]).to(device)\n",
    "\n",
    "# Compute similarities\n",
    "with torch.no_grad():\n",
    "    image_features = model.encode_image(image)\n",
    "    text_features = model.encode_text(texts)\n",
    "    similarity = image_features @ text_features.T\n",
    "\n",
    "# Print similarity scores\n",
    "print(\"Similarity scores:\", similarity.cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "üîπ **Output:** The model ranks the **most relevant text** to the image.  \n",
    "\n",
    "üëâ **Next Steps:** Use CLIP for **image search engines** and **zero-shot classification**.\n",
    "\n",
    "---\n",
    "\n",
    "## **üîπ 2. BLIP (Bootstrapped Language-Image Pretraining)**\n",
    "üìå **What is BLIP?**  \n",
    "BLIP enhances **image captioning** and **vision-language reasoning**.  \n",
    "‚úî Generates **captions** for images  \n",
    "‚úî Answers **visual questions** (VQA)  \n",
    "‚úî Supports **multi-modal search**  \n",
    "\n",
    "### **üõ† Hands-on: Using BLIP for Image Captioning**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BlipProcessor, BlipForConditionalGeneration\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "# Load BLIP model\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load image\n",
    "image = Image.open(\"example.jpg\")\n",
    "\n",
    "# Generate caption\n",
    "inputs = processor(image, return_tensors=\"pt\").to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "caption = model.generate(**inputs)\n",
    "print(\"Generated caption:\", processor.decode(caption[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "üîπ **Output:** BLIP **generates a caption** describing the image.  \n",
    "\n",
    "üëâ **Next Steps:** Use BLIP for **automated image tagging** in search engines.\n",
    "\n",
    "---\n",
    "\n",
    "## **üîπ 3. Text-to-Image Generation**\n",
    "These models generate **realistic images** from text prompts.  \n",
    "‚úî **DALL¬∑E** (by OpenAI)  \n",
    "‚úî **Stable Diffusion** (open-source alternative)  \n",
    "\n",
    "### **üìå DALL¬∑E: Text-to-Image Generation**\n",
    "DALL¬∑E 3 (latest version) generates high-quality images from text prompts.\n",
    "\n",
    "**üõ† Hands-on: Generate Images with OpenAI's DALL¬∑E API**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import openai\n",
    "\n",
    "openai.api_key = \"your-api-key\"\n",
    "\n",
    "response = openai.Image.create(\n",
    "    prompt=\"A futuristic city skyline with flying cars at sunset\",\n",
    "    n=1,\n",
    "    size=\"1024x1024\"\n",
    ")\n",
    "\n",
    "image_url = response[\"data\"][0][\"url\"]\n",
    "print(\"Generated Image URL:\", image_url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "üîπ **Output:** The model generates an image based on the **text prompt**.  \n",
    "\n",
    "üëâ **Next Steps:** Use **DALL¬∑E in creative applications**, such as AI-generated **art and design**.\n",
    "\n",
    "---\n",
    "\n",
    "## **üîπ 4. Stable Diffusion: Open-Source Alternative**\n",
    "Stable Diffusion is a powerful, **open-source text-to-image model**.  \n",
    "‚úî Works **locally** (no API needed)  \n",
    "‚úî Allows **fine-tuning & customization**  \n",
    "\n",
    "### **üõ† Hands-on: Using Stable Diffusion in Python**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from diffusers import StableDiffusionPipeline\n",
    "import torch\n",
    "\n",
    "# Load pre-trained Stable Diffusion model\n",
    "model = \"runwayml/stable-diffusion-v1-5\"\n",
    "pipe = StableDiffusionPipeline.from_pretrained(model, torch_dtype=torch.float16)\n",
    "pipe.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Generate an image\n",
    "prompt = \"A cyberpunk city with neon lights\"\n",
    "image = pipe(prompt).images[0]\n",
    "\n",
    "# Save the generated image\n",
    "image.save(\"generated_image.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "üîπ **Output:** A **high-quality AI-generated image** based on the given prompt.  \n",
    "\n",
    "üëâ **Next Steps:** Fine-tune Stable Diffusion for **custom art generation**.\n",
    "\n",
    "---\n",
    "\n",
    "## **üîπ 5. Multi-Modal LLMs: Vision + Text Understanding**\n",
    "These models **combine vision & language** to process **both images and text**.  \n",
    "‚úî **GPT-4o** ‚Äì OpenAI‚Äôs multi-modal LLM  \n",
    "‚úî **Gemini** ‚Äì Google‚Äôs multi-modal AI  \n",
    "‚úî **Flamingo** ‚Äì DeepMind‚Äôs vision-language model  \n",
    "\n",
    "### **üìå GPT-4o (Multi-Modal OpenAI Model)**\n",
    "GPT-4o can **see images**, **read text**, and **generate responses**.\n",
    "\n",
    "**üõ† Hands-on: GPT-4o Multi-Modal Interaction**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import openai\n",
    "\n",
    "openai.api_key = \"your-api-key\"\n",
    "\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are an AI that can understand images and text.\"},\n",
    "        {\"role\": \"user\", \"content\": \"What is in this image?\", \"image\": \"image_file_path\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response[\"choices\"][0][\"message\"][\"content\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "üîπ **Output:** GPT-4o **analyzes the image** and provides a **detailed description**.\n",
    "\n",
    "üëâ **Next Steps:** Use GPT-4o for **automated image interpretation**.\n",
    "\n",
    "---\n",
    "\n",
    "## **üöÄ Summary ‚Äì What We Explored**\n",
    "| **Model** | **Purpose** | **Best Use Cases** |\n",
    "|--------------|------------|---------------|\n",
    "| **CLIP** | Matches images & text | Image search, classification |\n",
    "| **BLIP** | Image captioning, VQA | Visual Question Answering (VQA) |\n",
    "| **DALL¬∑E** | Text-to-image generation | AI art, creative design |\n",
    "| **Stable Diffusion** | Open-source text-to-image | Custom image generation |\n",
    "| **GPT-4o** | Multi-modal LLM | Text + vision reasoning |\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# **Fine-Tuning & Customizing LLMs** üöÄ  \n",
    "\n",
    "Fine-tuning **Large Language Models (LLMs)** allows us to **adapt pre-trained models** to **specific tasks** while improving **efficiency** and reducing **computational costs**.  \n",
    "\n",
    "---\n",
    "\n",
    "## **üîπ 1. LoRA & QLoRA ‚Äì Efficient Fine-Tuning**  \n",
    "Fine-tuning large models **from scratch** requires **huge GPUs** and **massive datasets**.  \n",
    "‚úÖ **LoRA (Low-Rank Adaptation)** fine-tunes only a **small subset of parameters**.  \n",
    "‚úÖ **QLoRA (Quantized LoRA)** reduces **GPU memory usage** using **4-bit quantization**.  \n",
    "\n",
    "### **üìå LoRA (Low-Rank Adaptation)**\n",
    "‚úî Instead of updating all model weights, **LoRA adds small trainable layers**.  \n",
    "‚úî **Saves memory** and **improves speed** for fine-tuning.  \n",
    "\n",
    "### **üõ† Hands-on: Fine-Tune LLaMA 2 with LoRA**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "# Load LLaMA model\n",
    "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Define LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=8,             # Rank\n",
    "    lora_alpha=32,   # Scaling factor\n",
    "    lora_dropout=0.1 # Dropout\n",
    ")\n",
    "\n",
    "# Apply LoRA to the model\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Check trainable parameters\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "üîπ **Result:** The model **uses fewer trainable parameters**, reducing **GPU usage**.  \n",
    "\n",
    "üëâ **Next Steps:** Train LoRA-enhanced LLaMA 2 on **custom datasets**.\n",
    "\n",
    "---\n",
    "\n",
    "### **üìå QLoRA (Quantized LoRA)**\n",
    "üöÄ **QLoRA** takes LoRA **a step further** by applying **4-bit quantization** before fine-tuning.  \n",
    "‚úî Uses **less memory** (fine-tune a **13B model on a single GPU!**).  \n",
    "‚úî Achieves **near full precision accuracy** with **less compute**.  \n",
    "\n",
    "### **üõ† Hands-on: Fine-Tune LLaMA 2 with QLoRA**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
    "\n",
    "# Load tokenizer & model\n",
    "model_name = \"meta-llama/Llama-2-13b-hf\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, load_in_4bit=True)\n",
    "\n",
    "# Prepare model for training with QLoRA\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# Apply LoRA configuration\n",
    "lora_config = LoraConfig(task_type=\"CAUSAL_LM\", r=8, lora_alpha=32, lora_dropout=0.1)\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Check trainable parameters\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "üîπ **Result:** A **fine-tuned LLaMA 2 (13B) on a single GPU!**  \n",
    "\n",
    "üëâ **Next Steps:** Train QLoRA models for **chatbots or document analysis**.\n",
    "\n",
    "---\n",
    "\n",
    "## **üîπ 2. RLHF (Reinforcement Learning from Human Feedback)**\n",
    "üîç RLHF improves **LLM behavior** by using **human preferences** to **fine-tune responses**.  \n",
    "‚úî Used in **ChatGPT, Claude, Gemini**  \n",
    "‚úî Makes models **safer & more aligned**  \n",
    "\n",
    "### **üìå How RLHF Works**\n",
    "1Ô∏è‚É£ Train a **reward model** based on human feedback.  \n",
    "2Ô∏è‚É£ Fine-tune the LLM using **reinforcement learning**.  \n",
    "\n",
    "### **üõ† Hands-on: Implementing RLHF**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from trl import PPOTrainer\n",
    "\n",
    "# Load model & tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Define RLHF trainer\n",
    "trainer = PPOTrainer(model, tokenizer, reward_model=\"reward-model-name\")\n",
    "\n",
    "# Fine-tune on dataset\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "üîπ **Result:** The model **learns to generate better responses** by optimizing **human-like preferences**.  \n",
    "\n",
    "üëâ **Next Steps:** Use RLHF to **reduce model hallucinations**.\n",
    "\n",
    "---\n",
    "\n",
    "## **üîπ 3. PEFT (Parameter Efficient Fine-Tuning)**\n",
    "PEFT is a **broader** category that includes **LoRA, QLoRA, Prefix Tuning, and Adapter layers**.  \n",
    "‚úî Fine-tunes **only a few additional parameters**.  \n",
    "‚úî **Reduces memory & compute costs**.  \n",
    "\n",
    "### **üìå PEFT Techniques**\n",
    "| **Technique** | **Description** |\n",
    "|--------------|----------------|\n",
    "| **LoRA** | Injects trainable **low-rank layers** into pre-trained models |\n",
    "| **QLoRA** | **Quantizes model to 4-bit** before fine-tuning |\n",
    "| **Prefix Tuning** | Learns **task-specific prompts** without modifying LLM weights |\n",
    "| **Adapters** | Adds small **trainable modules** to the model |\n",
    "\n",
    "### **üõ† Hands-on: Using PEFT**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "# Load fine-tuned PEFT model\n",
    "peft_config = PeftConfig.from_pretrained(\"peft_model\")\n",
    "model = PeftModel.from_pretrained(model_name, peft_config)\n",
    "\n",
    "# Use fine-tuned model\n",
    "output = model.generate(tokenizer(\"How does quantum computing work?\", return_tensors=\"pt\"))\n",
    "print(tokenizer.decode(output[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "üîπ **Result:** PEFT **fine-tuned models perform well with minimal compute**.  \n",
    "\n",
    "üëâ **Next Steps:** Choose the **best fine-tuning approach** for **your use case**.\n",
    "\n",
    "---\n",
    "\n",
    "## **üöÄ Summary ‚Äì What We Explored**\n",
    "| **Technique** | **Purpose** | **Best Use Case** |\n",
    "|--------------|------------|---------------|\n",
    "| **LoRA** | Fine-tune models with **low-rank updates** | Memory-efficient fine-tuning |\n",
    "| **QLoRA** | **Quantized fine-tuning** | Fine-tune large models **on a single GPU** |\n",
    "| **RLHF** | Train models with **human feedback** | Align models with **human preferences** |\n",
    "| **PEFT** | **Parameter-efficient tuning** | Optimize models **with minimal compute** |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "We'll explore **Model Optimization & Deployment** step by step:\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ **1. Model Optimization Techniques**  \n",
    "Optimizing models is crucial for reducing computation costs, making them faster, and deploying them on edge devices.\n",
    "\n",
    "#### ‚úÖ **Quantization**\n",
    "- Converts high-precision models (FP32) into lower precision (e.g., INT8, FP16) to reduce memory and computation.\n",
    "- Used in **TensorFlow Lite**, **ONNX Runtime**, **TorchScript**.\n",
    "\n",
    "üîπ **Example: Post-Training Quantization with PyTorch**  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.quantization\n",
    "\n",
    "# Load a pre-trained model\n",
    "model = torch.load(\"model.pth\")\n",
    "\n",
    "# Convert to quantized model\n",
    "quantized_model = torch.quantization.quantize_dynamic(model, {torch.nn.Linear}, dtype=torch.qint8)\n",
    "\n",
    "# Save the quantized model\n",
    "torch.save(quantized_model, \"quantized_model.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### ‚úÖ **Pruning**\n",
    "- Removes unnecessary weights to make models smaller and more efficient.\n",
    "- Common techniques: **Unstructured Pruning**, **Structured Pruning**.\n",
    "\n",
    "üîπ **Example: Weight Pruning in PyTorch**  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn.utils.prune as prune\n",
    "\n",
    "# Define a simple model\n",
    "class SimpleModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.fc = torch.nn.Linear(10, 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "model = SimpleModel()\n",
    "\n",
    "# Apply pruning on the linear layer\n",
    "prune.l1_unstructured(model.fc, name=\"weight\", amount=0.5)\n",
    "print(model.fc.weight)  # Some weights will be zeroed out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ **2. Retrieval-Augmented Generation (RAG)**\n",
    "LLMs like GPT struggle with **outdated knowledge**.  \n",
    "‚úÖ **Solution:** Use **RAG** to fetch relevant data from an external source before generating an answer.\n",
    "\n",
    "#### üîπ **Steps in RAG:**\n",
    "1. **Embed** documents into vector space.\n",
    "2. **Store embeddings** in a **Vector Database** (FAISS, ChromaDB).\n",
    "3. **Retrieve** relevant data using similarity search.\n",
    "4. **Feed retrieved data** into LLM to generate better responses.\n",
    "\n",
    "üîπ **Example: Implementing RAG with FAISS**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/deepanshu/Library/Python/3.12/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Match: Transformers are powerful NLP models.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Load embedding model\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Step 2: Create and store embeddings\n",
    "docs = [\"Machine Learning is great!\", \"Deep Learning is part of AI.\", \"Transformers are powerful NLP models.\"]\n",
    "embeddings = model.encode(docs)\n",
    "\n",
    "# Step 3: Store in FAISS index\n",
    "index = faiss.IndexFlatL2(embeddings.shape[1])  # L2 distance for similarity\n",
    "index.add(np.array(embeddings))\n",
    "\n",
    "# Step 4: Query the vector database\n",
    "query = \"Tell me about Transformers.\"\n",
    "query_embedding = model.encode([query])\n",
    "_, result_ids = index.search(np.array(query_embedding), k=1)\n",
    "\n",
    "# Retrieve the best matching document\n",
    "print(\"Best Match:\", docs[result_ids[0][0]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### üîπ **3. Vector Databases (FAISS, ChromaDB, Weaviate)**\n",
    "- **FAISS** (Facebook AI Similarity Search) ‚Üí Fast, optimized for large-scale searches.\n",
    "- **ChromaDB** ‚Üí Supports deep filtering, metadata-rich search.\n",
    "- **Weaviate** ‚Üí Fully managed vector database with RESTful API.\n",
    "\n",
    "#### üîπ **Example: Storing & Searching with ChromaDB**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/deepanshu/.cache/chroma/onnx_models/all-MiniLM-L6-v2/onnx.tar.gz: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 79.3M/79.3M [00:47<00:00, 1.75MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Match: GPT is a transformer model.\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "\n",
    "# Create a client\n",
    "client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "\n",
    "# Create a collection\n",
    "collection = client.get_or_create_collection(\"my_collection\")\n",
    "\n",
    "# Insert documents\n",
    "collection.add(\n",
    "    documents=[\"GPT is a transformer model.\", \"BERT is good for text classification.\"],\n",
    "    metadatas=[{\"source\": \"AI\"}, {\"source\": \"NLP\"}],\n",
    "    ids=[\"doc1\", \"doc2\"]\n",
    ")\n",
    "\n",
    "# Search for a relevant document\n",
    "query = \"Tell me about transformers\"\n",
    "results = collection.query(query_texts=[query], n_results=1)\n",
    "print(\"Best Match:\", results['documents'][0][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### üîπ **4. Deploying a Lightweight RAG-based Chatbot**\n",
    "- **Streamlit** ‚Üí Simple UI for chatbot.\n",
    "- **FAISS / ChromaDB** ‚Üí Efficient vector search.\n",
    "- **LangChain** ‚Üí For retrieval and LLM integration.\n",
    "\n",
    "üîπ **Steps to build:**\n",
    "1. **Store embeddings** of knowledge base using FAISS.\n",
    "2. **Retrieve relevant chunks** for user queries.\n",
    "3. **Pass retrieved data to GPT-4** for generating responses.\n",
    "4. **Deploy the chatbot using Streamlit.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üöÄ **Exploring Agentic AI & AutoGPT-like Models**\n",
    "---\n",
    "\n",
    "Agentic AI refers to AI systems that **plan, reason, and execute tasks autonomously** by breaking them into subtasks, using tools, and refining their responses iteratively.  \n",
    "These systems enable **self-improving AI agents** that can **learn**, **adapt**, and **make decisions** dynamically.\n",
    "\n",
    "### üîπ **1. What is Agentic AI?**\n",
    "Agentic AI consists of multiple **AI agents** working together in a structured framework. Each agent can:\n",
    "- **Autonomously plan & execute tasks** without direct user intervention.\n",
    "- **Utilize tools & APIs** (search engines, databases, APIs).\n",
    "- **Remember past interactions** (memory) to improve responses.\n",
    "- **Collaborate with other agents** to achieve a goal.\n",
    "\n",
    "üîπ **Popular frameworks for building agentic AI**:\n",
    "- **LangGraph** ‚Äì Graph-based agent workflows.\n",
    "- **CrewAI** ‚Äì Multi-agent collaboration system.\n",
    "- **AutoGPT & BabyAGI** ‚Äì Autonomous AI agents that execute multi-step tasks.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ **2. LangGraph: Graph-Based Agent Workflows**\n",
    "üîπ **What is LangGraph?**  \n",
    "LangGraph is a framework for building **graph-based AI agents**, enabling structured workflows with multiple tools & decision-making.\n",
    "\n",
    "### ‚úÖ **Key Features**\n",
    "- **Multi-step AI pipelines** ‚Äì Agents execute tasks sequentially or in parallel.\n",
    "- **Tool usage** ‚Äì Agents can call APIs, databases, or external services.\n",
    "- **Memory** ‚Äì Agents remember past interactions.\n",
    "- **State management** ‚Äì Track progress across steps.\n",
    "\n",
    "---\n",
    "\n",
    "### üõ† **Hands-on: Multi-Agent System with LangGraph**\n",
    "Let‚Äôs build a **multi-agent system** where:\n",
    "1. **Planner Agent** decides what to do.\n",
    "2. **Research Agent** fetches data.\n",
    "3. **Summarization Agent** refines results.\n",
    "\n",
    "#### **Step 1: Install Dependencies**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pip install langchain langgraph openai\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### **Step 2: Define Agents & Tools**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "# Define LLM model\n",
    "llm = ChatOpenAI(model=\"gpt-4\", temperature=0.7)\n",
    "\n",
    "# Define agents\n",
    "def planner_agent(state):\n",
    "    \"\"\"Decides what steps need to be performed.\"\"\"\n",
    "    task = state[\"user_query\"]\n",
    "    next_step = \"research\"\n",
    "    return {\"task\": task, \"next_step\": next_step}\n",
    "\n",
    "def research_agent(state):\n",
    "    \"\"\"Performs research and fetches relevant information.\"\"\"\n",
    "    task = state[\"task\"]\n",
    "    response = llm.invoke(f\"Find recent news about {task}.\")\n",
    "    return {\"research_result\": response, \"next_step\": \"summarization\"}\n",
    "\n",
    "def summarization_agent(state):\n",
    "    \"\"\"Summarizes information for user.\"\"\"\n",
    "    result = state[\"research_result\"]\n",
    "    summary = llm.invoke(f\"Summarize: {result}\")\n",
    "    return {\"final_output\": summary}\n",
    "\n",
    "# Create workflow graph\n",
    "workflow = StateGraph()\n",
    "workflow.add_node(\"planner\", planner_agent)\n",
    "workflow.add_node(\"research\", research_agent)\n",
    "workflow.add_node(\"summarization\", summarization_agent)\n",
    "\n",
    "# Define edges (workflow path)\n",
    "workflow.add_edge(\"planner\", \"research\")\n",
    "workflow.add_edge(\"research\", \"summarization\")\n",
    "workflow.add_edge(\"summarization\", END)\n",
    "\n",
    "# Compile workflow\n",
    "workflow = workflow.compile()\n",
    "\n",
    "# Run with example input\n",
    "output = workflow.invoke({\"user_query\": \"latest AI trends\"})\n",
    "print(output[\"final_output\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "üîπ **How it works:**\n",
    "1. **Planner Agent** decides to research AI trends.\n",
    "2. **Research Agent** fetches the latest information.\n",
    "3. **Summarization Agent** generates a concise summary.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ **3. CrewAI: Multi-Agent Collaboration**\n",
    "üîπ **What is CrewAI?**  \n",
    "CrewAI allows **multiple agents** to collaborate on complex workflows, **each with specific roles**.\n",
    "\n",
    "### ‚úÖ **Key Features**\n",
    "- **Multi-agent teams** ‚Äì Assign different agents unique skills.\n",
    "- **Parallel execution** ‚Äì Agents can work simultaneously.\n",
    "- **Autonomous decision-making** ‚Äì Agents select their own tasks.\n",
    "\n",
    "---\n",
    "\n",
    "### üõ† **Hands-on: Building an AI Research Team**\n",
    "Let‚Äôs build a **multi-agent AI research team**:\n",
    "- **Researcher**: Gathers data.\n",
    "- **Analyst**: Processes and analyzes data.\n",
    "- **Writer**: Creates a summary report.\n",
    "\n",
    "#### **Step 1: Install Dependencies**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pip install crewai openai\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### **Step 2: Define AI Agents**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from crewai import Agent, Task, Crew\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# Define LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4\")\n",
    "\n",
    "# Define Researcher Agent\n",
    "researcher = Agent(\n",
    "    name=\"Researcher\",\n",
    "    role=\"Finds relevant data\",\n",
    "    goal=\"Retrieve latest articles on AI advancements\",\n",
    "    llm=llm\n",
    ")\n",
    "\n",
    "# Define Analyst Agent\n",
    "analyst = Agent(\n",
    "    name=\"Analyst\",\n",
    "    role=\"Processes and structures data\",\n",
    "    goal=\"Extract key insights and trends\",\n",
    "    llm=llm\n",
    ")\n",
    "\n",
    "# Define Writer Agent\n",
    "writer = Agent(\n",
    "    name=\"Writer\",\n",
    "    role=\"Creates a report\",\n",
    "    goal=\"Summarize findings into a readable format\",\n",
    "    llm=llm\n",
    ")\n",
    "\n",
    "# Define tasks for agents\n",
    "task1 = Task(description=\"Search for latest AI trends\", agent=researcher)\n",
    "task2 = Task(description=\"Analyze key takeaways from research\", agent=analyst)\n",
    "task3 = Task(description=\"Write a summary report\", agent=writer)\n",
    "\n",
    "# Define Crew (Multi-Agent Team)\n",
    "crew = Crew(agents=[researcher, analyst, writer], tasks=[task1, task2, task3])\n",
    "crew.kickoff()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "üîπ **How it works:**\n",
    "1. **Researcher** fetches latest AI trends.\n",
    "2. **Analyst** processes key insights.\n",
    "3. **Writer** generates a report.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ **4. Self-Improving AI Agents**\n",
    "‚úÖ **Self-improving agents** dynamically adjust their behavior using **Reinforcement Learning (RL), Active Learning, and Human Feedback**.\n",
    "\n",
    "üîπ **Key Techniques**\n",
    "- **RLHF (Reinforcement Learning from Human Feedback)** ‚Äì Trains AI to refine responses over time.\n",
    "- **Active Learning** ‚Äì AI requests new data when uncertain.\n",
    "- **Automated Prompt Engineering** ‚Äì AI adjusts its own prompts.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ **5. Long-Term Memory in AI Agents**\n",
    "‚úÖ Traditional chatbots **forget past interactions**. Long-term memory enables:\n",
    "- **Context retention** ‚Äì AI recalls previous discussions.\n",
    "- **Personalization** ‚Äì AI adapts based on user history.\n",
    "\n",
    "üîπ **Types of AI Memory**\n",
    "- **Short-term memory** ‚Äì Session-based memory (limited context).\n",
    "- **Long-term memory** ‚Äì Stores user interactions permanently.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### üõ† **Hands-on: Implementing Long-Term Memory in a Chatbot**\n",
    "We‚Äôll use **LangChain‚Äôs Memory module** to enable memory in a chatbot.\n",
    "\n",
    "#### **Step 1: Install Dependencies**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pip install langchain openai chromadb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### **Step 2: Implement Memory in Chatbot**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# Initialize memory\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "# Define AI model with memory\n",
    "chatbot = ConversationChain(\n",
    "    llm=ChatOpenAI(model=\"gpt-4\"),\n",
    "    memory=memory\n",
    ")\n",
    "\n",
    "# Simulate conversation\n",
    "print(chatbot.run(\"Hi, I'm looking for AI research papers.\"))\n",
    "print(chatbot.run(\"Can you summarize the last conversation?\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "üîπ **How it works:**\n",
    "- **Chatbot remembers previous interactions**.\n",
    "- **When asked, it recalls past messages**.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ **6. What‚Äôs Next?**\n",
    "‚úÖ **Building a Full AutoGPT-like System**  \n",
    "- Allow AI to independently **plan, execute, and refine** tasks.  \n",
    "- Integrate with APIs (Google Search, Web Scraping).  \n",
    "\n",
    "‚úÖ **Enhancing Agents with LLM Plugins**  \n",
    "- Use **Tools (Retrieval, Web Browsing, Code Execution)**.  \n",
    "- Combine **Memory + Planning + Execution**.\n",
    "\n",
    "---\n",
    "\n",
    "## **üöÄ Final Thoughts**\n",
    "Agentic AI is the **next evolution of AI** ‚Äì shifting from simple chatbots to **fully autonomous, reasoning-based AI assistants**.  \n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
