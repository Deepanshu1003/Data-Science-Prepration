{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3️⃣ Calculus (Derivatives & Gradient Descent)**  \n",
    "\n",
    "### **Key Concepts**\n",
    "- **Derivatives**: Chain rule, Partial derivatives  \n",
    "- **Gradient Descent**: Learning rate, Convergence, Optimization  \n",
    "- **Convexity**: Local vs. Global minima  \n",
    "- **Hessian Matrix**: Second-order derivatives & optimization  \n",
    "\n",
    "### **Important Formulas**\n",
    "- **Gradient Descent Update Rule**:  \n",
    "  \\[\n",
    "  \\theta_{new} = \\theta - \\alpha \\nabla f(\\theta)\n",
    "  \\]\n",
    "  where \\( \\alpha \\) is the learning rate and \\( \\nabla f(\\theta) \\) is the gradient.  \n",
    "- **Chain Rule**:  \n",
    "  \\[\n",
    "  \\frac{dy}{dx} = \\frac{dy}{du} \\cdot \\frac{du}{dx}\n",
    "  \\]\n",
    "- **Second Derivative Test for Minima/Maxima**:  \n",
    "  - If \\( f''(x) > 0 \\), **local minimum**  \n",
    "  - If \\( f''(x) < 0 \\), **local maximum**  \n",
    "\n",
    "### **Sample Questions**\n",
    "1. What is the role of the **learning rate** in gradient descent? What happens if it’s too high or too low?  \n",
    "2. Compute the derivative of:  \n",
    "   \\[\n",
    "   f(x) = x^3 - 3x^2 + 5x - 2\n",
    "   \\]\n",
    "3. What is the gradient of **f(x, y) = x² + y²**?  \n",
    "4. Explain why **gradient descent might get stuck in local minima** and how momentum-based optimization helps.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
