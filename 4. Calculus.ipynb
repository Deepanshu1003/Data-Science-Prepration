{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Calculus (Derivatives & Gradient Descent)**  \n",
    "\n",
    "### **Key Concepts**\n",
    "- **Derivatives**: Chain rule, Partial derivatives  \n",
    "- **Gradient Descent**: Learning rate, Convergence, Optimization  \n",
    "- **Convexity**: Local vs. Global minima  \n",
    "- **Hessian Matrix**: Second-order derivatives & optimization  \n",
    "\n",
    "### **Important Formulas**\n",
    "- **Gradient Descent Update Rule**:  \n",
    "  $$\n",
    "  \\theta_{new} = \\theta - \\alpha \\nabla f(\\theta)\n",
    "  $$\n",
    "  where $ \\alpha $ is the learning rate and $ \\nabla f(\\theta) $ is the gradient.  \n",
    "- **Chain Rule**:  \n",
    "  $$\n",
    "  \\frac{dy}{dx} = \\frac{dy}{du} \\cdot \\frac{du}{dx}\n",
    "  $$\n",
    "- **Second Derivative Test for Minima/Maxima**:  \n",
    "  - If $ f''(x) > 0 $, **local minimum**  \n",
    "  - If $ f''(x) < 0 $, **local maximum**  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Calculus for Data Science Interviews**  \n",
    "\n",
    "#### **1. Fundamentals of Calculus**  \n",
    "- Limits and Continuity  \n",
    "- Differentiation vs. Integration  \n",
    "- Functions and Their Behavior  \n",
    "\n",
    "#### **2. Derivatives & Their Applications**  \n",
    "- Definition of Derivatives (First Principles)  \n",
    "- Rules of Differentiation (Power, Product, Quotient, Chain Rule)  \n",
    "- Partial Derivatives (Multivariable Functions)  \n",
    "- Gradient and Directional Derivatives  \n",
    "\n",
    "#### **3. Optimization in Machine Learning**  \n",
    "- Critical Points: Local Minima, Maxima, and Saddle Points  \n",
    "- Convex vs. Non-Convex Functions  \n",
    "- Second Derivative and Hessian Matrix  \n",
    "- Taylor Series Approximation  \n",
    "\n",
    "#### **4. Gradient-Based Optimization**  \n",
    "- Gradient Descent Algorithm  \n",
    "  - Batch Gradient Descent  \n",
    "  - Stochastic Gradient Descent (SGD)  \n",
    "  - Mini-batch Gradient Descent  \n",
    "- Learning Rate and Convergence  \n",
    "- Momentum-Based Optimization (Nesterov Accelerated Gradient)  \n",
    "- Adaptive Optimization Algorithms (Adam, RMSprop, Adagrad)  \n",
    "\n",
    "#### **5. Jacobian & Hessian in Deep Learning**  \n",
    "- Jacobian Matrix (First-Order Derivatives for Vectors)  \n",
    "- Hessian Matrix (Second-Order Derivatives for Optimization)  \n",
    "- Eigenvalues and Eigenvectors for Curvature Analysis  \n",
    "\n",
    "#### **6. Backpropagation in Neural Networks**  \n",
    "- Chain Rule for Neural Networks  \n",
    "- Computing Gradients Using Backpropagation  \n",
    "- Vanishing & Exploding Gradient Problems  \n",
    "\n",
    "#### **7. Applications of Calculus in Data Science**  \n",
    "- Loss Functions & Optimization (MSE, Cross-Entropy)  \n",
    "- Regularization (L1 & L2 Norms)  \n",
    "- Logistic Regression & Softmax Function Derivatives  \n",
    "- Support Vector Machines (SVM) and the Role of Calculus  \n",
    "- Bayesian Optimization (Gradient-Based Inference)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# **1. Fundamentals of Calculus for Data Science**  \n",
    "\n",
    "Before diving into derivatives and optimization, let's establish a strong foundation in calculus.  \n",
    "\n",
    "---\n",
    "\n",
    "## **1.1 Limits and Continuity**  \n",
    "Limits define the behavior of a function as it approaches a particular value. Understanding limits is crucial for defining derivatives and integrals.  \n",
    "\n",
    "### **Definition of a Limit:**  \n",
    "$$\n",
    "\\lim_{x \\to a} f(x) = L\n",
    "$$\n",
    "This means that as $ x $ approaches $ a $, $ f(x) $ approaches $ L $.  \n",
    "\n",
    "#### **Key Concepts:**  \n",
    "- **Left-Hand Limit (LHL) & Right-Hand Limit (RHL):** A function is continuous at $ x = a $ if  \n",
    "  $$\n",
    "  \\lim_{x \\to a^-} f(x) = \\lim_{x \\to a^+} f(x) = f(a)\n",
    "  $$\n",
    "- **Indeterminate Forms:** Limits often result in forms like $ \\frac{0}{0} $ or $ \\infty - \\infty $, which require **L'HÃ´pitalâ€™s Rule** to evaluate.  \n",
    "- **Continuity:** A function is **continuous** at a point if its left-hand limit, right-hand limit, and function value exist and are equal.  \n",
    "\n",
    "#### **Python Example: Evaluating Limits**  \n",
    "Let's compute the limit of $ f(x) = \\frac{x^2 - 1}{x - 1} $ as $ x \\to 1 $.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Limit: 2\n"
     ]
    }
   ],
   "source": [
    "import sympy as sp\n",
    "\n",
    "x = sp.Symbol('x')\n",
    "f = (x**2 - 1) / (x - 1)\n",
    "limit_value = sp.limit(f, x, 1)\n",
    "print(\"Limit:\", limit_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## **1.2 Differentiation vs. Integration**  \n",
    "Differentiation and integration are fundamental operations in calculus.  \n",
    "\n",
    "- **Differentiation** measures the **rate of change** of a function.  \n",
    "- **Integration** computes the **accumulation** of a function over an interval (area under the curve).  \n",
    "\n",
    "| Concept | Definition | Notation |\n",
    "|---------|-----------|----------|\n",
    "| **Derivative** | Slope of a function at a point | $ f'(x) = \\frac{d}{dx} f(x) $ |\n",
    "| **Integral** | Area under the function curve | $ \\int f(x)dx $ |\n",
    "\n",
    "### **Python Example: Differentiation & Integration**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Derivative: 3*x**2 - 3\n",
      "Integral: x**4/4 - 3*x**2/2 + 5*x\n"
     ]
    }
   ],
   "source": [
    "f = x**3 - 3*x + 5\n",
    "\n",
    "# Differentiation\n",
    "derivative = sp.diff(f, x)\n",
    "print(\"Derivative:\", derivative)\n",
    "\n",
    "# Integration\n",
    "integral = sp.integrate(f, x)\n",
    "print(\"Integral:\", integral)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## **1.3 Functions and Their Behavior**  \n",
    "A function is a mathematical relationship where each input (x) maps to a single output (y).  \n",
    "\n",
    "### **Types of Functions in Data Science:**  \n",
    "- **Linear Functions:** $ f(x) = ax + b $  \n",
    "- **Polynomial Functions:** $ f(x) = ax^n + bx^{n-1} + ... $  \n",
    "- **Exponential & Logarithmic Functions:** Used in ML models like logistic regression.  \n",
    "- **Trigonometric Functions:** Used in Fourier Transforms for signal processing.  \n",
    "\n",
    "#### **Example: Visualizing Functions in Python**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.linspace(-10, 10, 400)\n",
    "y = x**2  # Quadratic function\n",
    "\n",
    "plt.plot(x, y, label=\"f(x) = x^2\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"f(x)\")\n",
    "plt.title(\"Function Visualization\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### **Key Takeaways**\n",
    "âœ… **Limits** define function behavior near a point.  \n",
    "âœ… **Differentiation vs. Integration** â€“ One finds the rate of change, the other accumulates.  \n",
    "âœ… **Functions shape machine learning models** â€“ Linear, polynomial, and exponential functions are widely used.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2. Derivatives & Their Applications in Data Science**  \n",
    "\n",
    "Derivatives play a key role in **optimization, machine learning algorithms, and neural networks**. Letâ€™s explore their definition, rules, and applications.\n",
    "\n",
    "---\n",
    "\n",
    "## **2.1 Definition of Derivatives (First Principles)**  \n",
    "A **derivative** measures how a function changes as its input changes. It is defined as:  \n",
    "\n",
    "$$\n",
    "f'(x) = \\lim_{h \\to 0} \\frac{f(x+h) - f(x)}{h}\n",
    "$$\n",
    "\n",
    "This gives the **instantaneous rate of change** of $ f(x) $ at a given point $ x $.  \n",
    "\n",
    "### **Example: Compute the Derivative of $ f(x) = x^2 $ Using First Principles**\n",
    "$$\n",
    "f'(x) = \\lim_{h \\to 0} \\frac{(x+h)^2 - x^2}{h}\n",
    "$$\n",
    "\n",
    "Expanding $ (x+h)^2 $:\n",
    "$$\n",
    "= \\lim_{h \\to 0} \\frac{x^2 + 2xh + h^2 - x^2}{h} = \\lim_{h \\to 0} \\frac{2xh + h^2}{h}\n",
    "$$\n",
    "\n",
    "Dividing by $ h $:\n",
    "$$\n",
    "= \\lim_{h \\to 0} (2x + h) = 2x\n",
    "$$\n",
    "\n",
    "Thus, **$ f'(x) = 2x $**.  \n",
    "\n",
    "### **Python Code for First Principles Derivative**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy as sp\n",
    "\n",
    "x, h = sp.symbols('x h')\n",
    "f = x**2  # Define the function\n",
    "derivative_limit = sp.limit((f.subs(x, x + h) - f) / h, h, 0)\n",
    "print(\"Derivative using first principles:\", derivative_limit)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## **2.2 Rules of Differentiation**  \n",
    "\n",
    "To compute derivatives efficiently, we use differentiation rules instead of first principles.  \n",
    "\n",
    "### **Power Rule**  \n",
    "$$\n",
    "\\frac{d}{dx} x^n = n x^{n-1}\n",
    "$$  \n",
    "ðŸ”¹ Example: $ f(x) = x^3 \\Rightarrow f'(x) = 3x^2 $  \n",
    "\n",
    "### **Sum Rule**  \n",
    "$$\n",
    "\\frac{d}{dx} [f(x) + g(x)] = f'(x) + g'(x)\n",
    "$$  \n",
    "\n",
    "### **Product Rule**  \n",
    "$$\n",
    "\\frac{d}{dx} [f(x) g(x)] = f'(x) g(x) + f(x) g'(x)\n",
    "$$  \n",
    "ðŸ”¹ Example: If $ f(x) = x^2 $ and $ g(x) = e^x $, then  \n",
    "$$\n",
    "f'(x) = 2x, \\quad g'(x) = e^x\n",
    "$$\n",
    "$$\n",
    "\\frac{d}{dx} (x^2 e^x) = 2x e^x + x^2 e^x\n",
    "$$\n",
    "\n",
    "### **Quotient Rule**  \n",
    "$$\n",
    "\\frac{d}{dx} \\left(\\frac{f(x)}{g(x)}\\right) = \\frac{f'(x) g(x) - f(x) g'(x)}{g(x)^2}\n",
    "$$  \n",
    "ðŸ”¹ Example: If $ f(x) = x^2 $ and $ g(x) = x+1 $, then  \n",
    "$$\n",
    "\\frac{d}{dx} \\left(\\frac{x^2}{x+1}\\right) = \\frac{(2x)(x+1) - x^2(1)}{(x+1)^2}\n",
    "$$\n",
    "\n",
    "### **Chain Rule**  \n",
    "$$\n",
    "\\frac{dy}{dx} = \\frac{dy}{du} \\cdot \\frac{du}{dx}\n",
    "$$  \n",
    "ðŸ”¹ Example: $ f(x) = (3x^2 + 5)^4 $  \n",
    "Let $ u = 3x^2 + 5 $, then  \n",
    "$$\n",
    "\\frac{du}{dx} = 6x, \\quad \\frac{d}{du} u^4 = 4u^3\n",
    "$$\n",
    "$$\n",
    "f'(x) = 4(3x^2 + 5)^3 \\cdot 6x = 24x (3x^2 + 5)^3\n",
    "$$\n",
    "\n",
    "### **Python Code for Chain Rule**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = sp.Function('u')(x)\n",
    "f = u**4\n",
    "u_expr = 3*x**2 + 5\n",
    "chain_rule_derivative = sp.diff(f, u) * sp.diff(u_expr, x)\n",
    "print(\"Derivative using chain rule:\", chain_rule_derivative)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## **2.3 Partial Derivatives (Multivariable Functions)**  \n",
    "A **partial derivative** computes the derivative of a function with respect to one variable while keeping others constant.\n",
    "\n",
    "### **Example: Compute Partial Derivatives for $ f(x, y) = x^2y + 3y^3 $**\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial x} = 2xy\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial y} = x^2 + 9y^2\n",
    "$$\n",
    "\n",
    "### **Python Code for Partial Derivatives**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = sp.symbols('x y')\n",
    "f = x**2 * y + 3*y**3\n",
    "\n",
    "# Partial derivatives\n",
    "df_dx = sp.diff(f, x)\n",
    "df_dy = sp.diff(f, y)\n",
    "\n",
    "print(\"âˆ‚f/âˆ‚x:\", df_dx)\n",
    "print(\"âˆ‚f/âˆ‚y:\", df_dy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "## **2.4 Gradient and Directional Derivatives**  \n",
    "\n",
    "### **Gradient (âˆ‡f)**\n",
    "The **gradient** of a function $ f(x, y) $ is a vector of partial derivatives:\n",
    "$$\n",
    "\\nabla f = \\left( \\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y} \\right)\n",
    "$$\n",
    "\n",
    "ðŸ”¹ Example: If $ f(x, y) = x^2 + y^2 $, then  \n",
    "$$\n",
    "\\nabla f = (2x, 2y)\n",
    "$$\n",
    "\n",
    "### **Directional Derivative**  \n",
    "The **directional derivative** measures the rate of change of a function in a particular direction **$ \\mathbf{v} = (a, b) $**:\n",
    "$$\n",
    "D_{\\mathbf{v}} f = \\nabla f \\cdot \\mathbf{v} = \\frac{\\partial f}{\\partial x} a + \\frac{\\partial f}{\\partial y} b\n",
    "$$\n",
    "\n",
    "### **Python Code for Gradient**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_f = [sp.diff(f, var) for var in (x, y)]\n",
    "print(\"Gradient âˆ‡f:\", grad_f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Takeaways**  \n",
    "âœ… **Derivatives** measure the rate of change of a function.  \n",
    "âœ… **Differentiation rules** (Power, Product, Chain) simplify calculations.  \n",
    "âœ… **Partial derivatives** extend derivatives to multivariable functions.  \n",
    "âœ… **Gradients & directional derivatives** are critical for optimization in ML.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Optimization in Machine Learning** ðŸŽ¯  \n",
    "\n",
    "Before diving into **Gradient Descent**, letâ€™s explore some **fundamental mathematical concepts** that shape optimization in machine learning. These concepts help us understand how models learn efficiently and avoid pitfalls like getting stuck in **local minima**.\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Critical Points: Local Minima, Maxima, and Saddle Points**  \n",
    "\n",
    "A **critical point** of a function $ f(x) $ is a point where the **gradient (first derivative) is zero**:\n",
    "$$\n",
    "\\nabla f(x) = 0\n",
    "$$\n",
    "However, not all critical points are **minima**. They can be:  \n",
    "\n",
    "### **1.1 Local Minimum**  \n",
    "- A point where **function value is smaller** than nearby points.  \n",
    "- Formally, $ f(x) $ has a **local minimum** at $ x = a $ if:  \n",
    "  $$\n",
    "  f(a) \\leq f(x) \\quad \\text{for all } x \\text{ near } a.\n",
    "  $$  \n",
    "\n",
    "ðŸ”¹ **Example**:  \n",
    "$$\n",
    "f(x) = x^2\n",
    "$$\n",
    "$ f'(x) = 2x = 0 $ at $ x = 0 $  \n",
    "$ f''(x) = 2 > 0 \\Rightarrow $ **Local Minimum at $ x = 0 $**  \n",
    "\n",
    "### **1.2 Local Maximum**  \n",
    "- A point where **function value is larger** than nearby points.  \n",
    "- Formally, $ f(x) $ has a **local maximum** at $ x = a $ if:  \n",
    "  $$\n",
    "  f(a) \\geq f(x) \\quad \\text{for all } x \\text{ near } a.\n",
    "  $$\n",
    "\n",
    "ðŸ”¹ **Example**:  \n",
    "$$\n",
    "f(x) = -x^2\n",
    "$$\n",
    "$ f'(x) = -2x = 0 $ at $ x = 0 $  \n",
    "$ f''(x) = -2 < 0 \\Rightarrow $ **Local Maximum at $ x = 0 $**  \n",
    "\n",
    "### **1.3 Saddle Point**\n",
    "- A point where **first derivative is zero**, but it's neither a minimum nor a maximum.  \n",
    "- **Has a mix of increasing and decreasing directions**.  \n",
    "\n",
    "ðŸ”¹ **Example**:  \n",
    "$$\n",
    "f(x, y) = x^2 - y^2\n",
    "$$\n",
    "$ \\nabla f = (2x, -2y) $ â†’ $ (0, 0) $ is a critical point.  \n",
    "However, along **$ x $-axis**, $ f(x,0) = x^2 $ (**minima**),  \n",
    "while along **$ y $-axis**, $ f(0,y) = -y^2 $ (**maxima**).  \n",
    "\n",
    "ðŸ”¹ **Saddle points in deep learning:** Optimization algorithms may slow down at saddle points instead of true minima.  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## **2. Convex vs. Non-Convex Functions**  \n",
    "\n",
    "### **2.1 Convex Functions**  \n",
    "A function is **convex** if the line segment connecting any two points on the function always lies **above** or **on** the function.  \n",
    "\n",
    "**Mathematical Definition:**  \n",
    "$ f(x) $ is convex if its **second derivative is non-negative**:  \n",
    "$$\n",
    "f''(x) \\geq 0\n",
    "$$\n",
    "\n",
    "ðŸ”¹ **Example of a Convex Function**:  \n",
    "$$\n",
    "f(x) = x^2\n",
    "$$\n",
    "$ f''(x) = 2 \\geq 0 $, so itâ€™s convex.  \n",
    "\n",
    "âœ… **Properties:**  \n",
    "- **One unique global minimum** (good for optimization).  \n",
    "- **Gradient Descent always converges** (if step size is correct).  \n",
    "\n",
    "---\n",
    "\n",
    "### **2.2 Non-Convex Functions**  \n",
    "A function is **non-convex** if the second derivative changes sign or the function has **multiple minima and maxima**.  \n",
    "\n",
    "ðŸ”¹ **Example of a Non-Convex Function**:  \n",
    "$$\n",
    "f(x) = x^4 - 3x^2 + 2\n",
    "$$\n",
    "$ f''(x) $ changes sign â†’ multiple minima and maxima.  \n",
    "\n",
    "âš ï¸ **Challenges:**  \n",
    "- May have **multiple local minima** â†’ Gradient Descent can get stuck in the wrong place.  \n",
    "- Harder to optimize in deep learning (but techniques like momentum and Adam optimizer help).  \n",
    "\n",
    "ðŸ”¹ **Visualization**:  \n",
    "| Function Type | Shape |\n",
    "|--------------|-------|\n",
    "| Convex | U-Shaped (One Global Minimum) |\n",
    "| Non-Convex | Multiple Valleys & Peaks |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## **3. Second Derivative & Hessian Matrix**  \n",
    "\n",
    "### **3.1 Second Derivative for 1D Functions**  \n",
    "The **second derivative** helps classify critical points.  \n",
    "\n",
    "$$\n",
    "f''(x) > 0 \\Rightarrow \\text{Local Minimum}\n",
    "$$\n",
    "$$\n",
    "f''(x) < 0 \\Rightarrow \\text{Local Maximum}\n",
    "$$\n",
    "\n",
    "ðŸ”¹ **Example:**  \n",
    "For $ f(x) = x^3 - 3x^2 + 5x - 2 $,  \n",
    "$$\n",
    "f'(x) = 3x^2 - 6x + 5\n",
    "$$\n",
    "$$\n",
    "f''(x) = 6x - 6\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### **3.2 Hessian Matrix for Multivariable Functions**  \n",
    "For functions with multiple variables, we use the **Hessian Matrix**:\n",
    "\n",
    "$$\n",
    "H = \\begin{bmatrix} \n",
    "\\frac{\\partial^2 f}{\\partial x^2} & \\frac{\\partial^2 f}{\\partial x \\partial y} \\\\ \n",
    "\\frac{\\partial^2 f}{\\partial y \\partial x} & \\frac{\\partial^2 f}{\\partial y^2} \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "#### **Classification Using Hessian**\n",
    "- If all **eigenvalues** of $ H $ are **positive**, function is **convex**.\n",
    "- If **mixed signs**, it's a **saddle point**.\n",
    "\n",
    "ðŸ”¹ **Python Code to Compute Hessian**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sympy import symbols, diff, hessian\n",
    "\n",
    "x, y = symbols('x y')\n",
    "f = x**2 + 3*x*y + y**2\n",
    "\n",
    "H = hessian(f, (x, y))\n",
    "print(H)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "âœ… **Hessian is useful for deep learning optimizers like Newtonâ€™s Method.**  \n",
    "\n",
    "---\n",
    "\n",
    "## **4. Taylor Series Approximation**  \n",
    "\n",
    "Taylor Series helps approximate functions using derivatives.\n",
    "\n",
    "ðŸ”¹ **Formula** for Taylor Series at $ x = a $:\n",
    "\n",
    "$$\n",
    "f(x) \\approx f(a) + f'(a)(x-a) + \\frac{f''(a)}{2!} (x-a)^2 + \\dots\n",
    "$$\n",
    "\n",
    "ðŸ”¹ **Example:**  \n",
    "Approximating $ e^x $ at $ x = 0 $:\n",
    "\n",
    "$$\n",
    "e^x \\approx 1 + x + \\frac{x^2}{2!} + \\frac{x^3}{3!} + \\dots\n",
    "$$\n",
    "\n",
    "ðŸ”¹ **Python Code for Taylor Expansion**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sympy as sp\n",
    "\n",
    "x = sp.Symbol('x')\n",
    "f = sp.exp(x)  # e^x function\n",
    "taylor_expansion = f.series(x, 0, 4).removeO()\n",
    "print(taylor_expansion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "âœ… **Why it matters?**  \n",
    "- Used in **Newtonâ€™s Method** for optimization.  \n",
    "- Helps understand **gradient-based** optimization behavior.  \n",
    "\n",
    "---\n",
    "\n",
    "## **Whatâ€™s Next? Gradient Descent! ðŸš€**  \n",
    "\n",
    "Now that we understand:\n",
    "âœ… **Critical Points**  \n",
    "âœ… **Convexity & Non-Convexity**  \n",
    "âœ… **Second Derivative & Hessian**  \n",
    "âœ… **Taylor Series for Approximation**  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# **Gradient-Based Optimization & Its Variants** ðŸš€  \n",
    "\n",
    "Gradient-based optimization is the backbone of machine learning and deep learning. Let's explore how it works and its different variants that improve efficiency and convergence.\n",
    "\n",
    "---\n",
    "\n",
    "## **1. What is Gradient-Based Optimization?**  \n",
    "\n",
    "Gradient-based optimization relies on **gradients (derivatives)** to adjust model parameters and minimize a **loss function**. It follows the direction of steepest descent using **partial derivatives**.\n",
    "\n",
    "ðŸ”¹ **Mathematical Formulation:**  \n",
    "For a function $ f(x) $, the gradient descent update rule is:  \n",
    "$$\n",
    "\\theta_{new} = \\theta - \\alpha \\nabla f(\\theta)\n",
    "$$\n",
    "where:  \n",
    "- $ \\theta $ = model parameter (e.g., weights in a neural network)  \n",
    "- $ \\alpha $ = learning rate (step size)  \n",
    "- $ \\nabla f(\\theta) $ = gradient (direction of steepest ascent)  \n",
    "\n",
    "---\n",
    "\n",
    "## **2. Basic Gradient Descent (Vanilla Gradient Descent)**  \n",
    "\n",
    "Gradient Descent updates parameters iteratively:  \n",
    "1. Compute the **gradient** of the function.  \n",
    "2. Move in the **opposite direction** of the gradient.  \n",
    "3. Repeat until convergence.  \n",
    "\n",
    "ðŸ”¹ **Example: Minimizing $ f(x) = x^2 $**  \n",
    "$$\n",
    "\\frac{d}{dx} f(x) = 2x\n",
    "$$\n",
    "If $ x = 4 $ and $ \\alpha = 0.1 $, then:  \n",
    "$$\n",
    "x_{new} = 4 - (0.1 \\times 2 \\times 4) = 3.2\n",
    "$$\n",
    "\n",
    "ðŸ”¹ **Python Code for Basic Gradient Descent**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function and its gradient\n",
    "def f(x):\n",
    "    return x**2\n",
    "\n",
    "def grad_f(x):\n",
    "    return 2*x\n",
    "\n",
    "# Gradient Descent Algorithm\n",
    "x = 4  # Initial value\n",
    "alpha = 0.1  # Learning rate\n",
    "iterations = 10\n",
    "\n",
    "for i in range(iterations):\n",
    "    x = x - alpha * grad_f(x)\n",
    "    print(f\"Iteration {i+1}: x = {x:.4f}, f(x) = {f(x):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "âœ… **Problems with Basic Gradient Descent:**  \n",
    "- **Too slow** when the learning rate is small.  \n",
    "- **Too fast (diverges)** if the learning rate is large.  \n",
    "- Can **get stuck in local minima** for non-convex functions.  \n",
    "\n",
    "---\n",
    "\n",
    "## **3. Variants of Gradient Descent**  \n",
    "\n",
    "### **3.1 Batch Gradient Descent (BGD)**\n",
    "- Uses the **entire dataset** to compute the gradient.  \n",
    "- Guarantees convergence to the **global minimum (for convex functions)**.  \n",
    "- **Slow** for large datasets.  \n",
    "\n",
    "ðŸ”¹ **Update Rule:**  \n",
    "$$\n",
    "\\theta = \\theta - \\alpha \\nabla J(\\theta)\n",
    "$$\n",
    "where $ J(\\theta) $ is the cost function computed over all training examples.\n",
    "\n",
    "---\n",
    "\n",
    "### **3.2 Stochastic Gradient Descent (SGD)**\n",
    "- Updates parameters using **one random sample at a time** instead of the entire dataset.  \n",
    "- **Faster** but has **higher variance** (noisy updates).  \n",
    "- Helps escape **local minima** due to randomness.  \n",
    "\n",
    "ðŸ”¹ **Python Code for SGD**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sgd(X, y, theta, alpha=0.01, epochs=10):\n",
    "    for epoch in range(epochs):\n",
    "        for i in range(len(X)):\n",
    "            grad = 2 * (theta * X[i] - y[i]) * X[i]\n",
    "            theta -= alpha * grad  # Parameter update\n",
    "    return theta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "âœ… **Trade-offs:**  \n",
    "- Faster but **less stable**.  \n",
    "- Works well for large-scale learning.  \n",
    "\n",
    "---\n",
    "\n",
    "### **3.3 Mini-Batch Gradient Descent**\n",
    "- Uses a **small batch** of data points (e.g., 32 or 64 samples).  \n",
    "- **Balances stability (BGD) and speed (SGD)**.  \n",
    "\n",
    "ðŸ”¹ **Update Rule:**  \n",
    "$$\n",
    "\\theta = \\theta - \\alpha \\nabla J(\\theta) \\quad \\text{(using a batch of data)}\n",
    "$$\n",
    "\n",
    "âœ… **Best choice for deep learning** ðŸš€  \n",
    "\n",
    "---\n",
    "\n",
    "## **4. Advanced Optimizers (Beyond Vanilla Gradient Descent)**  \n",
    "\n",
    "### **4.1 Momentum-Based Optimization**\n",
    "- Addresses **oscillations** by adding a **velocity term**.  \n",
    "- Helps move faster in relevant directions and dampens oscillations.  \n",
    "\n",
    "ðŸ”¹ **Momentum Update Rule:**  \n",
    "$$\n",
    "v = \\beta v + (1 - \\beta) \\nabla f(\\theta)\n",
    "$$\n",
    "$$\n",
    "\\theta = \\theta - \\alpha v\n",
    "$$\n",
    "\n",
    "ðŸ”¹ **Python Code for Momentum Optimizer**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def momentum_optimizer(grad_f, theta, alpha=0.1, beta=0.9, iterations=10):\n",
    "    v = 0  # Initialize velocity\n",
    "    for i in range(iterations):\n",
    "        v = beta * v + (1 - beta) * grad_f(theta)\n",
    "        theta = theta - alpha * v\n",
    "        print(f\"Iteration {i+1}: theta = {theta:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "âœ… **Benefits:**  \n",
    "- Reduces oscillations.  \n",
    "- Works well for deep learning.  \n",
    "\n",
    "---\n",
    "\n",
    "### **4.2 Adaptive Learning Rate Optimizers**  \n",
    "\n",
    "#### **4.2.1 AdaGrad**\n",
    "- Adapts learning rate based on **past gradients**.  \n",
    "- Works well for **sparse data** (e.g., NLP).  \n",
    "\n",
    "ðŸ”¹ **Update Rule:**  \n",
    "$$\n",
    "\\theta = \\theta - \\frac{\\alpha}{\\sqrt{G + \\epsilon}} \\nabla f(\\theta)\n",
    "$$\n",
    "\n",
    "where $ G $ is the sum of past squared gradients.  \n",
    "\n",
    "âœ… **Problem:** Learning rate **shrinks too much** over time.  \n",
    "\n",
    "---\n",
    "\n",
    "#### **4.2.2 RMSProp**\n",
    "- Fixes AdaGradâ€™s issue by using an **exponentially weighted average** of past squared gradients.  \n",
    "- Keeps learning rate adaptive without shrinking too much.  \n",
    "\n",
    "ðŸ”¹ **Update Rule:**  \n",
    "$$\n",
    "E[g^2]_t = \\beta E[g^2]_{t-1} + (1-\\beta) g^2_t\n",
    "$$\n",
    "$$\n",
    "\\theta = \\theta - \\frac{\\alpha}{\\sqrt{E[g^2]_t + \\epsilon}} \\nabla f(\\theta)\n",
    "$$\n",
    "\n",
    "âœ… **Great for recurrent neural networks (RNNs).**  \n",
    "\n",
    "---\n",
    "\n",
    "### **4.3 Adam Optimizer (Adaptive Moment Estimation)**\n",
    "**Most commonly used optimizer in deep learning.**  \n",
    "\n",
    "- **Combines** Momentum + RMSProp.  \n",
    "- Uses **first moment (mean of gradients)** and **second moment (variance of gradients)**.  \n",
    "- Adaptively adjusts learning rates for **each parameter**.  \n",
    "\n",
    "ðŸ”¹ **Update Rules:**  \n",
    "$$\n",
    "m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) g_t\n",
    "$$\n",
    "$$\n",
    "v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2\n",
    "$$\n",
    "$$\n",
    "\\theta = \\theta - \\alpha \\frac{m_t}{\\sqrt{v_t} + \\epsilon}\n",
    "$$\n",
    "\n",
    "ðŸ”¹ **Python Code for Adam Optimizer**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "âœ… **Best for deep learning models.**  \n",
    "\n",
    "---\n",
    "\n",
    "## **5. Summary & When to Use Which Optimizer?**  \n",
    "\n",
    "| Optimizer | Pros | Cons | Best For |\n",
    "|-----------|------|------|----------|\n",
    "| **SGD** | Fast, good for convex problems | Noisy updates | Large datasets |\n",
    "| **Momentum** | Faster convergence | Needs tuning | Deep learning |\n",
    "| **AdaGrad** | Adaptive learning rates | Learning rate decays too much | Sparse data (NLP) |\n",
    "| **RMSProp** | Solves AdaGrad issue | Needs hyperparameter tuning | RNNs, sequential models |\n",
    "| **Adam** | Best of all worlds | Computationally expensive | General deep learning |\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Learning Rate & Convergence in Optimization** ðŸš€  \n",
    "\n",
    "The **learning rate (Î±)** is a crucial hyperparameter in gradient-based optimization. It determines how much the model updates its parameters during training. If chosen incorrectly, it can **slow down training**, **cause divergence**, or **fail to find the optimal solution**.  \n",
    "\n",
    "---\n",
    "\n",
    "## **1. What is the Learning Rate (Î±)?**  \n",
    "The learning rate controls the step size in **Gradient Descent** when updating parameters.  \n",
    "\n",
    "ðŸ”¹ **Gradient Descent Update Rule:**  \n",
    "$$\n",
    "\\theta_{new} = \\theta - \\alpha \\nabla f(\\theta)\n",
    "$$\n",
    "where:  \n",
    "- $ \\alpha $ (learning rate) = step size  \n",
    "- $ \\nabla f(\\theta) $ = gradient of the function at $ \\theta $  \n",
    "\n",
    "### **Choosing the Right Learning Rate**  \n",
    "- **Too Small (Î± â†’ 0)** â†’ **Slow convergence**, takes many steps to reach the optimal point.  \n",
    "- **Too Large (Î± â†’ high value)** â†’ **Divergence**, overshooting the minimum and never converging.  \n",
    "- **Optimal (Î± = balanced)** â†’ **Fast convergence** without oscillations.  \n",
    "\n",
    "ðŸ”¹ **Visualizing the Effect of Learning Rate**  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Objective function\n",
    "def f(x):\n",
    "    return x**2\n",
    "\n",
    "# Gradient of the function\n",
    "def grad_f(x):\n",
    "    return 2*x\n",
    "\n",
    "# Gradient Descent Implementation\n",
    "def gradient_descent(alpha, x_init=4, iterations=20):\n",
    "    x = x_init\n",
    "    history = [x]\n",
    "    for _ in range(iterations):\n",
    "        x = x - alpha * grad_f(x)\n",
    "        history.append(x)\n",
    "    return history\n",
    "\n",
    "# Different learning rates\n",
    "alphas = [0.1, 0.5, 1.2]  # Small, optimal, large\n",
    "colors = ['r', 'g', 'b']\n",
    "x_vals = np.linspace(-5, 5, 100)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(x_vals, f(x_vals), label=\"Objective Function\")\n",
    "\n",
    "# Simulate GD for different learning rates\n",
    "for alpha, color in zip(alphas, colors):\n",
    "    path = gradient_descent(alpha)\n",
    "    plt.plot(path, f(np.array(path)), 'o-', color=color, label=f\"Î± = {alpha}\")\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"f(x)\")\n",
    "plt.title(\"Effect of Learning Rate on Gradient Descent\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "âœ… **Observations from the plot:**  \n",
    "- **Red (Î± = 0.1, small)** â†’ Slow progress towards minimum.  \n",
    "- **Green (Î± = 0.5, optimal)** â†’ Fast convergence.  \n",
    "- **Blue (Î± = 1.2, large)** â†’ Jumps around and may diverge.  \n",
    "\n",
    "---\n",
    "\n",
    "## **2. Convergence in Optimization**  \n",
    "\n",
    "### **What is Convergence?**  \n",
    "A model is said to **converge** when the updates (parameter changes) become very small, and the loss function stops decreasing significantly.  \n",
    "\n",
    "ðŸ”¹ **Mathematical Condition for Convergence:**  \n",
    "$$\n",
    "\\| \\theta_{t+1} - \\theta_t \\| < \\epsilon\n",
    "$$\n",
    "where $ \\epsilon $ is a small threshold value.  \n",
    "\n",
    "### **How to Check for Convergence?**  \n",
    "- **Loss function decrease:** If the loss function remains **almost constant**, the model may have converged.  \n",
    "- **Gradient norm:** If $ \\nabla f(\\theta) $ is close to **zero**, the model has reached an optimal point.  \n",
    "\n",
    "ðŸ”¹ **Python Code to Detect Convergence**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged at iteration 62 with x = 0.000004\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[4,\n",
       " 3.2,\n",
       " 2.56,\n",
       " 2.048,\n",
       " 1.6384,\n",
       " 1.31072,\n",
       " 1.0485760000000002,\n",
       " 0.8388608000000002,\n",
       " 0.6710886400000001,\n",
       " 0.5368709120000001,\n",
       " 0.4294967296000001,\n",
       " 0.3435973836800001,\n",
       " 0.27487790694400005,\n",
       " 0.21990232555520003,\n",
       " 0.17592186044416003,\n",
       " 0.140737488355328,\n",
       " 0.11258999068426241,\n",
       " 0.09007199254740993,\n",
       " 0.07205759403792794,\n",
       " 0.057646075230342354,\n",
       " 0.04611686018427388,\n",
       " 0.03689348814741911,\n",
       " 0.029514790517935284,\n",
       " 0.02361183241434823,\n",
       " 0.018889465931478583,\n",
       " 0.015111572745182867,\n",
       " 0.012089258196146294,\n",
       " 0.009671406556917036,\n",
       " 0.007737125245533628,\n",
       " 0.006189700196426903,\n",
       " 0.004951760157141522,\n",
       " 0.003961408125713218,\n",
       " 0.0031691265005705745,\n",
       " 0.00253530120045646,\n",
       " 0.0020282409603651678,\n",
       " 0.0016225927682921343,\n",
       " 0.0012980742146337075,\n",
       " 0.001038459371706966,\n",
       " 0.0008307674973655728,\n",
       " 0.0006646139978924582,\n",
       " 0.0005316911983139665,\n",
       " 0.00042535295865117324,\n",
       " 0.0003402823669209386,\n",
       " 0.00027222589353675085,\n",
       " 0.0002177807148294007,\n",
       " 0.00017422457186352054,\n",
       " 0.00013937965749081642,\n",
       " 0.00011150372599265314,\n",
       " 8.920298079412252e-05,\n",
       " 7.136238463529802e-05,\n",
       " 5.7089907708238416e-05,\n",
       " 4.567192616659073e-05,\n",
       " 3.653754093327259e-05,\n",
       " 2.923003274661807e-05,\n",
       " 2.3384026197294454e-05,\n",
       " 1.8707220957835564e-05,\n",
       " 1.4965776766268452e-05,\n",
       " 1.1972621413014761e-05,\n",
       " 9.578097130411809e-06,\n",
       " 7.662477704329448e-06,\n",
       " 6.129982163463559e-06,\n",
       " 4.903985730770847e-06]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def gradient_descent_convergence(alpha, x_init=4, epsilon=1e-6, max_iter=100):\n",
    "    x = x_init\n",
    "    history = [x]\n",
    "    for i in range(max_iter):\n",
    "        grad = grad_f(x)\n",
    "        x_new = x - alpha * grad\n",
    "        if abs(x_new - x) < epsilon:  # Convergence check\n",
    "            print(f\"Converged at iteration {i+1} with x = {x_new:.6f}\")\n",
    "            break\n",
    "        x = x_new\n",
    "        history.append(x)\n",
    "    return history\n",
    "\n",
    "gradient_descent_convergence(0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "âœ… **Practical Guidelines for Convergence:**  \n",
    "- Set a **maximum number of iterations** to prevent infinite loops.  \n",
    "- Monitor **loss value** and **gradient norm** to confirm if optimization is still improving.  \n",
    "- Use a **learning rate schedule** (decay over time) to ensure smooth convergence.  \n",
    "\n",
    "---\n",
    "\n",
    "## **3. Learning Rate Scheduling (Dynamic Learning Rates)**  \n",
    "\n",
    "Instead of using a **fixed** learning rate, a **learning rate scheduler** dynamically adjusts it during training.  \n",
    "\n",
    "### **Types of Learning Rate Schedules**  \n",
    "1ï¸âƒ£ **Step Decay** â†’ Reduce learning rate every few epochs.  \n",
    "$$\n",
    "\\alpha_t = \\alpha_0 \\times \\gamma^{\\lfloor \\frac{t}{step\\_size} \\rfloor}\n",
    "$$\n",
    "\n",
    "2ï¸âƒ£ **Exponential Decay** â†’ Reduce learning rate exponentially.  \n",
    "$$\n",
    "\\alpha_t = \\alpha_0 e^{-\\lambda t}\n",
    "$$\n",
    "\n",
    "3ï¸âƒ£ **Adaptive Learning Rate (Adam, RMSProp)** â†’ Adjust learning rate per parameter using past gradients.  \n",
    "\n",
    "ðŸ”¹ **Python Code for Exponential Decay**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "initial_learning_rate = 0.1\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate, decay_steps=100, decay_rate=0.96, staircase=True\n",
    ")\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "âœ… **Why Use Learning Rate Scheduling?**  \n",
    "- Speeds up convergence.  \n",
    "- Prevents divergence at later stages of training.  \n",
    "- Helps escape **local minima** in non-convex functions.  \n",
    "\n",
    "---\n",
    "\n",
    "## **4. Summary**  \n",
    "\n",
    "| Learning Rate (Î±) | Effect on Training |\n",
    "|-----------------|----------------|\n",
    "| **Too Small** | Very slow convergence |\n",
    "| **Too Large** | May oscillate or diverge |\n",
    "| **Optimal** | Fast and stable convergence |\n",
    "\n",
    "ðŸ”¹ **Best Practices:**  \n",
    "- Start with a moderate learning rate (e.g., **0.01 or 0.001** for deep learning).  \n",
    "- Monitor **loss function** and adjust $ \\alpha $ dynamically.  \n",
    "- Use **adaptive optimizers** (e.g., **Adam, RMSProp**) to avoid manual tuning.  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Jacobian, Hessian, and Eigenvalues in Deep Learning** ðŸš€  \n",
    "\n",
    "Jacobian and Hessian matrices play a crucial role in optimization, backpropagation, and stability analysis in deep learning. Eigenvalues help in understanding curvature and convergence properties of optimization algorithms.  \n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## **1. Jacobian Matrix ðŸ“Œ**  \n",
    "\n",
    "The **Jacobian matrix** represents the **first-order partial derivatives** of a vector-valued function. It describes **how small changes in input affect the output**, making it important for backpropagation in neural networks.\n",
    "\n",
    "### **Mathematical Definition**  \n",
    "For a function **f**: $ \\mathbb{R}^n \\to \\mathbb{R}^m $, the **Jacobian matrix** $ J $ is:  \n",
    "$$\n",
    "J_{ij} = \\frac{\\partial f_i}{\\partial x_j}\n",
    "$$\n",
    "where:  \n",
    "- $ f(x) $ is a vector-valued function.  \n",
    "- $ x $ is an input vector.  \n",
    "- $ J $ is an $ m \\times n $ matrix of first-order derivatives.  \n",
    "\n",
    "ðŸ”¹ **Example: Simple Function**  \n",
    "$$\n",
    "f(x, y) = \\begin{bmatrix} x^2 + y \\\\ \\sin(x) + y^2 \\end{bmatrix}\n",
    "$$\n",
    "The Jacobian matrix is:\n",
    "$$\n",
    "J = \\begin{bmatrix} 2x & 1 \\\\ \\cos(x) & 2y \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "ðŸ”¹ **Python Code to Compute Jacobian**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sympy import symbols, Matrix, diff\n",
    "\n",
    "x, y = symbols('x y')\n",
    "f = Matrix([x**2 + y, np.sin(x) + y**2])\n",
    "\n",
    "J = f.jacobian([x, y])\n",
    "print(J)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "âœ… **Jacobian is used in:**  \n",
    "- Backpropagation in deep learning.  \n",
    "- Sensitivity analysis of neural networks.  \n",
    "- Linearization of complex functions.  \n",
    "\n",
    "---\n",
    "\n",
    "## **2. Hessian Matrix ðŸ“Œ**  \n",
    "\n",
    "The **Hessian matrix** contains **second-order partial derivatives** of a scalar function. It captures curvature information, making it crucial for **optimization** (e.g., Newtonâ€™s method).  \n",
    "\n",
    "### **Mathematical Definition**  \n",
    "For a function **f**: $ \\mathbb{R}^n \\to \\mathbb{R} $, the **Hessian matrix** $ H $ is:\n",
    "$$\n",
    "H_{ij} = \\frac{\\partial^2 f}{\\partial x_i \\partial x_j}\n",
    "$$\n",
    "where $ H $ is an $ n \\times n $ matrix.  \n",
    "\n",
    "ðŸ”¹ **Example: Scalar Function**  \n",
    "$$\n",
    "f(x, y) = x^2 + xy + y^2\n",
    "$$\n",
    "Hessian:\n",
    "$$\n",
    "H = \\begin{bmatrix} \\frac{\\partial^2 f}{\\partial x^2} & \\frac{\\partial^2 f}{\\partial x \\partial y} \\\\ \\frac{\\partial^2 f}{\\partial y \\partial x} & \\frac{\\partial^2 f}{\\partial y^2} \\end{bmatrix} = \\begin{bmatrix} 2 & 1 \\\\ 1 & 2 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "ðŸ”¹ **Python Code for Hessian Computation**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sympy import hessian\n",
    "\n",
    "f = x**2 + x*y + y**2\n",
    "H = hessian(f, (x, y))\n",
    "print(H)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "âœ… **Hessian is used in:**  \n",
    "- Newtonâ€™s optimization method.  \n",
    "- Stability analysis of critical points.  \n",
    "- Second-order optimization methods (e.g., BFGS).  \n",
    "\n",
    "---\n",
    "\n",
    "## **3. Eigenvalues & Eigenvectors ðŸ“Œ**  \n",
    "\n",
    "Eigenvalues help in understanding **curvature**, **stability**, and **convergence** in optimization problems.  \n",
    "\n",
    "### **Mathematical Definition**  \n",
    "For a matrix $ A $, an eigenvalue $ \\lambda $ and an eigenvector $ v $ satisfy:\n",
    "$$\n",
    "Av = \\lambda v\n",
    "$$\n",
    "where:  \n",
    "- $ \\lambda $ represents the **scaling factor**.  \n",
    "- $ v $ is a nonzero vector.  \n",
    "\n",
    "### **Hessian & Eigenvalues Interpretation**  \n",
    "Eigenvalues of the **Hessian** help determine **convexity**:  \n",
    "- **All positive eigenvalues** â†’ Convex function (**local minimum**).  \n",
    "- **All negative eigenvalues** â†’ Concave function (**local maximum**).  \n",
    "- **Mixed signs** â†’ Saddle point.  \n",
    "\n",
    "ðŸ”¹ **Python Code to Compute Eigenvalues**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "H = np.array([[2, 1], [1, 2]])\n",
    "eigenvalues, eigenvectors = np.linalg.eig(H)\n",
    "\n",
    "print(\"Eigenvalues:\", eigenvalues)\n",
    "print(\"Eigenvectors:\\n\", eigenvectors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "âœ… **Eigenvalues are used in:**  \n",
    "- Determining convexity of a function.  \n",
    "- Principal Component Analysis (PCA) for dimensionality reduction.  \n",
    "- Stability analysis in deep learning models.  \n",
    "\n",
    "---\n",
    "\n",
    "## **4. Summary ðŸš€**  \n",
    "\n",
    "| **Concept**  | **Definition**  | **Key Application**  |\n",
    "|-------------|----------------|----------------------|\n",
    "| **Jacobian** | First-order derivatives matrix | Backpropagation, sensitivity analysis |\n",
    "| **Hessian** | Second-order derivatives matrix | Optimization, Newtonâ€™s method |\n",
    "| **Eigenvalues** | Scalars describing transformation properties | PCA, stability analysis |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Backpropagation & Gradients in Deep Learning** ðŸš€  \n",
    "\n",
    "Backpropagation is the foundation of how neural networks learn. It relies on **computing gradients using the chain rule**, which updates weights through **gradient descent**. However, issues like **vanishing and exploding gradients** can slow down or destabilize learning.  \n",
    "\n",
    "---\n",
    "\n",
    "## **1. Chain Rule in Backpropagation** ðŸ”„  \n",
    "\n",
    "Neural networks are **composed of multiple layers**, and the error (loss) must be **propagated backward** to update the weights efficiently. The **chain rule** is used to compute gradients through each layer.  \n",
    "\n",
    "### **Mathematical Definition**  \n",
    "If a function $ y $ depends on $ u $, and $ u $ depends on $ x $, then:  \n",
    "$$\n",
    "\\frac{dy}{dx} = \\frac{dy}{du} \\cdot \\frac{du}{dx}\n",
    "$$\n",
    "\n",
    "### **Example: Simple Chain Rule**  \n",
    "$$\n",
    "z = (x^2 + 3x)^{10}\n",
    "$$\n",
    "Using the chain rule:\n",
    "$$\n",
    "\\frac{dz}{dx} = 10(x^2 + 3x)^9 \\cdot (2x + 3)\n",
    "$$\n",
    "\n",
    "ðŸ”¹ **Python Code for Chain Rule**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sympy import symbols, diff\n",
    "\n",
    "x = symbols('x')\n",
    "u = x**2 + 3*x\n",
    "z = u**10\n",
    "\n",
    "dz_dx = diff(z, x)  # Compute derivative\n",
    "print(dz_dx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "âœ… **In backpropagation, the chain rule helps propagate gradients through multiple layers.**  \n",
    "\n",
    "---\n",
    "\n",
    "## **2. Computing Gradients in Backpropagation** ðŸ“‰  \n",
    "\n",
    "The **gradient of a function** represents the rate of change of a function with respect to its inputs. In neural networks, we compute gradients **with respect to weights and biases** using backpropagation.\n",
    "\n",
    "### **Gradient Computation in Neural Networks**\n",
    "For a neural network layer:  \n",
    "$$\n",
    "z = W x + b\n",
    "$$\n",
    "where:  \n",
    "- $ W $ is the weight matrix,  \n",
    "- $ x $ is the input,  \n",
    "- $ b $ is the bias.  \n",
    "\n",
    "The activation function $ a = f(z) $ applies non-linearity. The loss function $ L $ is minimized by computing:  \n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W}, \\quad \\frac{\\partial L}{\\partial b}\n",
    "$$\n",
    "\n",
    "ðŸ”¹ **Example: Gradient of a Single Neuron**  \n",
    "$$\n",
    "L = (y - \\hat{y})^2\n",
    "$$\n",
    "Using chain rule:\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W} = \\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial z} \\cdot \\frac{\\partial z}{\\partial W}\n",
    "$$\n",
    "\n",
    "ðŸ”¹ **Python Code for Gradient Computation**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Define input and weight\n",
    "x = torch.tensor([2.0], requires_grad=True)\n",
    "w = torch.tensor([3.0], requires_grad=True)\n",
    "b = torch.tensor([1.0], requires_grad=True)\n",
    "\n",
    "# Compute output and loss\n",
    "z = w * x + b\n",
    "loss = (z - 5)**2\n",
    "\n",
    "# Compute gradients\n",
    "loss.backward()\n",
    "\n",
    "print(\"Gradient w.r.t w:\", w.grad)\n",
    "print(\"Gradient w.r.t b:\", b.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "âœ… **Gradients are used to update weights using gradient descent.**  \n",
    "\n",
    "---\n",
    "\n",
    "## **3. Vanishing & Exploding Gradients** âš ï¸  \n",
    "\n",
    "As gradients are backpropagated through deep networks, they may **become too small (vanish)** or **too large (explode)**, leading to poor training efficiency.\n",
    "\n",
    "### **Vanishing Gradient Problem**  \n",
    "- Occurs when gradients become **very small** in deep networks.  \n",
    "- **Problem**: Weight updates shrink, slowing down learning.  \n",
    "- **Causes**: Sigmoid/tanh activations in deep networks lead to derivatives close to 0.  \n",
    "\n",
    "ðŸ”¹ **Solution:**  \n",
    "âœ… **ReLU Activation**: ReLU (Rectified Linear Unit) avoids the problem by having gradients of **1** for positive values.  \n",
    "âœ… **Batch Normalization**: Normalizes activations to stabilize gradients.  \n",
    "âœ… **Xavier/He Initialization**: Proper weight initialization prevents small gradients.\n",
    "\n",
    "ðŸ”¹ **Python Example: Vanishing Gradient with Sigmoid**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "sigmoid = nn.Sigmoid()\n",
    "x = torch.tensor([-10.0, 0.0, 10.0], requires_grad=True)\n",
    "\n",
    "y = sigmoid(x)\n",
    "y.backward(torch.ones_like(x))  # Compute gradient\n",
    "\n",
    "print(x.grad)  # Gradients are very small for large |x|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **Exploding Gradient Problem**  \n",
    "- Occurs when gradients become **too large** in deep networks.  \n",
    "- **Problem**: Weights grow exponentially, leading to instability.  \n",
    "- **Causes**: Unstable weight updates due to large derivatives.\n",
    "\n",
    "ðŸ”¹ **Solution:**  \n",
    "âœ… **Gradient Clipping**: Limits gradient values to prevent instability.  \n",
    "âœ… **Adaptive Optimizers (Adam, RMSprop)**: Adjust learning rates to prevent overshooting.  \n",
    "âœ… **Proper Weight Initialization**: Ensures balanced gradients.\n",
    "\n",
    "ðŸ”¹ **Python Example: Gradient Clipping**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "w = torch.tensor([100.0], requires_grad=True)\n",
    "loss = w**2  # Large gradient\n",
    "loss.backward()\n",
    "\n",
    "torch.nn.utils.clip_grad_norm_([w], max_norm=10)  # Clipping\n",
    "\n",
    "print(w.grad)  # Gradient is clipped\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Summary ðŸš€**  \n",
    "\n",
    "| **Concept**  | **Definition**  | **Key Application**  |\n",
    "|-------------|----------------|----------------------|\n",
    "| **Chain Rule** | Computes derivatives of composite functions | Backpropagation in deep learning |\n",
    "| **Computing Gradients** | Measures weight updates using loss derivatives | Optimizing neural networks |\n",
    "| **Vanishing Gradients** | Gradients shrink too much, slowing learning | Use ReLU, BatchNorm, Xavier Init |\n",
    "| **Exploding Gradients** | Gradients grow too large, making training unstable | Use Gradient Clipping, Adam |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Backpropagation in Deep Learning** ðŸš€  \n",
    "\n",
    "Backpropagation (short for **\"backward propagation of errors\"**) is the fundamental algorithm that allows neural networks to **learn** by adjusting their weights based on the error they produce. It uses **gradient descent** and the **chain rule** to update parameters in deep learning models.  \n",
    "\n",
    "---\n",
    "\n",
    "## **1. Understanding Backpropagation: How Does It Work?**  \n",
    "\n",
    "### **Forward Pass** ðŸ¹  \n",
    "1ï¸âƒ£ **Inputs** $ x $ are passed through the network layer by layer.  \n",
    "2ï¸âƒ£ Each neuron applies a **linear transformation** using weights $ W $ and biases $ b $.  \n",
    "3ï¸âƒ£ An **activation function** $ f $ (e.g., ReLU, Sigmoid) is applied to introduce non-linearity.  \n",
    "4ï¸âƒ£ The output $ y_{\\text{pred}} $ is compared to the true value $ y_{\\text{true}} $, and the **loss function** calculates the error.  \n",
    "\n",
    "$$\n",
    "z = W x + b, \\quad a = f(z)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### **Backward Pass (Gradient Computation)** ðŸ”„  \n",
    "1ï¸âƒ£ Compute the **loss function gradient** (how much the error changes with respect to output).  \n",
    "2ï¸âƒ£ Use the **chain rule** to propagate this error back through the layers.  \n",
    "3ï¸âƒ£ Compute gradients for each weight $ W $ and bias $ b $.  \n",
    "4ï¸âƒ£ Update weights using **gradient descent**:\n",
    "\n",
    "$$\n",
    "W_{\\text{new}} = W - \\alpha \\frac{\\partial L}{\\partial W}\n",
    "$$\n",
    "\n",
    "where $ \\alpha $ is the **learning rate**.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Mathematical Explanation of Backpropagation** ðŸ§®  \n",
    "\n",
    "Consider a **simple neural network** with one hidden layer:\n",
    "\n",
    "$$\n",
    "y_{\\text{pred}} = f(W_2 a + b_2)\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "$$\n",
    "a = f(W_1 x + b_1)\n",
    "$$\n",
    "\n",
    "### **Step 1: Compute the Loss Function**\n",
    "We define a **loss function** $ L $, such as Mean Squared Error (MSE):\n",
    "\n",
    "$$\n",
    "L = \\frac{1}{2} (y_{\\text{pred}} - y_{\\text{true}})^2\n",
    "$$\n",
    "\n",
    "### **Step 2: Compute Gradients Using the Chain Rule**\n",
    "Using **gradient descent**, we update weights by computing **partial derivatives**:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W_2} = \\frac{\\partial L}{\\partial y_{\\text{pred}}} \\cdot \\frac{\\partial y_{\\text{pred}}}{\\partial W_2}\n",
    "$$\n",
    "\n",
    "Similarly, for the hidden layer:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W_1} = \\frac{\\partial L}{\\partial y_{\\text{pred}}} \\cdot \\frac{\\partial y_{\\text{pred}}}{\\partial a} \\cdot \\frac{\\partial a}{\\partial W_1}\n",
    "$$\n",
    "\n",
    "ðŸ”¹ This process **propagates the error backward**, allowing us to adjust weights **proportionally to their impact on the loss**.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Python Implementation of Backpropagation** ðŸ  \n",
    "\n",
    "### **Manual Backpropagation for a Simple Neural Network**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sigmoid activation function and its derivative\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)  # Derivative of sigmoid\n",
    "\n",
    "# Input data (X) and output labels (y)\n",
    "X = np.array([[0,0], [0,1], [1,0], [1,1]])  # XOR input\n",
    "y = np.array([[0], [1], [1], [0]])  # XOR output\n",
    "\n",
    "# Initialize weights randomly\n",
    "np.random.seed(42)\n",
    "W1 = np.random.rand(2, 2)\n",
    "W2 = np.random.rand(2, 1)\n",
    "\n",
    "learning_rate = 0.1\n",
    "epochs = 10000\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    Z1 = np.dot(X, W1)\n",
    "    A1 = sigmoid(Z1)\n",
    "    Z2 = np.dot(A1, W2)\n",
    "    A2 = sigmoid(Z2)\n",
    "\n",
    "    # Compute loss\n",
    "    loss = y - A2\n",
    "\n",
    "    # Backward pass\n",
    "    dA2 = loss * sigmoid_derivative(A2)  # Output layer gradient\n",
    "    dW2 = np.dot(A1.T, dA2)  # Weight update for W2\n",
    "\n",
    "    dA1 = np.dot(dA2, W2.T) * sigmoid_derivative(A1)  # Hidden layer gradient\n",
    "    dW1 = np.dot(X.T, dA1)  # Weight update for W1\n",
    "\n",
    "    # Update weights\n",
    "    W2 += learning_rate * dW2\n",
    "    W1 += learning_rate * dW1\n",
    "\n",
    "    # Print loss every 1000 epochs\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {np.mean(np.abs(loss))}\")\n",
    "\n",
    "print(\"Training Complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "ðŸ”¹ **Key Steps in the Code:**\n",
    "1. **Forward Pass**: Compute activations.\n",
    "2. **Backward Pass**: Compute gradients using the **chain rule**.\n",
    "3. **Weight Updates**: Use **gradient descent** to update weights.\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Vanishing & Exploding Gradient Problem** âš ï¸  \n",
    "\n",
    "When **backpropagation goes wrong**, it is usually due to:  \n",
    "ðŸ”¸ **Vanishing Gradients** â€“ Gradients become **too small**, slowing down learning.  \n",
    "ðŸ”¸ **Exploding Gradients** â€“ Gradients become **too large**, making training unstable.  \n",
    "\n",
    "### **Solutions**\n",
    "âœ… Use **ReLU activation** instead of Sigmoid to prevent vanishing gradients.  \n",
    "âœ… Apply **Batch Normalization** to stabilize weight updates.  \n",
    "âœ… Use **Gradient Clipping** for exploding gradients.\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Summary & Next Steps ðŸš€**  \n",
    "\n",
    "| **Concept**  | **Definition**  | **Key Role in Backpropagation**  |\n",
    "|-------------|----------------|----------------------|\n",
    "| **Forward Pass** | Computes output from input | Determines prediction |\n",
    "| **Loss Function** | Measures error | Guides weight updates |\n",
    "| **Chain Rule** | Computes gradients layer-by-layer | Enables error propagation |\n",
    "| **Gradient Descent** | Updates weights using gradients | Optimizes network learning |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Applications of Calculus in Data Science**  \n",
    "\n",
    "Calculus plays a crucial role in various machine learning and deep learning algorithms, primarily in optimization, model training, and regularization. Let's explore key applications where **derivatives, gradients, and optimization techniques** are fundamental.  \n",
    "\n",
    "---\n",
    "\n",
    "## **1. Loss Functions & Optimization** ðŸ”¥  \n",
    "In machine learning, a **loss function** quantifies the difference between the modelâ€™s prediction and the actual target value. The goal is to minimize this loss using **gradient-based optimization**.  \n",
    "\n",
    "### **Common Loss Functions**\n",
    "1ï¸âƒ£ **Mean Squared Error (MSE)** (Regression):  \n",
    "   - Used in linear regression and neural networks.  \n",
    "   - Penalizes large errors more than small errors.  \n",
    "   - Formula:\n",
    "\n",
    "$$\n",
    "\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "   - **Gradient (Derivative of MSE w.r.t model parameters):**\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{MSE}}{\\partial W} = \\frac{2}{n} \\sum (y_i - \\hat{y}_i) \\cdot (-X_i)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "2ï¸âƒ£ **Cross-Entropy Loss** (Classification):  \n",
    "   - Used in logistic regression, softmax classifiers, and neural networks.  \n",
    "   - Measures the difference between two probability distributions.  \n",
    "   - Formula (Binary Classification):\n",
    "\n",
    "$$\n",
    "\\text{CE Loss} = -\\frac{1}{n} \\sum_{i=1}^{n} y_i \\log (\\hat{y}_i) + (1 - y_i) \\log (1 - \\hat{y}_i)\n",
    "$$\n",
    "\n",
    "   - **Gradient of Cross-Entropy Loss (for logistic regression):**  \n",
    "     \n",
    "$$\n",
    "\\frac{\\partial \\text{Loss}}{\\partial W} = X^T (\\hat{y} - y)\n",
    "$$\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## **2. Regularization (L1 & L2 Norms) ðŸ“‰**  \n",
    "Regularization helps prevent **overfitting** by adding penalties to model parameters.  \n",
    "\n",
    "1ï¸âƒ£ **L1 Regularization (Lasso)**  \n",
    "   - Encourages sparsity (some weights become **exactly zero**).  \n",
    "   - **Penalty term:**\n",
    "\n",
    "$$\n",
    "\\lambda \\sum |W|\n",
    "$$\n",
    "\n",
    "   - **Gradient of L1 Regularization:**\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial W} (\\lambda |W|) = \\lambda \\cdot \\text{sign}(W)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "2ï¸âƒ£ **L2 Regularization (Ridge Regression)**  \n",
    "   - Encourages **small weights** but does not set them to zero.  \n",
    "   - **Penalty term:**\n",
    "\n",
    "$$\n",
    "\\lambda \\sum W^2\n",
    "$$\n",
    "\n",
    "   - **Gradient of L2 Regularization:**\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial W} (\\lambda W^2) = 2 \\lambda W\n",
    "$$\n",
    "\n",
    "ðŸ’¡ **Regularized Loss = Loss Function + Regularization Term**\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## **3. Logistic Regression & Softmax Function Derivatives**  \n",
    "\n",
    "### **Logistic Regression & Sigmoid Function**  \n",
    "Logistic regression predicts probabilities using the **sigmoid function**:\n",
    "\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "ðŸ”¹ **Derivative of Sigmoid**:\n",
    "\n",
    "$$\n",
    "\\frac{d\\sigma}{dz} = \\sigma(z) (1 - \\sigma(z))\n",
    "$$\n",
    "\n",
    "ðŸ’¡ Used in **binary classification** models like spam detection and fraud detection.\n",
    "\n",
    "---\n",
    "\n",
    "### **Softmax Function & Multiclass Classification**  \n",
    "Softmax is used in **multiclass classification** to compute probabilities:\n",
    "\n",
    "$$\n",
    "\\sigma(z_i) = \\frac{e^{z_i}}{\\sum_{j} e^{z_j}}\n",
    "$$\n",
    "\n",
    "ðŸ”¹ **Derivative of Softmax** (Jacobian Matrix):\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\sigma_i}{\\partial z_j} =\n",
    "\\begin{cases}\n",
    "\\sigma_i (1 - \\sigma_i), & i = j \\\\\n",
    "- \\sigma_i \\sigma_j, & i \\neq j\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "ðŸ’¡ Used in models like **image classification (CNNs), NLP (Transformers), and recommendation systems**.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## **4. Support Vector Machines (SVM) and the Role of Calculus**  \n",
    "\n",
    "SVMs find an **optimal hyperplane** that separates data points in classification tasks.  \n",
    "\n",
    "ðŸ”¹ **Hinge Loss Function**:  \n",
    "   - Loss for incorrectly classified points:\n",
    "\n",
    "$$\n",
    "L = \\sum \\max(0, 1 - y_i (W X_i + b))\n",
    "$$\n",
    "\n",
    "ðŸ”¹ **Gradient of Hinge Loss**:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W} =\n",
    "\\begin{cases}\n",
    "0, & \\text{if } y_i (W X_i + b) \\geq 1 \\\\\n",
    "- y_i X_i, & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "ðŸ’¡ SVMs are useful for **high-dimensional classification**, like **face recognition and text classification**.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## **5. Bayesian Optimization (Gradient-Based Inference)**  \n",
    "\n",
    "Bayesian Optimization is used when function evaluations are **expensive** (e.g., hyperparameter tuning in ML models). It relies on:  \n",
    "ðŸ”¹ **Gaussian Processes** to model the objective function.  \n",
    "ðŸ”¹ **Expected Improvement (EI)** for selecting the next sample point.  \n",
    "\n",
    "ðŸ”¹ **Gradient of Gaussian Process Mean Function**:\n",
    "\n",
    "$$\n",
    "\\mu(x) = k(x, X) K^{-1} y\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\nabla \\mu(x) = \\nabla k(x, X) K^{-1} y\n",
    "$$\n",
    "\n",
    "ðŸ’¡ Used in **hyperparameter tuning**, e.g., **optimizing learning rates in deep learning**.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## **Summary Table** ðŸŽ¯  \n",
    "\n",
    "| **Concept** | **Equation** | **Use Case** |\n",
    "|------------|------------|------------|\n",
    "| **MSE Loss** | $ \\frac{1}{n} \\sum (y_i - \\hat{y}_i)^2 $ | Regression models |\n",
    "| **Cross-Entropy Loss** | $ -\\sum y \\log \\hat{y} $ | Classification |\n",
    "| **L1 Regularization** | $ \\lambda |W| $ | Sparse models (Lasso) |\n",
    "| **L2 Regularization** | $ \\lambda W^2 $ | Prevents large weights (Ridge) |\n",
    "| **Sigmoid Derivative** | $ \\sigma(z)(1 - \\sigma(z)) $ | Logistic Regression |\n",
    "| **Softmax Derivative** | $ \\sigma_i (1 - \\sigma_i) $ | Multiclass Classification |\n",
    "| **SVM Gradient** | $ - y_i X_i $ | Support Vector Machines |\n",
    "| **Bayesian Optimization Gradient** | $ \\nabla \\mu(x) = \\nabla k(x, X) K^{-1} y $ | Hyperparameter tuning |\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
