{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Linear Algebra**  \n",
    "\n",
    "### **Key Concepts**\n",
    "- **Vectors & Matrices**: Addition, Multiplication, Transpose  \n",
    "- **Determinant & Inverse**: Singular matrices, Determinant rules  \n",
    "- **Eigenvalues & Eigenvectors**: Spectral decomposition, PCA  \n",
    "- **Matrix Factorization**: SVD (Singular Value Decomposition), QR decomposition  \n",
    "\n",
    "### **Important Formulas**\n",
    "- **Dot Product**:  \n",
    "  $$\n",
    "  \\mathbf{a} \\cdot \\mathbf{b} = \\sum a_i b_i\n",
    "  $$\n",
    "- **Matrix Multiplication**:  \n",
    "  $$\n",
    "  (AB)_{ij} = \\sum_{k} A_{ik} B_{kj}\n",
    "  $$\n",
    "- **Determinant of a 2√ó2 Matrix**:  \n",
    "  $$\n",
    "  \\det \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix} = ad - bc\n",
    "  $$\n",
    "- **Eigenvalue Equation**:  \n",
    "  $$\n",
    "  Ax = \\lambda x\n",
    "  $$\n",
    "  where $ \\lambda $ are eigenvalues, and $ x $ are eigenvectors.  \n",
    "\n",
    "### **Sample Questions**\n",
    "1. What is the **geometric interpretation** of eigenvectors?  \n",
    "2. If **A is a 3√ó3 matrix** with determinant = 0, what can you say about its invertibility?  \n",
    "3. Compute the eigenvalues of  \n",
    "   $$\n",
    "   A = \\begin{bmatrix} 4 & 2 \\\\ 1 & 3 \\end{bmatrix}\n",
    "   $$  \n",
    "4. Explain how **PCA (Principal Component Analysis)** uses eigenvalues and eigenvectors.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## **Linear Algebra for Data Science Interviews**  \n",
    "\n",
    "### **1Ô∏è‚É£ Basics of Vectors and Matrices**  \n",
    "- Definition of Vectors and Matrices  \n",
    "- Vector Operations: Addition, Subtraction, Scalar Multiplication  \n",
    "- Matrix Operations: Addition, Multiplication, Transpose  \n",
    "- Dot Product & Cross Product  \n",
    "\n",
    "### **2Ô∏è‚É£ Properties of Matrices**  \n",
    "- Identity Matrix & Zero Matrix  \n",
    "- Diagonal, Symmetric, and Orthogonal Matrices  \n",
    "- Rank of a Matrix  \n",
    "- Trace of a Matrix  \n",
    "\n",
    "### **3Ô∏è‚É£ Determinant & Inverse of a Matrix**  \n",
    "- Determinant of a Matrix (2√ó2, 3√ó3, n√ón)  \n",
    "- Properties of Determinants  \n",
    "- Singular vs. Non-Singular Matrices  \n",
    "- Inverse of a Matrix & Conditions for Invertibility  \n",
    "\n",
    "### **4Ô∏è‚É£ Eigenvalues & Eigenvectors**  \n",
    "- Definition and Interpretation  \n",
    "- Characteristic Equation  \n",
    "- Spectral Decomposition  \n",
    "- Diagonalization of Matrices  \n",
    "\n",
    "### **5Ô∏è‚É£ Matrix Factorization Techniques**  \n",
    "- **Singular Value Decomposition (SVD)**  \n",
    "- **QR Decomposition**  \n",
    "- **LU Decomposition**  \n",
    "\n",
    "### **6Ô∏è‚É£ Applications in Data Science & Machine Learning**  \n",
    "- **Principal Component Analysis (PCA)** ‚Äì How it uses Eigenvalues & Eigenvectors  \n",
    "- **Dimensionality Reduction** ‚Äì Role of Matrix Factorization  \n",
    "- **Linear Regression** ‚Äì Normal Equations & Least Squares Solution  \n",
    "- **Neural Networks** ‚Äì Role of Matrices in Weights & Activations  \n",
    "- **Recommendation Systems** ‚Äì Matrix Factorization in Collaborative Filtering  \n",
    "\n",
    "### **7Ô∏è‚É£ Special Concepts & Theorems**  \n",
    "- Cramer‚Äôs Rule  \n",
    "- Rank-Nullity Theorem  \n",
    "- Moore-Penrose Pseudoinverse  \n",
    "- Frobenius Norm & Spectral Norm  \n",
    "\n",
    "Do you want to dive into each section in this order, or should we start with a specific topic? üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **1Ô∏è‚É£ Basics of Vectors and Matrices**  \n",
    "\n",
    "### **1.1 Vectors**  \n",
    "A **vector** is an ordered list of numbers. It can be represented as:  \n",
    "$$\n",
    "\\mathbf{v} = \\begin{bmatrix} v_1 \\\\ v_2 \\\\ v_3 \\end{bmatrix}\n",
    "$$\n",
    "where $ v_1, v_2, v_3 $ are elements of the vector.  \n",
    "\n",
    "#### **Operations on Vectors**  \n",
    "1. **Vector Addition & Subtraction**  \n",
    "   - Two vectors of the same dimension can be added or subtracted element-wise.\n",
    "\n",
    "2. **Scalar Multiplication**  \n",
    "   - Multiplying a vector by a scalar scales each element.\n",
    "\n",
    "3. **Dot Product**  \n",
    "   - The **dot product** of two vectors $ \\mathbf{a} $ and $ \\mathbf{b} $ is given by:  \n",
    "     $$\n",
    "     \\mathbf{a} \\cdot \\mathbf{b} = \\sum a_i b_i\n",
    "     $$\n",
    "\n",
    "### **Python Example**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector Addition: [3 3 3]\n",
      "Scalar Multiplication: [ 6  9 12]\n",
      "Dot Product: -2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "# Define vectors\n",
    "v1 = np.array([2, 3, 4])\n",
    "v2 = np.array([1, 0, -1])\n",
    "\n",
    "# Vector addition\n",
    "add_result = v1 + v2\n",
    "\n",
    "# Scalar multiplication\n",
    "scalar_mult = 3 * v1\n",
    "\n",
    "# Dot product\n",
    "dot_product = np.dot(v1, v2)\n",
    "\n",
    "print(\"Vector Addition:\", add_result)\n",
    "print(\"Scalar Multiplication:\", scalar_mult)\n",
    "print(\"Dot Product:\", dot_product)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **1.2 Matrices**  \n",
    "A **matrix** is a 2D array of numbers, represented as:  \n",
    "$$\n",
    "A = \\begin{bmatrix} \n",
    "a_{11} & a_{12} \\\\ \n",
    "a_{21} & a_{22} \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "#### **Operations on Matrices**\n",
    "1. **Matrix Addition & Subtraction**  \n",
    "   - Can be performed if both matrices have the same dimensions.\n",
    "  \n",
    "2. **Matrix Multiplication**  \n",
    "   - If $ A $ is an $ m \\times n $ matrix and $ B $ is an $ n \\times p $ matrix, then $ AB $ results in an $ m \\times p $ matrix.\n",
    "   - Formula:  \n",
    "     $$\n",
    "     (AB)_{ij} = \\sum_{k} A_{ik} B_{kj}\n",
    "     $$\n",
    "\n",
    "3. **Matrix Transpose**  \n",
    "   - Flips rows into columns:  \n",
    "     $$\n",
    "     A^T_{ij} = A_{ji}\n",
    "     $$\n",
    "\n",
    "### **Python Example**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix Addition:\n",
      " [[3 2]\n",
      " [4 7]]\n",
      "Matrix Multiplication:\n",
      " [[ 4  6]\n",
      " [10 12]]\n",
      "Transpose of A:\n",
      " [[1 3]\n",
      " [2 4]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define matrices\n",
    "A = np.array([[1, 2], [3, 4]])\n",
    "B = np.array([[2, 0], [1, 3]])\n",
    "\n",
    "# Matrix addition\n",
    "add_matrix = A + B\n",
    "\n",
    "# Matrix multiplication\n",
    "mult_matrix = np.dot(A, B)  # Or use A @ B\n",
    "\n",
    "# Transpose of a matrix\n",
    "transpose_A = A.T\n",
    "\n",
    "print(\"Matrix Addition:\\n\", add_matrix)\n",
    "print(\"Matrix Multiplication:\\n\", mult_matrix)\n",
    "print(\"Transpose of A:\\n\", transpose_A)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### **1.3 Special Matrices**  \n",
    "1. **Identity Matrix ($ I $)**  \n",
    "   - A square matrix with 1s on the diagonal and 0s elsewhere.\n",
    "   - Example:\n",
    "     $$\n",
    "     I = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix}\n",
    "     $$\n",
    "\n",
    "2. **Zero Matrix ($ 0 $)**  \n",
    "   - A matrix with all elements as 0.\n",
    "\n",
    "3. **Diagonal Matrix**  \n",
    "   - A matrix where all non-diagonal elements are zero.\n",
    "\n",
    "### **Python Example**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identity Matrix:\n",
      " [[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n",
      "Zero Matrix:\n",
      " [[0. 0.]\n",
      " [0. 0.]]\n",
      "Diagonal Matrix:\n",
      " [[4 0 0]\n",
      " [0 5 0]\n",
      " [0 0 6]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Identity matrix\n",
    "I = np.eye(3)\n",
    "\n",
    "# Zero matrix\n",
    "Z = np.zeros((2, 2))\n",
    "\n",
    "# Diagonal matrix\n",
    "D = np.diag([4, 5, 6])\n",
    "\n",
    "print(\"Identity Matrix:\\n\", I)\n",
    "print(\"Zero Matrix:\\n\", Z)\n",
    "print(\"Diagonal Matrix:\\n\", D)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "## **Key Takeaways**\n",
    "- **Vectors** are 1D arrays, and **matrices** are 2D arrays.  \n",
    "- **Basic operations** like addition, scalar multiplication, and dot product apply to vectors.  \n",
    "- **Matrix multiplication** requires conforming dimensions.  \n",
    "- **Special matrices** like identity, zero, and diagonal have unique properties.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Dot Product & Cross Product in Linear Algebra**  \n",
    "\n",
    "Both **dot product** and **cross product** are fundamental operations in vector algebra, widely used in data science, physics, and machine learning.  \n",
    "\n",
    "---\n",
    "\n",
    "## **Dot Product (Scalar Product)**\n",
    "The **dot product** of two vectors $ \\mathbf{a} $ and $ \\mathbf{b} $ is given by:  \n",
    "$$\n",
    "\\mathbf{a} \\cdot \\mathbf{b} = \\sum a_i b_i\n",
    "$$\n",
    "or in summation notation:  \n",
    "$$\n",
    "\\mathbf{a} \\cdot \\mathbf{b} = a_1b_1 + a_2b_2 + \\dots + a_n b_n\n",
    "$$\n",
    "\n",
    "### **Geometric Interpretation**\n",
    "- The dot product measures the **similarity** between two vectors.\n",
    "- If $ \\theta $ is the angle between two vectors:\n",
    "  $$\n",
    "  \\mathbf{a} \\cdot \\mathbf{b} = ||\\mathbf{a}|| ||\\mathbf{b}|| \\cos\\theta\n",
    "  $$\n",
    "- **If $ \\theta = 90^\\circ $ (perpendicular vectors), the dot product is 0** (orthogonal vectors).  \n",
    "- **If $ \\theta = 0^\\circ $, the vectors are in the same direction (maximum similarity).**\n",
    "\n",
    "### **Python Example: Dot Product**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dot Product: -2\n",
      "Cosine of Angle: -0.2626128657194451\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "# Define vectors\n",
    "a = np.array([2, 3, 4])\n",
    "b = np.array([1, 0, -1])\n",
    "\n",
    "# Compute dot product\n",
    "dot_product = np.dot(a, b)\n",
    "\n",
    "# Compute angle (cosine similarity)\n",
    "cos_theta = dot_product / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "print(\"Dot Product:\", dot_product)\n",
    "print(\"Cosine of Angle:\", cos_theta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "üîπ **Use Case in Data Science**:  \n",
    "- Used in **cosine similarity** (e.g., text similarity in NLP).  \n",
    "- Measures how aligned two feature vectors are in ML.\n",
    "\n",
    "---\n",
    "\n",
    "## **Cross Product (Vector Product)**\n",
    "The **cross product** is defined only for **3D vectors** and results in another **vector** perpendicular to both input vectors.  \n",
    "\n",
    "$$\n",
    "\\mathbf{a} \\times \\mathbf{b} =\n",
    "\\begin{bmatrix} a_1 \\\\ a_2 \\\\ a_3 \\end{bmatrix}\n",
    "\\times\n",
    "\\begin{bmatrix} b_1 \\\\ b_2 \\\\ b_3 \\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "a_2 b_3 - a_3 b_2 \\\\\n",
    "a_3 b_1 - a_1 b_3 \\\\\n",
    "a_1 b_2 - a_2 b_1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "### **Geometric Interpretation**\n",
    "- The **resulting vector is perpendicular to both $ \\mathbf{a} $ and $ \\mathbf{b} $**.\n",
    "- The **magnitude** (length) of the cross product is:\n",
    "  $$\n",
    "  ||\\mathbf{a} \\times \\mathbf{b}|| = ||\\mathbf{a}|| ||\\mathbf{b}|| \\sin\\theta\n",
    "  $$\n",
    "  where $ \\theta $ is the angle between $ \\mathbf{a} $ and $ \\mathbf{b} $.\n",
    "\n",
    "### **Python Example: Cross Product**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Product: [-3  6 -3]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define 3D vectors\n",
    "a = np.array([2, 3, 4])\n",
    "b = np.array([1, 0, -1])\n",
    "\n",
    "# Compute cross product\n",
    "cross_product = np.cross(a, b)\n",
    "\n",
    "print(\"Cross Product:\", cross_product)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "üîπ **Use Case in Data Science & AI**:  \n",
    "- Used in **computer graphics & physics simulations**.  \n",
    "- Helps in **calculating normals** to planes in 3D space.\n",
    "\n",
    "---\n",
    "\n",
    "## **Key Differences**\n",
    "| Feature         | Dot Product | Cross Product |\n",
    "|----------------|------------|--------------|\n",
    "| **Result**     | Scalar (number) | Vector |\n",
    "| **Definition** | $ \\sum a_i b_i $ | Determinant formula |\n",
    "| **Dimension**  | Works for any n-dimensional vectors | Only for 3D vectors |\n",
    "| **Use Case**   | Similarity measurement, ML, NLP | 3D physics, graphics, robotics |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2Ô∏è‚É£ Properties of Vectors and Matrices**  \n",
    "\n",
    "Understanding the **properties of vectors and matrices** is crucial for data science, machine learning, and numerical computing. This section will cover:  \n",
    "\n",
    "1. **Properties of Vectors**  \n",
    "2. **Properties of Matrices**  \n",
    "\n",
    "We'll include **Python examples** to reinforce the concepts.\n",
    "\n",
    "---\n",
    "\n",
    "## **Properties of Vectors**  \n",
    "\n",
    "### **1.1 Vector Addition Properties**  \n",
    "For any vectors **$ a $** and **$ b $**:  \n",
    "\n",
    "1. **Commutative Property**:  \n",
    "   $$\n",
    "   \\mathbf{a} + \\mathbf{b} = \\mathbf{b} + \\mathbf{a}\n",
    "   $$\n",
    "   - Order of addition doesn‚Äôt matter.\n",
    "\n",
    "2. **Associative Property**:  \n",
    "   $$\n",
    "   (\\mathbf{a} + \\mathbf{b}) + \\mathbf{c} = \\mathbf{a} + (\\mathbf{b} + \\mathbf{c})\n",
    "   $$\n",
    "   - Grouping doesn‚Äôt affect the result.\n",
    "\n",
    "3. **Additive Identity**:  \n",
    "   $$\n",
    "   \\mathbf{a} + \\mathbf{0} = \\mathbf{a}\n",
    "   $$\n",
    "   - Adding the **zero vector** results in the same vector.\n",
    "\n",
    "4. **Additive Inverse**:  \n",
    "   $$\n",
    "   \\mathbf{a} + (-\\mathbf{a}) = \\mathbf{0}\n",
    "   $$\n",
    "   - A vector plus its negative results in the **zero vector**.\n",
    "\n",
    "### **Python Example**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.array([2, 3, 4])\n",
    "b = np.array([1, -1, 2])\n",
    "zero_vector = np.zeros(3)\n",
    "\n",
    "# Commutative Property\n",
    "print(\"a + b:\", a + b)\n",
    "print(\"b + a:\", b + a)\n",
    "\n",
    "# Associative Property\n",
    "c = np.array([-2, 1, 0])\n",
    "print(\"(a + b) + c:\", (a + b) + c)\n",
    "print(\"a + (b + c):\", a + (b + c))\n",
    "\n",
    "# Additive Identity\n",
    "print(\"a + zero_vector:\", a + zero_vector)\n",
    "\n",
    "# Additive Inverse\n",
    "print(\"a + (-a):\", a + (-a))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **1.2 Scalar Multiplication Properties**\n",
    "For a vector $ \\mathbf{a} $ and scalars $ c $ and $ d $:\n",
    "\n",
    "1. **Distributive Property (Vector Addition)**:  \n",
    "   $$\n",
    "   c (\\mathbf{a} + \\mathbf{b}) = c\\mathbf{a} + c\\mathbf{b}\n",
    "   $$\n",
    "2. **Distributive Property (Scalar Addition)**:  \n",
    "   $$\n",
    "   (c + d) \\mathbf{a} = c\\mathbf{a} + d\\mathbf{a}\n",
    "   $$\n",
    "3. **Associative Property**:  \n",
    "   $$\n",
    "   c (d\\mathbf{a}) = (cd)\\mathbf{a}\n",
    "   $$\n",
    "4. **Multiplicative Identity**:  \n",
    "   $$\n",
    "   1\\mathbf{a} = \\mathbf{a}\n",
    "   $$\n",
    "\n",
    "---\n",
    "\n",
    "## **Properties of Matrices**  \n",
    "\n",
    "A **matrix** is a rectangular array of numbers arranged in **rows** and **columns**.  \n",
    "\n",
    "### **2.1 Matrix Addition Properties**  \n",
    "For matrices $ A, B, C $ of the same size:\n",
    "\n",
    "1. **Commutative Property**:  \n",
    "   $$\n",
    "   A + B = B + A\n",
    "   $$\n",
    "2. **Associative Property**:  \n",
    "   $$\n",
    "   (A + B) + C = A + (B + C)\n",
    "   $$\n",
    "3. **Additive Identity** (Zero Matrix $ 0 $):  \n",
    "   $$\n",
    "   A + 0 = A\n",
    "   $$\n",
    "4. **Additive Inverse**:  \n",
    "   $$\n",
    "   A + (-A) = 0\n",
    "   $$\n",
    "\n",
    "### **Python Example**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[1, 2], [3, 4]])\n",
    "B = np.array([[2, 3], [4, 5]])\n",
    "zero_matrix = np.zeros((2, 2))\n",
    "\n",
    "# Commutative Property\n",
    "print(\"A + B:\\n\", A + B)\n",
    "print(\"B + A:\\n\", B + A)\n",
    "\n",
    "# Associative Property\n",
    "C = np.array([[5, 6], [7, 8]])\n",
    "print(\"(A + B) + C:\\n\", (A + B) + C)\n",
    "print(\"A + (B + C):\\n\", A + (B + C))\n",
    "\n",
    "# Additive Identity\n",
    "print(\"A + zero_matrix:\\n\", A + zero_matrix)\n",
    "\n",
    "# Additive Inverse\n",
    "print(\"A + (-A):\\n\", A + (-A))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### **2.2 Matrix Multiplication Properties**  \n",
    "For matrices $ A, B, C $ where multiplication is defined:\n",
    "\n",
    "1. **Associative Property**:  \n",
    "   $$\n",
    "   (AB)C = A(BC)\n",
    "   $$\n",
    "   - Grouping of multiplication does not matter.\n",
    "\n",
    "2. **Distributive Property**:  \n",
    "   $$\n",
    "   A(B + C) = AB + AC\n",
    "   $$\n",
    "   - Matrix multiplication distributes over addition.\n",
    "\n",
    "3. **Multiplicative Identity**:  \n",
    "   $$\n",
    "   AI = A\n",
    "   $$\n",
    "   - Multiplication with the identity matrix $ I $ returns the same matrix.\n",
    "\n",
    "4. **Non-Commutativity** (Important!):  \n",
    "   $$\n",
    "   AB \\neq BA\n",
    "   $$\n",
    "   - Unlike scalar multiplication, matrix multiplication is **not** commutative.\n",
    "\n",
    "### **Python Example**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I = np.eye(2)  # Identity Matrix\n",
    "A = np.array([[1, 2], [3, 4]])\n",
    "B = np.array([[2, 0], [1, 3]])\n",
    "\n",
    "# Associative Property\n",
    "print(\"(AB)C:\\n\", np.dot(np.dot(A, B), I))\n",
    "print(\"A(BC):\\n\", np.dot(A, np.dot(B, I)))\n",
    "\n",
    "# Distributive Property\n",
    "C = np.array([[5, 6], [7, 8]])\n",
    "print(\"A(B + C):\\n\", np.dot(A, B + C))\n",
    "print(\"AB + AC:\\n\", np.dot(A, B) + np.dot(A, C))\n",
    "\n",
    "# Identity Property\n",
    "print(\"AI:\\n\", np.dot(A, I))\n",
    "\n",
    "# Non-Commutativity\n",
    "print(\"AB:\\n\", np.dot(A, B))\n",
    "print(\"BA:\\n\", np.dot(B, A))  # Will be different from AB\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **2.3 Special Matrices**\n",
    "1. **Identity Matrix $ I $**  \n",
    "   - A square matrix with **1s on the diagonal** and **0s elsewhere**.  \n",
    "   - Multiplying any matrix $ A $ with $ I $ gives $ A $.\n",
    "\n",
    "2. **Zero Matrix $ 0 $**  \n",
    "   - All elements are zero.\n",
    "   - Multiplication with any matrix gives a **zero matrix**.\n",
    "\n",
    "3. **Diagonal Matrix**  \n",
    "   - Non-zero elements exist only on the diagonal.\n",
    "   - Example:\n",
    "     $$\n",
    "     D = \\begin{bmatrix} 3 & 0 \\\\ 0 & 5 \\end{bmatrix}\n",
    "     $$\n",
    "\n",
    "4. **Symmetric Matrix**  \n",
    "   - A matrix $ A $ is symmetric if:\n",
    "     $$\n",
    "     A^T = A\n",
    "     $$\n",
    "   - Example:\n",
    "     $$\n",
    "     \\begin{bmatrix} 1 & 2 \\\\ 2 & 3 \\end{bmatrix}\n",
    "     $$\n",
    "\n",
    "### **Python Example**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Identity Matrix\n",
    "I = np.eye(3)\n",
    "print(\"Identity Matrix:\\n\", I)\n",
    "\n",
    "# Zero Matrix\n",
    "Z = np.zeros((3, 3))\n",
    "print(\"Zero Matrix:\\n\", Z)\n",
    "\n",
    "# Diagonal Matrix\n",
    "D = np.diag([3, 5, 7])\n",
    "print(\"Diagonal Matrix:\\n\", D)\n",
    "\n",
    "# Symmetric Matrix\n",
    "S = np.array([[1, 2], [2, 3]])\n",
    "print(\"Symmetric Matrix:\\n\", S)\n",
    "print(\"Transpose of S:\\n\", S.T)  # Should be same as S\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Exploring Orthogonal Matrix, Rank, and Trace of a Matrix**  \n",
    "\n",
    "These three properties are important in **linear algebra** and have applications in **machine learning, data science, and optimization**. Let's explore them with **definitions, properties, examples, and Python code**.  \n",
    "\n",
    "---\n",
    "\n",
    "## **Orthogonal Matrix**  \n",
    "\n",
    "### **Definition**  \n",
    "A **square matrix $ A $** is **orthogonal** if its **transpose is equal to its inverse**:  \n",
    "\n",
    "$$\n",
    "A^T A = A A^T = I\n",
    "$$\n",
    "\n",
    "where:  \n",
    "- $ A^T $ is the **transpose** of $ A $  \n",
    "- $ I $ is the **identity matrix**  \n",
    "\n",
    "### **Properties of Orthogonal Matrices**  \n",
    "1. **Preserves length (Norm is unchanged)**: $ ||Ax|| = ||x|| $  \n",
    "2. **Determinant is ¬±1**: $ \\det(A) = \\pm 1 $  \n",
    "3. **Preserves dot product**: $ (Ax) \\cdot (Ay) = x \\cdot y $  \n",
    "4. **Inverse is its transpose**: $ A^{-1} = A^T $  \n",
    "\n",
    "### **Example of an Orthogonal Matrix**\n",
    "$$\n",
    "A = \\begin{bmatrix} 0 & 1 \\\\ -1 & 0 \\end{bmatrix}\n",
    "$$\n",
    "$$\n",
    "A^T = \\begin{bmatrix} 0 & -1 \\\\ 1 & 0 \\end{bmatrix}\n",
    "$$\n",
    "$$\n",
    "A^T A = \\begin{bmatrix} 0 & -1 \\\\ 1 & 0 \\end{bmatrix} \\cdot \\begin{bmatrix} 0 & 1 \\\\ -1 & 0 \\end{bmatrix} = I\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### **Python Example: Checking Orthogonality**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define a matrix\n",
    "A = np.array([[0, 1], [-1, 0]])\n",
    "\n",
    "# Compute A^T * A\n",
    "orthogonality_check = np.dot(A.T, A)\n",
    "\n",
    "# Check if it equals the identity matrix\n",
    "is_orthogonal = np.allclose(orthogonality_check, np.eye(2))\n",
    "\n",
    "print(\"Matrix A:\\n\", A)\n",
    "print(\"A^T * A:\\n\", orthogonality_check)\n",
    "print(\"Is A orthogonal?\", is_orthogonal)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## **Rank of a Matrix**  \n",
    "\n",
    "### **Definition**  \n",
    "The **rank of a matrix** is the **number of linearly independent rows or columns**. It tells us:  \n",
    "- The **dimensionality** of the column space (range) of the matrix.  \n",
    "- Whether a system of linear equations has **a unique solution, infinite solutions, or no solution**.  \n",
    "\n",
    "### **Key Properties**  \n",
    "1. **$ \\text{Rank}(A) \\leq \\min(m, n) $ for an $ m \\times n $ matrix**  \n",
    "2. **Full rank**:\n",
    "   - If $ \\text{Rank}(A) = n $ (number of columns), it's **full column rank** (invertible if square).  \n",
    "   - If $ \\text{Rank}(A) = m $ (number of rows), it's **full row rank**.  \n",
    "3. **A singular matrix has rank $ < n $** (not invertible).  \n",
    "\n",
    "---\n",
    "\n",
    "### **Example of Rank Calculation**\n",
    "$$\n",
    "A = \\begin{bmatrix} 1 & 2 \\\\ 2 & 4 \\end{bmatrix}\n",
    "$$\n",
    "- **Second row is 2√ó the first row ‚Üí Linearly dependent**  \n",
    "- **Rank(A) = 1 (not full rank)**  \n",
    "\n",
    "---\n",
    "\n",
    "### **Python Example: Compute Matrix Rank**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[1, 2], [2, 4]])\n",
    "rank_A = np.linalg.matrix_rank(A)\n",
    "\n",
    "print(\"Matrix A:\\n\", A)\n",
    "print(\"Rank of A:\", rank_A)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## **Trace of a Matrix**  \n",
    "\n",
    "### **Definition**  \n",
    "The **trace of a square matrix** is the **sum of its diagonal elements**:  \n",
    "\n",
    "$$\n",
    "\\text{Tr}(A) = \\sum_{i} A_{ii}\n",
    "$$\n",
    "\n",
    "### **Properties**  \n",
    "1. **Trace of sum**:  \n",
    "   $$\n",
    "   \\text{Tr}(A + B) = \\text{Tr}(A) + \\text{Tr}(B)\n",
    "   $$\n",
    "2. **Trace of product (only if order matches)**:  \n",
    "   $$\n",
    "   \\text{Tr}(AB) = \\text{Tr}(BA)\n",
    "   $$\n",
    "3. **Trace of identity matrix**:  \n",
    "   $$\n",
    "   \\text{Tr}(I_n) = n\n",
    "   $$\n",
    "\n",
    "---\n",
    "\n",
    "### **Example of Trace Calculation**\n",
    "$$\n",
    "A = \\begin{bmatrix} 3 & 5 \\\\ 1 & 4 \\end{bmatrix}\n",
    "$$\n",
    "$$\n",
    "\\text{Tr}(A) = 3 + 4 = 7\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### **Python Example: Compute Matrix Trace**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[3, 5], [1, 4]])\n",
    "trace_A = np.trace(A)\n",
    "\n",
    "print(\"Matrix A:\\n\", A)\n",
    "print(\"Trace of A:\", trace_A)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "## **Key Takeaways**\n",
    "- **Vectors** and **matrices** follow commutative, associative, and distributive properties for addition.\n",
    "- **Matrix multiplication is associative and distributive but NOT commutative**.\n",
    "- **Special matrices** (identity, zero, diagonal, symmetric) have unique properties useful in ML & AI.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Determinant and Inverse of a Matrix**  \n",
    "\n",
    "Determinants and inverses play a crucial role in **linear algebra**, especially in **solving linear systems, transformations, and eigenvalue problems**. Let's explore their **definitions, properties, and Python code implementations**.  \n",
    "\n",
    "---\n",
    "\n",
    "## **Determinant of a Matrix**  \n",
    "\n",
    "### **Definition**  \n",
    "The **determinant** of a square matrix $ A $ (denoted as $ \\det(A) $ or $ |A| $) is a **scalar value** that represents the scaling factor of the linear transformation described by $ A $.  \n",
    "\n",
    "### **Determinant of a 2√ó2 Matrix**  \n",
    "For a $ 2 \\times 2 $ matrix:  \n",
    "\n",
    "$$\n",
    "A = \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\det(A) = ad - bc\n",
    "$$\n",
    "\n",
    "### **Determinant of a 3√ó3 Matrix**  \n",
    "For a $ 3 \\times 3 $ matrix:  \n",
    "\n",
    "$$\n",
    "A = \\begin{bmatrix} a & b & c \\\\ d & e & f \\\\ g & h & i \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\det(A) = a(ei - fh) - b(di - fg) + c(dh - eg)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### **Properties of Determinants**  \n",
    "1. **Determinant of Identity Matrix**: $ \\det(I) = 1 $  \n",
    "2. **Determinant of a Singular Matrix**: If $ \\det(A) = 0 $, then $ A $ is **singular** (not invertible).  \n",
    "3. **Multiplicative Property**: $ \\det(AB) = \\det(A) \\cdot \\det(B) $  \n",
    "4. **Effect of Row Operations**:  \n",
    "   - Swapping two rows **negates** the determinant.  \n",
    "   - Multiplying a row by $ k $ **multiplies** the determinant by $ k $.  \n",
    "   - Adding a multiple of one row to another **does not** change the determinant.  \n",
    "5. **Determinant of a Transpose**: $ \\det(A^T) = \\det(A) $  \n",
    "\n",
    "---\n",
    "\n",
    "### **Python Code: Computing Determinant**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define a matrix\n",
    "A = np.array([[4, 3], [6, 3]])\n",
    "\n",
    "# Compute determinant\n",
    "det_A = np.linalg.det(A)\n",
    "\n",
    "print(\"Matrix A:\\n\", A)\n",
    "print(\"Determinant of A:\", det_A)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## **Singular vs. Non-Singular Matrices**  \n",
    "\n",
    "### **Singular Matrix**  \n",
    "A matrix $ A $ is **singular** if:  \n",
    "- $ \\det(A) = 0 $  \n",
    "- It **does not have an inverse**  \n",
    "- The rows or columns are **linearly dependent**  \n",
    "\n",
    "Example of a **singular** matrix:  \n",
    "$$\n",
    "A = \\begin{bmatrix} 2 & 4 \\\\ 1 & 2 \\end{bmatrix}\n",
    "$$\n",
    "$$\n",
    "\\det(A) = (2 \\times 2) - (4 \\times 1) = 4 - 4 = 0\n",
    "$$\n",
    "This matrix is singular because the second row is a multiple of the first row.\n",
    "\n",
    "---\n",
    "\n",
    "### **Non-Singular Matrix**  \n",
    "A matrix $ A $ is **non-singular** if:  \n",
    "- $ \\det(A) \\neq 0 $  \n",
    "- It has an **inverse**  \n",
    "- The rows or columns are **linearly independent**  \n",
    "\n",
    "Example of a **non-singular** matrix:  \n",
    "$$\n",
    "A = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix}\n",
    "$$\n",
    "$$\n",
    "\\det(A) = (1 \\times 4) - (2 \\times 3) = 4 - 6 = -2\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### **Python Code: Checking if a Matrix is Singular**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[2, 4], [1, 2]])\n",
    "\n",
    "# Compute determinant\n",
    "det_A = np.linalg.det(A)\n",
    "\n",
    "# Check if the matrix is singular\n",
    "if np.isclose(det_A, 0):\n",
    "    print(\"Matrix A is singular (not invertible).\")\n",
    "else:\n",
    "    print(\"Matrix A is non-singular (invertible).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## **Inverse of a Matrix**  \n",
    "\n",
    "### **Definition**  \n",
    "The **inverse of a square matrix $ A $**, denoted as $ A^{-1} $, is the matrix that satisfies:  \n",
    "\n",
    "$$\n",
    "A A^{-1} = A^{-1} A = I\n",
    "$$\n",
    "\n",
    "where $ I $ is the **identity matrix**.\n",
    "\n",
    "### **Conditions for Inverse to Exist**  \n",
    "A matrix $ A $ is **invertible** if and only if:  \n",
    "- $ \\det(A) \\neq 0 $ (non-singular)  \n",
    "- It is a **square matrix** (same number of rows and columns)  \n",
    "\n",
    "---\n",
    "\n",
    "### **Formula for the Inverse of a 2√ó2 Matrix**  \n",
    "For  \n",
    "$$\n",
    "A = \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix}\n",
    "$$\n",
    "The inverse is:\n",
    "\n",
    "$$\n",
    "A^{-1} = \\frac{1}{\\det(A)} \\begin{bmatrix} d & -b \\\\ -c & a \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Example:**\n",
    "$$\n",
    "A = \\begin{bmatrix} 4 & 7 \\\\ 2 & 6 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\det(A) = (4 \\times 6) - (7 \\times 2) = 24 - 14 = 10\n",
    "$$\n",
    "\n",
    "$$\n",
    "A^{-1} = \\frac{1}{10} \\begin{bmatrix} 6 & -7 \\\\ -2 & 4 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### **Python Code: Computing Matrix Inverse**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a matrix\n",
    "A = np.array([[4, 7], [2, 6]])\n",
    "\n",
    "# Compute inverse\n",
    "A_inv = np.linalg.inv(A)\n",
    "\n",
    "print(\"Matrix A:\\n\", A)\n",
    "print(\"Inverse of A:\\n\", A_inv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### **Properties of Matrix Inverse**  \n",
    "1. **Inverse of a product**:  \n",
    "   $$\n",
    "   (AB)^{-1} = B^{-1} A^{-1}\n",
    "   $$\n",
    "2. **Inverse of a transpose**:  \n",
    "   $$\n",
    "   (A^T)^{-1} = (A^{-1})^T\n",
    "   $$\n",
    "3. **Inverse of an inverse**:  \n",
    "   $$\n",
    "   (A^{-1})^{-1} = A\n",
    "   $$\n",
    "4. **If $ A $ is orthogonal**:  \n",
    "   $$\n",
    "   A^{-1} = A^T\n",
    "   $$\n",
    "\n",
    "---\n",
    "\n",
    "## **Summary Table**  \n",
    "\n",
    "| Concept | Definition | Condition |\n",
    "|---------|-----------|-----------|\n",
    "| **Determinant** | A scalar that measures the transformation scaling of a matrix | $ \\det(A) \\neq 0 $ for an invertible matrix |\n",
    "| **Singular Matrix** | A matrix with $ \\det(A) = 0 $, meaning it has no inverse | Rows/columns are linearly dependent |\n",
    "| **Non-Singular Matrix** | A matrix with $ \\det(A) \\neq 0 $, meaning it has an inverse | Rows/columns are linearly independent |\n",
    "| **Inverse of a Matrix** | A matrix that satisfies $ A A^{-1} = I $ | Exists only if $ \\det(A) \\neq 0 $ |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Eigenvalues and Eigenvectors**  \n",
    "\n",
    "Eigenvalues and eigenvectors play a crucial role in **machine learning, PCA (Principal Component Analysis), stability analysis, and differential equations**. They help in **understanding linear transformations, dimensionality reduction, and system stability**.  \n",
    "\n",
    "---\n",
    "\n",
    "## **What Are Eigenvalues and Eigenvectors?**  \n",
    "\n",
    "For a **square matrix** $ A $, an **eigenvector** is a **nonzero vector** $ x $ such that multiplying it by $ A $ results in a **scaled version** of itself:  \n",
    "\n",
    "$$\n",
    "A x = \\lambda x\n",
    "$$\n",
    "\n",
    "where:  \n",
    "- $ x $ is the **eigenvector**  \n",
    "- $ \\lambda $ is the **eigenvalue**  \n",
    "\n",
    "### **Example Interpretation**  \n",
    "If $ A $ represents a **transformation (rotation, scaling, etc.)**, then an **eigenvector** is a special direction that **remains unchanged** (except for scaling), and the **eigenvalue** tells how much it is scaled.  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## **How to Compute Eigenvalues and Eigenvectors?**  \n",
    "\n",
    "### **Step 1: Compute Eigenvalues**\n",
    "Eigenvalues are found by solving the **characteristic equation**:  \n",
    "\n",
    "$$\n",
    "\\det(A - \\lambda I) = 0\n",
    "$$\n",
    "\n",
    "where $ I $ is the identity matrix.\n",
    "\n",
    "For a **2√ó2 matrix**:  \n",
    "\n",
    "$$\n",
    "A = \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\det \\begin{bmatrix} a-\\lambda & b \\\\ c & d-\\lambda \\end{bmatrix} = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "(a - \\lambda)(d - \\lambda) - bc = 0\n",
    "$$\n",
    "\n",
    "Solving this quadratic equation gives **two eigenvalues**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 2: Compute Eigenvectors**\n",
    "For each **eigenvalue $ \\lambda $**, solve:  \n",
    "\n",
    "$$\n",
    "(A - \\lambda I) x = 0\n",
    "$$\n",
    "\n",
    "This system of equations gives the eigenvector(s) corresponding to $ \\lambda $.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## **Example Calculation (2√ó2 Matrix)**  \n",
    "\n",
    "### **Matrix:**\n",
    "$$\n",
    "A = \\begin{bmatrix} 4 & 2 \\\\ 1 & 3 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "### **Step 1: Compute Eigenvalues**  \n",
    "$$\n",
    "\\det(A - \\lambda I) = \\det \\begin{bmatrix} 4 - \\lambda & 2 \\\\ 1 & 3 - \\lambda \\end{bmatrix} = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "(4 - \\lambda)(3 - \\lambda) - (2 \\times 1) = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "(4 - \\lambda)(3 - \\lambda) - 2 = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "12 - 4\\lambda - 3\\lambda + \\lambda^2 - 2 = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\lambda^2 - 7\\lambda + 10 = 0\n",
    "$$\n",
    "\n",
    "Solving $ (\\lambda - 5)(\\lambda - 2) = 0 $, we get:\n",
    "\n",
    "$$\n",
    "\\lambda_1 = 5, \\quad \\lambda_2 = 2\n",
    "$$\n",
    "\n",
    "### **Step 2: Compute Eigenvectors**  \n",
    "For **$ \\lambda_1 = 5 $**:  \n",
    "Solve:  \n",
    "$$\n",
    "\\begin{bmatrix} 4-5 & 2 \\\\ 1 & 3-5 \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} -1 & 2 \\\\ 1 & -2 \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} = 0\n",
    "$$\n",
    "\n",
    "From **row 1**: $ -x_1 + 2x_2 = 0 $ ‚Üí $ x_1 = 2x_2 $  \n",
    "\n",
    "Eigenvector:  \n",
    "$$\n",
    "x = \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "For **$ \\lambda_2 = 2 $**:  \n",
    "Solve:  \n",
    "$$\n",
    "\\begin{bmatrix} 4-2 & 2 \\\\ 1 & 3-2 \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} 2 & 2 \\\\ 1 & 1 \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} = 0\n",
    "$$\n",
    "\n",
    "From **row 1**: $ 2x_1 + 2x_2 = 0 $ ‚Üí $ x_1 = -x_2 $  \n",
    "\n",
    "Eigenvector:  \n",
    "$$\n",
    "x = \\begin{bmatrix} -1 \\\\ 1 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## **Python Code to Compute Eigenvalues & Eigenvectors**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix A:\n",
      " [[4 2]\n",
      " [1 3]]\n",
      "Eigenvalues:\n",
      " [5. 2.]\n",
      "Eigenvectors:\n",
      " [[ 0.89442719 -0.70710678]\n",
      " [ 0.4472136   0.70710678]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define matrix A\n",
    "A = np.array([[4, 2], [1, 3]])\n",
    "\n",
    "# Compute eigenvalues and eigenvectors\n",
    "eigenvalues, eigenvectors = np.linalg.eig(A)\n",
    "\n",
    "print(\"Matrix A:\\n\", A)\n",
    "print(\"Eigenvalues:\\n\", eigenvalues)\n",
    "print(\"Eigenvectors:\\n\", eigenvectors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## **Properties of Eigenvalues and Eigenvectors**  \n",
    "\n",
    "1. **Sum of Eigenvalues = Trace of Matrix**  \n",
    "   $$\n",
    "   \\sum \\lambda_i = \\text{trace}(A) = \\sum A_{ii}\n",
    "   $$\n",
    "\n",
    "2. **Product of Eigenvalues = Determinant of Matrix**  \n",
    "   $$\n",
    "   \\prod \\lambda_i = \\det(A)\n",
    "   $$\n",
    "\n",
    "3. **Eigenvalues of a Diagonal Matrix**  \n",
    "   If $ A $ is diagonal:  \n",
    "   $$\n",
    "   A = \\begin{bmatrix} \\lambda_1 & 0 \\\\ 0 & \\lambda_2 \\end{bmatrix}\n",
    "   $$\n",
    "   then its eigenvalues are $ \\lambda_1, \\lambda_2 $.\n",
    "\n",
    "4. **Eigenvectors of a Symmetric Matrix Are Orthogonal**  \n",
    "   If $ A $ is symmetric ($ A^T = A $), then eigenvectors corresponding to different eigenvalues are **orthogonal**.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## **Applications in Data Science**  \n",
    "\n",
    "### **1. Principal Component Analysis (PCA)**\n",
    "- PCA uses **eigenvalues and eigenvectors** of the **covariance matrix** to identify the most important directions (principal components) in data.\n",
    "\n",
    "### **2. Google's PageRank Algorithm**\n",
    "- The importance of web pages is determined using **eigenvectors** of a transition probability matrix.\n",
    "\n",
    "### **3. Stability of Dynamical Systems**\n",
    "- Eigenvalues help determine **stability**:  \n",
    "  - If all eigenvalues have **negative real parts**, the system is **stable**.  \n",
    "  - If any eigenvalue has a **positive real part**, the system is **unstable**.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## **Summary Table**  \n",
    "\n",
    "| Concept | Definition |\n",
    "|---------|-----------|\n",
    "| **Eigenvalue** $ \\lambda $ | Scalar that scales the eigenvector during transformation |\n",
    "| **Eigenvector** $ x $ | Nonzero vector that remains in the same direction after transformation |\n",
    "| **Characteristic Equation** | $ \\det(A - \\lambda I) = 0 $ |\n",
    "| **Trace Property** | $ \\sum \\lambda_i = \\text{trace}(A) $ |\n",
    "| **Determinant Property** | $ \\prod \\lambda_i = \\det(A) $ |\n",
    "| **Eigenvectors of Symmetric Matrices** | Orthogonal |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Spectral Decomposition & Diagonalization**  \n",
    "\n",
    "Spectral decomposition and diagonalization are fundamental concepts in **linear algebra** with applications in **PCA (Principal Component Analysis), machine learning, and physics**.  \n",
    "\n",
    "---\n",
    "\n",
    "## **What is Spectral Decomposition?**  \n",
    "\n",
    "If a matrix $ A $ is **diagonalizable**, it can be written as:\n",
    "\n",
    "$$\n",
    "A = Q \\Lambda Q^{-1}\n",
    "$$\n",
    "\n",
    "where:  \n",
    "- $ Q $ is a **matrix of eigenvectors** (columns are eigenvectors of $ A $).  \n",
    "- $ \\Lambda $ is a **diagonal matrix** containing eigenvalues.  \n",
    "- $ Q^{-1} $ is the **inverse of $ Q $**.  \n",
    "\n",
    "This is known as the **spectral decomposition (eigendecomposition)**.\n",
    "\n",
    "‚úÖ **Key Condition**: A matrix is diagonalizable if it has **linearly independent eigenvectors**.  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## **Why is Spectral Decomposition Useful?**  \n",
    "\n",
    "1. **Efficient Computations**:  \n",
    "   - Powers of $ A $ are easier:  \n",
    "     $$\n",
    "     A^k = Q \\Lambda^k Q^{-1}\n",
    "     $$\n",
    "   - Helps in computing **exponentials, logarithms, and square roots** of matrices.\n",
    "\n",
    "2. **Principal Component Analysis (PCA)**:  \n",
    "   - PCA finds eigenvectors (principal components) of the **covariance matrix**.\n",
    "\n",
    "3. **Solving Differential Equations**:  \n",
    "   - Used in **stability analysis** and **system dynamics**.\n",
    "\n",
    "---\n",
    "\n",
    "## **Example of Spectral Decomposition**  \n",
    "\n",
    "### **Matrix:**  \n",
    "$$\n",
    "A = \\begin{bmatrix} 4 & 2 \\\\ 1 & 3 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "### **Step 1: Compute Eigenvalues**  \n",
    "Solve $ \\det(A - \\lambda I) = 0 $:\n",
    "\n",
    "$$\n",
    "\\det \\begin{bmatrix} 4-\\lambda & 2 \\\\ 1 & 3-\\lambda \\end{bmatrix} = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "(4-\\lambda)(3-\\lambda) - (2 \\times 1) = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\lambda^2 - 7\\lambda + 10 = 0\n",
    "$$\n",
    "\n",
    "Solving $ (\\lambda - 5)(\\lambda - 2) = 0 $, we get:\n",
    "\n",
    "$$\n",
    "\\lambda_1 = 5, \\quad \\lambda_2 = 2\n",
    "$$\n",
    "\n",
    "### **Step 2: Compute Eigenvectors**  \n",
    "\n",
    "For $ \\lambda_1 = 5 $:  \n",
    "$$\n",
    "(A - 5I)x = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} -1 & 2 \\\\ 1 & -2 \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "x_1 = 2x_2\n",
    "$$\n",
    "\n",
    "Eigenvector:  \n",
    "$$\n",
    "\\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "For $ \\lambda_2 = 2 $:  \n",
    "$$\n",
    "(A - 2I)x = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} 2 & 2 \\\\ 1 & 1 \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "x_1 = -x_2\n",
    "$$\n",
    "\n",
    "Eigenvector:  \n",
    "$$\n",
    "\\begin{bmatrix} -1 \\\\ 1 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "### **Step 3: Form Spectral Decomposition**  \n",
    "$$\n",
    "Q = \\begin{bmatrix} 2 & -1 \\\\ 1 & 1 \\end{bmatrix}, \\quad\n",
    "\\Lambda = \\begin{bmatrix} 5 & 0 \\\\ 0 & 2 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "A = Q \\Lambda Q^{-1}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## **Python Code for Spectral Decomposition**  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Matrix:\n",
      " [[4 2]\n",
      " [1 3]]\n",
      "Eigenvalues:\n",
      " [5. 2.]\n",
      "Eigenvectors:\n",
      " [[ 0.89442719 -0.70710678]\n",
      " [ 0.4472136   0.70710678]]\n",
      "Reconstructed A:\n",
      " [[4. 2.]\n",
      " [1. 3.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define matrix A\n",
    "A = np.array([[4, 2], [1, 3]])\n",
    "\n",
    "# Compute eigenvalues and eigenvectors\n",
    "eigenvalues, eigenvectors = np.linalg.eig(A)\n",
    "\n",
    "# Form diagonal matrix\n",
    "Lambda = np.diag(eigenvalues)\n",
    "\n",
    "# Compute inverse of eigenvectors matrix\n",
    "Q_inv = np.linalg.inv(eigenvectors)\n",
    "\n",
    "# Verify decomposition\n",
    "A_reconstructed = eigenvectors @ Lambda @ Q_inv\n",
    "\n",
    "print(\"Original Matrix:\\n\", A)\n",
    "print(\"Eigenvalues:\\n\", eigenvalues)\n",
    "print(\"Eigenvectors:\\n\", eigenvectors)\n",
    "print(\"Reconstructed A:\\n\", A_reconstructed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "‚úÖ **If A is reconstructed correctly, spectral decomposition is verified!**\n",
    "\n",
    "---\n",
    "\n",
    "# **Diagonalization**  \n",
    "\n",
    "A square matrix $ A $ is **diagonalizable** if there exists an invertible matrix $ P $ such that:\n",
    "\n",
    "$$\n",
    "A = P D P^{-1}\n",
    "$$\n",
    "\n",
    "where:  \n",
    "- $ D $ is a **diagonal matrix** of eigenvalues.  \n",
    "- $ P $ is a **matrix of eigenvectors**.  \n",
    "\n",
    "‚úÖ **A matrix is diagonalizable if and only if it has $ n $ linearly independent eigenvectors** (for an $ n \\times n $ matrix).\n",
    "\n",
    "---\n",
    "\n",
    "## **Example: Diagonalization of a Matrix**  \n",
    "\n",
    "Let  \n",
    "$$\n",
    "A = \\begin{bmatrix} 3 & 1 \\\\ 0 & 2 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "1Ô∏è‚É£ Compute eigenvalues by solving $ \\det(A - \\lambda I) = 0 $.  \n",
    "2Ô∏è‚É£ Compute eigenvectors.  \n",
    "3Ô∏è‚É£ Construct $ P $ (matrix of eigenvectors) and $ D $ (diagonal matrix of eigenvalues).  \n",
    "4Ô∏è‚É£ Verify $ A = P D P^{-1} $.  \n",
    "\n",
    "‚úÖ If possible, the matrix is **diagonalizable**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Matrix Factorization in Linear Algebra**  \n",
    "\n",
    "Matrix factorization is a technique for **decomposing a matrix into simpler matrices** to solve linear equations, reduce dimensionality, and perform efficient computations. It is widely used in **machine learning (PCA, recommender systems), numerical analysis, and signal processing**.  \n",
    "\n",
    "---\n",
    "\n",
    "## **Types of Matrix Factorization**  \n",
    "\n",
    "1. **LU Decomposition** (Lower-Upper Factorization)  \n",
    "   - Used to solve linear systems efficiently.  \n",
    "   - Factorizes a matrix into:  \n",
    "     $$\n",
    "     A = LU\n",
    "     $$\n",
    "     where:  \n",
    "     - $ L $ is a **lower triangular matrix**  \n",
    "     - $ U $ is an **upper triangular matrix**  \n",
    "\n",
    "2. **QR Decomposition**  \n",
    "   - Used for **orthogonalization** and solving least squares problems.  \n",
    "   - Factorizes a matrix into:  \n",
    "     $$\n",
    "     A = QR\n",
    "     $$\n",
    "     where:  \n",
    "     - $ Q $ is an **orthogonal matrix**  \n",
    "     - $ R $ is an **upper triangular matrix**  \n",
    "\n",
    "3. **Singular Value Decomposition (SVD)**  \n",
    "   - Used in **PCA, image compression, and latent semantic analysis**.  \n",
    "   - Factorizes a matrix into:  \n",
    "     $$\n",
    "     A = U \\Sigma V^T\n",
    "     $$\n",
    "     where:  \n",
    "     - $ U $ and $ V $ are **orthogonal matrices**  \n",
    "     - $ \\Sigma $ is a **diagonal matrix of singular values**  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## **LU Decomposition (Lower-Upper Factorization)**  \n",
    "\n",
    "LU decomposition expresses a **square matrix** as:  \n",
    "$$\n",
    "A = LU\n",
    "$$\n",
    "where:  \n",
    "- $ L $ is a **lower triangular matrix** (entries above diagonal are 0).  \n",
    "- $ U $ is an **upper triangular matrix** (entries below diagonal are 0).  \n",
    "\n",
    "### **Example**  \n",
    "Let:  \n",
    "$$\n",
    "A = \\begin{bmatrix} 2 & 3 \\\\ 5 & 7 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "LU factorization gives:  \n",
    "$$\n",
    "L = \\begin{bmatrix} 1 & 0 \\\\ 2.5 & 1 \\end{bmatrix}, \\quad U = \\begin{bmatrix} 2 & 3 \\\\ 0 & -0.5 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "‚úÖ **Python Code for LU Decomposition**  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lower Triangular Matrix L:\n",
      " [[1.  0. ]\n",
      " [0.4 1. ]]\n",
      "Upper Triangular Matrix U:\n",
      " [[5.  7. ]\n",
      " [0.  0.2]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "from scipy.linalg import lu\n",
    "\n",
    "# Define matrix A\n",
    "A = np.array([[2, 3], [5, 7]])\n",
    "\n",
    "# Perform LU decomposition\n",
    "P, L, U = lu(A)\n",
    "\n",
    "print(\"Lower Triangular Matrix L:\\n\", L)\n",
    "print(\"Upper Triangular Matrix U:\\n\", U)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "## **QR Decomposition (Orthogonal-Triangular Factorization)**  \n",
    "\n",
    "QR decomposition expresses a matrix as:  \n",
    "$$\n",
    "A = QR\n",
    "$$\n",
    "where:  \n",
    "- $ Q $ is an **orthogonal matrix** (columns are orthonormal vectors).  \n",
    "- $ R $ is an **upper triangular matrix**.  \n",
    "\n",
    "### **Example**  \n",
    "Let:  \n",
    "$$\n",
    "A = \\begin{bmatrix} 1 & -1 \\\\ 1 & 1 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "QR factorization gives:  \n",
    "$$\n",
    "Q = \\begin{bmatrix} 0.707 & -0.707 \\\\ 0.707 & 0.707 \\end{bmatrix}, \\quad\n",
    "R = \\begin{bmatrix} 1.414 & 0 \\\\ 0 & 1.414 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "‚úÖ **Python Code for QR Decomposition**  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orthogonal Matrix Q:\n",
      " [[-0.70710678 -0.70710678]\n",
      " [-0.70710678  0.70710678]]\n",
      "Upper Triangular Matrix R:\n",
      " [[-1.41421356e+00 -3.31822250e-16]\n",
      " [ 0.00000000e+00  1.41421356e+00]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "from scipy.linalg import qr\n",
    "\n",
    "# Define matrix A\n",
    "A = np.array([[1, -1], [1, 1]])\n",
    "\n",
    "# Perform QR decomposition\n",
    "Q, R = qr(A)\n",
    "\n",
    "print(\"Orthogonal Matrix Q:\\n\", Q)\n",
    "print(\"Upper Triangular Matrix R:\\n\", R)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## **Singular Value Decomposition (SVD)**  \n",
    "\n",
    "SVD factorizes any **m √ó n** matrix $ A $ as:  \n",
    "$$\n",
    "A = U \\Sigma V^T\n",
    "$$\n",
    "where:  \n",
    "- $ U $ (m √ó m) is an **orthogonal matrix** (left singular vectors).  \n",
    "- $ \\Sigma $ (m √ó n) is a **diagonal matrix of singular values**.  \n",
    "- $ V^T $ (n √ó n) is an **orthogonal matrix** (right singular vectors).  \n",
    "\n",
    "### **Applications of SVD**\n",
    "‚úÖ **PCA (Principal Component Analysis)**  \n",
    "‚úÖ **Dimensionality Reduction**  \n",
    "‚úÖ **Image Compression**  \n",
    "\n",
    "### **Example**  \n",
    "Let:  \n",
    "$$\n",
    "A = \\begin{bmatrix} 4 & 0 \\\\ 3 & -5 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "SVD gives:  \n",
    "$$\n",
    "U = \\begin{bmatrix} -0.8 & -0.6 \\\\ -0.6 & 0.8 \\end{bmatrix}, \\quad\n",
    "\\Sigma = \\begin{bmatrix} 6.4 & 0 \\\\ 0 & 3.2 \\end{bmatrix}, \\quad\n",
    "V = \\begin{bmatrix} -0.8 & 0.6 \\\\ 0.6 & 0.8 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "‚úÖ **Python Code for SVD**  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Left Singular Vectors (U):\n",
      " [[-0.4472136  -0.89442719]\n",
      " [-0.89442719  0.4472136 ]]\n",
      "Singular Values (Sigma):\n",
      " [[6.32455532 0.        ]\n",
      " [0.         3.16227766]]\n",
      "Right Singular Vectors (V^T):\n",
      " [[-0.70710678  0.70710678]\n",
      " [-0.70710678 -0.70710678]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "# Define matrix A\n",
    "A = np.array([[4, 0], [3, -5]])\n",
    "\n",
    "# Perform Singular Value Decomposition\n",
    "U, Sigma, Vt = np.linalg.svd(A)\n",
    "\n",
    "print(\"Left Singular Vectors (U):\\n\", U)\n",
    "print(\"Singular Values (Sigma):\\n\", np.diag(Sigma))\n",
    "print(\"Right Singular Vectors (V^T):\\n\", Vt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "## **Comparison of LU, QR, and SVD**  \n",
    "\n",
    "| Factorization | Form | Application |\n",
    "|--------------|----------------|----------------------|\n",
    "| **LU** | $ A = LU $ | Solving linear systems |\n",
    "| **QR** | $ A = QR $ | Least squares regression |\n",
    "| **SVD** | $ A = U\\Sigma V^T $ | PCA, Dimensionality Reduction |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Applications of Matrix Factorization in Data Science & Machine Learning**  \n",
    "\n",
    "Matrix factorization techniques are **widely used in data science and machine learning** for tasks like **dimensionality reduction, recommender systems, and feature extraction**. Let's explore key applications with examples and Python implementations.  \n",
    "\n",
    "---\n",
    "\n",
    "## **Principal Component Analysis (PCA) using SVD**  \n",
    "\n",
    "### **What is PCA?**  \n",
    "PCA is a technique for **dimensionality reduction** that:  \n",
    "‚úÖ Finds the **principal components** (new axes of maximum variance).  \n",
    "‚úÖ Uses **Singular Value Decomposition (SVD)** or **Eigen Decomposition**.  \n",
    "‚úÖ Reduces **high-dimensional data** to lower dimensions **while preserving variance**.  \n",
    "\n",
    "### **How PCA Uses SVD?**  \n",
    "PCA applies **SVD** on the covariance matrix of a dataset:  \n",
    "$$\n",
    "X = U \\Sigma V^T\n",
    "$$\n",
    "- **Eigenvectors (Columns of $V$)** ‚Üí Principal Components  \n",
    "- **Eigenvalues (Diagonal of $ \\Sigma $)** ‚Üí Variance along principal components  \n",
    "\n",
    "### **Example: PCA on a Dataset**\n",
    "‚úÖ **Python Code for PCA using SVD**  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load dataset\n",
    "iris = load_iris()\n",
    "X = iris.data  # 4D data\n",
    "\n",
    "# Apply PCA to reduce to 2D\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# Scatter plot of reduced data\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=iris.target, cmap='viridis', edgecolor='k')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.title('PCA on Iris Dataset')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "‚úÖ **Use Cases of PCA:**  \n",
    "- **Feature selection & dimensionality reduction** (e.g., gene expression analysis).  \n",
    "- **Noise removal** in datasets.  \n",
    "- **Visualization** of high-dimensional data in **2D or 3D**.  \n",
    "\n",
    "---\n",
    "\n",
    "## **Recommender Systems (Matrix Factorization for Collaborative Filtering)**  \n",
    "\n",
    "### **What is Collaborative Filtering?**  \n",
    "In recommendation systems (e.g., **Netflix, Amazon**), we use a **user-item interaction matrix** where:  \n",
    "- **Rows = Users**  \n",
    "- **Columns = Items (Movies, Products, etc.)**  \n",
    "- **Entries = Ratings or Interactions**  \n",
    "\n",
    "### **Matrix Factorization for Recommendations**  \n",
    "$$\n",
    "R \\approx P Q^T\n",
    "$$\n",
    "where:  \n",
    "- $ R $ = **User-Item Rating Matrix**  \n",
    "- $ P $ = **User Features Matrix**  \n",
    "- $ Q $ = **Item Features Matrix**  \n",
    "\n",
    "### **Example: Netflix Movie Recommendations**\n",
    "‚úÖ **Python Code for SVD in Recommender Systems**  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from scipy.sparse.linalg import svds\n",
    "\n",
    "# Sample user-item rating matrix (5 users, 4 movies)\n",
    "R = np.array([[5, 4, 0, 0], \n",
    "              [3, 0, 0, 5], \n",
    "              [0, 0, 5, 4], \n",
    "              [0, 3, 4, 0], \n",
    "              [5, 0, 3, 2]])\n",
    "\n",
    "# Apply SVD\n",
    "U, sigma, Vt = svds(R, k=2)  # Reduce to 2 features\n",
    "\n",
    "# Convert sigma into a diagonal matrix\n",
    "sigma = np.diag(sigma)\n",
    "\n",
    "# Reconstruct approximate rating matrix\n",
    "R_pred = np.dot(np.dot(U, sigma), Vt)\n",
    "\n",
    "print(\"Predicted Ratings:\\n\", R_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "‚úÖ **Use Cases of Matrix Factorization in Recommender Systems:**  \n",
    "- **Netflix & YouTube** (Movie recommendations).  \n",
    "- **Amazon & Flipkart** (Product recommendations).  \n",
    "- **Spotify & Apple Music** (Song recommendations).  \n",
    "\n",
    "---\n",
    "\n",
    "## **Latent Semantic Analysis (LSA) for Text Analysis**  \n",
    "\n",
    "### **What is LSA?**  \n",
    "Latent Semantic Analysis (LSA) is used in **Natural Language Processing (NLP)** to **extract topics from text**.  \n",
    "- It **reduces high-dimensional text data** into a **lower-dimensional semantic space**.  \n",
    "- It **uses SVD on the term-document matrix**.  \n",
    "\n",
    "### **How LSA Works?**  \n",
    "$$\n",
    "A = U \\Sigma V^T\n",
    "$$\n",
    "- $ A $ ‚Üí **Term-Document Matrix** (TF-IDF)  \n",
    "- $ U $ ‚Üí **Word-Topic Associations**  \n",
    "- $ \\Sigma $ ‚Üí **Strength of Topics**  \n",
    "- $ V^T $ ‚Üí **Document-Topic Associations**  \n",
    "\n",
    "### **Example: Topic Extraction Using LSA**\n",
    "‚úÖ **Python Code for LSA using SVD**  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Sample corpus\n",
    "documents = [\"AI is transforming the world\", \n",
    "             \"Deep learning powers AI\",\n",
    "             \"Mathematics is the foundation of AI\",\n",
    "             \"Science and research drive innovation\"]\n",
    "\n",
    "# Convert text to TF-IDF matrix\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Apply LSA (SVD)\n",
    "lsa = TruncatedSVD(n_components=2)  # Extract 2 topics\n",
    "X_lsa = lsa.fit_transform(X)\n",
    "\n",
    "print(\"Document-Topic Matrix:\\n\", X_lsa)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "‚úÖ **Use Cases of LSA:**  \n",
    "- **Topic modeling in NLP** (e.g., **news categorization, sentiment analysis**).  \n",
    "- **Document clustering** (e.g., **Google News, Wikipedia**).  \n",
    "- **Search engines** (e.g., **query expansion & synonym recognition**).  \n",
    "\n",
    "---\n",
    "\n",
    "## **Solving Linear Regression Using QR Decomposition**  \n",
    "\n",
    "### **Why QR for Linear Regression?**  \n",
    "- Linear regression solves **$ Xw = y $**.  \n",
    "- Instead of using the inverse, **QR decomposition** provides a **numerically stable** solution.  \n",
    "- QR decomposition factorizes $ X $ into **$ QR $**:  \n",
    "  $$\n",
    "  X = QR, \\quad w = R^{-1} Q^T y\n",
    "  $$\n",
    "\n",
    "‚úÖ **Python Code for QR Decomposition in Linear Regression**  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from numpy.linalg import qr, inv\n",
    "\n",
    "# Sample dataset (2 features, 5 samples)\n",
    "X = np.array([[1, 1], [1, 2], [1, 3], [1, 4], [1, 5]])\n",
    "y = np.array([2, 2.5, 3, 3.5, 4])  # Target values\n",
    "\n",
    "# Perform QR decomposition\n",
    "Q, R = qr(X)\n",
    "\n",
    "# Solve for w (regression coefficients)\n",
    "w = inv(R).dot(Q.T).dot(y)\n",
    "print(\"Regression Coefficients:\", w)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "‚úÖ **Use Cases of QR Factorization:**  \n",
    "- **Linear regression** (avoids inverting matrices).  \n",
    "- **Solving least squares problems efficiently**.  \n",
    "- **Eigenvalue computations in ML algorithms**.  \n",
    "\n",
    "---\n",
    "\n",
    "## **Image Compression Using SVD**  \n",
    "\n",
    "### **How SVD Helps in Image Compression?**  \n",
    "- Images are stored as **matrices of pixel values**.  \n",
    "- **SVD reduces storage requirements** by keeping only **top singular values**.  \n",
    "- We keep only **k singular values**, reducing **memory usage while preserving quality**.  \n",
    "\n",
    "‚úÖ **Python Code for Image Compression Using SVD**  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.io import imread\n",
    "\n",
    "# Load and convert image to grayscale\n",
    "img = rgb2gray(imread(\"example.jpg\"))\n",
    "\n",
    "# Apply SVD\n",
    "U, S, Vt = np.linalg.svd(img, full_matrices=False)\n",
    "\n",
    "# Keep only top k singular values\n",
    "k = 50  # Compression level\n",
    "compressed_img = np.dot(U[:, :k], np.dot(np.diag(S[:k]), Vt[:k, :]))\n",
    "\n",
    "# Display compressed image\n",
    "plt.imshow(compressed_img, cmap=\"gray\")\n",
    "plt.title(\"Compressed Image\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "‚úÖ **Use Cases of SVD in Image Processing:**  \n",
    "- **JPEG compression**.  \n",
    "- **Face recognition systems**.  \n",
    "- **Pattern recognition & filtering**.  \n",
    "\n",
    "---\n",
    "\n",
    "# **üîπ Summary of Applications**  \n",
    "\n",
    "| Technique | Application |\n",
    "|-----------|----------------|\n",
    "| **SVD (PCA)** | Dimensionality reduction |\n",
    "| **SVD (Recommender Systems)** | Movie & product recommendations |\n",
    "| **SVD (LSA)** | Topic modeling in NLP |\n",
    "| **QR Factorization** | Linear regression solutions |\n",
    "| **SVD (Image Compression)** | JPEG & Face recognition |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Role of Linear Algebra in Neural Networks**  \n",
    "\n",
    "Neural networks heavily rely on **linear algebra concepts** for computations like **weight updates, activations, and optimizations**. Let's explore how matrix operations, eigenvalues, SVD, and other matrix factorization techniques are used in **deep learning**.\n",
    "\n",
    "---\n",
    "\n",
    "## **Matrices in Neural Networks (Weights & Activations)**  \n",
    "\n",
    "Neural networks consist of **layers of neurons** where each layer is represented as a **matrix**:  \n",
    "- **Weights (W):** Connect neurons from one layer to the next.  \n",
    "- **Bias (b):** Shifts activations before applying an activation function.  \n",
    "- **Activation (A):** Result after applying the activation function.\n",
    "\n",
    "### **Example: Forward Propagation as Matrix Multiplication**  \n",
    "For a **single-layer neural network**:\n",
    "$$\n",
    "Z = W X + b\n",
    "$$\n",
    "$$\n",
    "A = f(Z)\n",
    "$$\n",
    "where:  \n",
    "- $ X $ = Input matrix (features)  \n",
    "- $ W $ = Weights matrix  \n",
    "- $ b $ = Bias vector  \n",
    "- $ Z $ = Pre-activation values  \n",
    "- $ A $ = Activations after applying activation function $ f $  \n",
    "\n",
    "‚úÖ **Python Code for Forward Propagation**  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "# Input features (2 features, 3 samples)\n",
    "X = np.array([[0.5, 1.2, 0.8], [0.3, 0.7, 0.2]])\n",
    "\n",
    "# Weights (2 neurons, 2 input features)\n",
    "W = np.array([[0.4, 0.2], [0.1, 0.7]])\n",
    "\n",
    "# Bias (2 neurons)\n",
    "b = np.array([[0.1], [0.2]])\n",
    "\n",
    "# Compute Z = WX + b\n",
    "Z = np.dot(W, X) + b\n",
    "\n",
    "# Activation (ReLU)\n",
    "A = np.maximum(0, Z)\n",
    "\n",
    "print(\"Pre-Activation (Z):\\n\", Z)\n",
    "print(\"Activation (A):\\n\", A)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "‚úÖ **Key Takeaways:**  \n",
    "- Forward propagation is just **matrix multiplication**.  \n",
    "- Activation functions (ReLU, Sigmoid) are applied **element-wise**.  \n",
    "\n",
    "---\n",
    "\n",
    "## **Eigenvalues & Neural Network Stability**  \n",
    "\n",
    "Eigenvalues help analyze the **stability of weight matrices** in deep networks.  \n",
    "\n",
    "### **Key Insights:**\n",
    "- If the largest **eigenvalue of W** is **too large**, activations **explode** (vanishing gradient problem).  \n",
    "- If the largest **eigenvalue is too small**, activations **vanish**.  \n",
    "- Proper initialization of weights ensures that eigenvalues **stay close to 1**.\n",
    "\n",
    "‚úÖ **Eigenvalues of a Weight Matrix in Python**  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from numpy.linalg import eig\n",
    "\n",
    "# Define a weight matrix\n",
    "W = np.array([[0.5, 0.2], [0.1, 0.7]])\n",
    "\n",
    "# Compute eigenvalues\n",
    "eigenvalues, _ = eig(W)\n",
    "\n",
    "print(\"Eigenvalues of W:\", eigenvalues)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Interpretation:**  \n",
    "- If **eigenvalues ‚âà 1**, activations are **stable**.  \n",
    "- If **eigenvalues >> 1**, activations **explode**.  \n",
    "- If **eigenvalues << 1**, activations **vanish**.  \n",
    "\n",
    "‚úÖ **Application:**  \n",
    "- Eigenvalue analysis is used in **Batch Normalization** and **Weight Initialization Techniques** (e.g., Xavier Initialization).  \n",
    "\n",
    "---\n",
    "\n",
    "## **Singular Value Decomposition (SVD) in Neural Networks**  \n",
    "\n",
    "SVD helps in **compressing large weight matrices** in deep learning models.  \n",
    "\n",
    "### **Why Use SVD?**\n",
    "- Large neural networks have **millions of parameters**.\n",
    "- Many weight matrices **have redundant information**.\n",
    "- **SVD decomposes** weight matrices into **smaller matrices**, reducing storage and computation.  \n",
    "\n",
    "‚úÖ **Example: Compressing a Neural Network Using SVD**  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from numpy.linalg import svd\n",
    "\n",
    "# Large weight matrix (4x4)\n",
    "W = np.array([[1, 2, 3, 4], \n",
    "              [4, 3, 2, 1], \n",
    "              [6, 5, 4, 3], \n",
    "              [8, 7, 6, 5]])\n",
    "\n",
    "# Apply SVD\n",
    "U, S, Vt = svd(W)\n",
    "\n",
    "# Keep only top 2 singular values (compression)\n",
    "S_reduced = np.diag(S[:2])\n",
    "U_reduced = U[:, :2]\n",
    "Vt_reduced = Vt[:2, :]\n",
    "\n",
    "# Reconstructed approximation of W\n",
    "W_compressed = U_reduced @ S_reduced @ Vt_reduced\n",
    "\n",
    "print(\"Compressed Weight Matrix:\\n\", W_compressed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "‚úÖ **Use Cases of SVD in Neural Networks:**  \n",
    "- **Compressing CNNs** for mobile devices (e.g., MobileNet).  \n",
    "- **Reducing overfitting** by eliminating redundant weights.  \n",
    "- **Speeding up inference** by using smaller weight matrices.  \n",
    "\n",
    "---\n",
    "\n",
    "## **QR Decomposition for Gradient Descent Optimization**  \n",
    "\n",
    "### **Why QR for Gradient Descent?**  \n",
    "- During **backpropagation**, gradients are updated using:\n",
    "  $$\n",
    "  W = W - \\eta \\cdot \\nabla W\n",
    "  $$\n",
    "- Computing **$ \\nabla W $** involves inverting matrices, which is **computationally expensive**.  \n",
    "- **QR decomposition provides a numerically stable way** to compute updates.  \n",
    "\n",
    "‚úÖ **Example: Using QR Decomposition in a Neural Network**  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Update weights using only R (more stable)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m learning_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.01\u001b[39m\n\u001b[0;32m---> 11\u001b[0m W_new \u001b[38;5;241m=\u001b[39m \u001b[43mW\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mR\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpdated Weights:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, W_new)\n",
      "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/torch/_tensor.py:1194\u001b[0m, in \u001b[0;36mTensor.__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m   1192\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[38;5;241m.\u001b[39m__array__, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m   1193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1196\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead."
     ]
    }
   ],
   "source": [
    "\n",
    "from numpy.linalg import qr\n",
    "\n",
    "# Random gradient matrix (4x4)\n",
    "grad = np.random.randn(4, 4)\n",
    "\n",
    "# QR decomposition\n",
    "Q, R = qr(grad)\n",
    "\n",
    "# Update weights using only R (more stable)\n",
    "learning_rate = 0.01\n",
    "W_new = W - learning_rate * R\n",
    "\n",
    "print(\"Updated Weights:\\n\", W_new)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "‚úÖ **Benefits of QR Decomposition:**  \n",
    "- More **numerically stable** than direct inversion.  \n",
    "- Used in **Natural Gradient Descent** for fast convergence.  \n",
    "\n",
    "---\n",
    "\n",
    "## **Hessian Matrix & Second-Order Optimization in Deep Learning**  \n",
    "\n",
    "### **What is the Hessian Matrix?**  \n",
    "The **Hessian matrix** is a second-order derivative of the loss function:\n",
    "$$\n",
    "H = \\frac{\\partial^2 L}{\\partial W^2}\n",
    "$$\n",
    "- It helps in **Newton's Method** for optimizing neural networks.  \n",
    "- **Eigenvalues of Hessian** tell us if the optimization is **stable or not**.  \n",
    "\n",
    "‚úÖ **Hessian Matrix in Python**  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hessian Matrix:\n",
      " tensor([[[[2., 0.],\n",
      "          [0., 0.]],\n",
      "\n",
      "         [[0., 2.],\n",
      "          [0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0.],\n",
      "          [2., 0.]],\n",
      "\n",
      "         [[0., 0.],\n",
      "          [0., 2.]]]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "\n",
    "# Define a loss function\n",
    "W = torch.tensor([[1.0, 2.0], [3.0, 4.0]], requires_grad=True)\n",
    "loss = torch.sum(W**2)\n",
    "\n",
    "# Compute Hessian\n",
    "H = torch.autograd.functional.hessian(lambda x: torch.sum(x**2), W)\n",
    "\n",
    "print(\"Hessian Matrix:\\n\", H)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "‚úÖ **Use Cases of Hessian in Deep Learning:**  \n",
    "- **Second-order optimization (Newton's Method).**  \n",
    "- **Detecting saddle points in deep networks.**  \n",
    "- **Curvature-based regularization.**  \n",
    "\n",
    "---\n",
    "\n",
    "# **üîπ Summary of Matrix Applications in Neural Networks**  \n",
    "\n",
    "| Concept | Application in Neural Networks |\n",
    "|-----------|------------------------------|\n",
    "| **Matrix Multiplication** | Forward & Backward Propagation |\n",
    "| **Eigenvalues** | Stability of weight matrices |\n",
    "| **SVD** | Compressing neural networks |\n",
    "| **QR Decomposition** | Faster gradient updates |\n",
    "| **Hessian Matrix** | Second-order optimization |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # **Advanced Topics in Linear Algebra for Data Science & Machine Learning**  \n",
    "\n",
    "Let's explore some **special topics** in linear algebra that are useful in **optimization, numerical stability, and deep learning**.  \n",
    "\n",
    "---\n",
    "\n",
    "## **Cramer's Rule**  \n",
    "\n",
    "### **What is Cramer's Rule?**  \n",
    "Cramer's Rule is a method to solve a system of **linear equations** using **determinants**.  \n",
    "\n",
    "For a system of equations:  \n",
    "$$\n",
    "AX = B\n",
    "$$\n",
    "where $ A $ is an $ n \\times n $ matrix and $ B $ is a column vector, the solution for $ x_i $ is:  \n",
    "\n",
    "$$\n",
    "x_i = \\frac{\\det(A_i)}{\\det(A)}\n",
    "$$\n",
    "where $ A_i $ is obtained by **replacing the $ i $-th column of $ A $ with $ B $**.\n",
    "\n",
    "### **Example: Solving a System using Cramer's Rule**\n",
    "Solve:  \n",
    "$$\n",
    "2x + 3y = 5\n",
    "$$\n",
    "$$\n",
    "4x + y = 6\n",
    "$$\n",
    "\n",
    "‚úÖ **Python Implementation**  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution: x = 1.2999999999999998, y = 0.7999999999999997\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "# Coefficient matrix A\n",
    "A = np.array([[2, 3], [4, 1]])\n",
    "\n",
    "# Right-hand side vector B\n",
    "B = np.array([5, 6])\n",
    "\n",
    "# Compute determinants\n",
    "det_A = np.linalg.det(A)\n",
    "\n",
    "if det_A != 0:\n",
    "    A1 = A.copy()\n",
    "    A1[:, 0] = B  # Replace first column with B\n",
    "    det_A1 = np.linalg.det(A1)\n",
    "    \n",
    "    A2 = A.copy()\n",
    "    A2[:, 1] = B  # Replace second column with B\n",
    "    det_A2 = np.linalg.det(A2)\n",
    "\n",
    "    x1 = det_A1 / det_A\n",
    "    x2 = det_A2 / det_A\n",
    "\n",
    "    print(f\"Solution: x = {x1}, y = {x2}\")\n",
    "else:\n",
    "    print(\"Matrix A is singular, no unique solution.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "‚úÖ **Applications:**  \n",
    "- Solving small **linear systems** (not used for large matrices due to computational cost).  \n",
    "- Used in **inverse computation** for small matrices.  \n",
    "\n",
    "---\n",
    "\n",
    "## **Rank-Nullity Theorem**  \n",
    "\n",
    "### **What is Rank-Nullity?**  \n",
    "For a matrix $ A $, the **rank-nullity theorem** states:  \n",
    "$$\n",
    "\\text{rank}(A) + \\text{nullity}(A) = \\text{number of columns of } A\n",
    "$$\n",
    "\n",
    "- **Rank(A)**: Number of **linearly independent** columns.  \n",
    "- **Nullity(A)**: Number of **free variables** (dim of null space).  \n",
    "\n",
    "### **Example: Compute Rank & Nullity**  \n",
    "\n",
    "‚úÖ **Python Code**  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank(A) = 2, Nullity(A) = 1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "# Define a matrix\n",
    "A = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "\n",
    "# Compute rank\n",
    "rank_A = np.linalg.matrix_rank(A)\n",
    "\n",
    "# Compute nullity (columns - rank)\n",
    "nullity_A = A.shape[1] - rank_A\n",
    "\n",
    "print(f\"Rank(A) = {rank_A}, Nullity(A) = {nullity_A}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "‚úÖ **Applications:**  \n",
    "- Used in **dimensionality reduction** (PCA, SVD).  \n",
    "- Helps in checking if a **system of equations has solutions**.  \n",
    "- Nullity is used in **kernel methods** for feature engineering.  \n",
    "\n",
    "---\n",
    "\n",
    "## **Moore-Penrose Pseudoinverse**  \n",
    "\n",
    "### **What is the Moore-Penrose Inverse?**  \n",
    "For a **non-square** or **singular matrix** $ A $, the inverse doesn‚Äôt exist. Instead, we use the **pseudoinverse**:  \n",
    "\n",
    "$$\n",
    "A^+ = (A^T A)^{-1} A^T\n",
    "$$\n",
    "\n",
    "It is useful for **solving least squares problems** when the system is overdetermined.\n",
    "\n",
    "### **Example: Compute Pseudoinverse**  \n",
    "\n",
    "‚úÖ **Python Code**  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pseudoinverse of A:\n",
      " [[-1.33333333 -0.33333333  0.66666667]\n",
      " [ 1.08333333  0.33333333 -0.41666667]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "# Define a non-square matrix\n",
    "A = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "\n",
    "# Compute Moore-Penrose Pseudoinverse\n",
    "A_pseudo = np.linalg.pinv(A)\n",
    "\n",
    "print(\"Pseudoinverse of A:\\n\", A_pseudo)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "‚úÖ **Applications:**  \n",
    "- **Solving linear regression**:  \n",
    "  $$\n",
    "  W = (X^T X)^{-1} X^T Y\n",
    "  $$\n",
    "- **Neural network weight updates** (used in backpropagation).  \n",
    "- **Dimensionality reduction techniques**.  \n",
    "\n",
    "---\n",
    "\n",
    "## **Frobenius Norm**  \n",
    "\n",
    "### **What is the Frobenius Norm?**  \n",
    "The **Frobenius norm** measures the size of a matrix and is defined as:  \n",
    "\n",
    "$$\n",
    "\\|A\\|_F = \\sqrt{\\sum_{i,j} |a_{ij}|^2}\n",
    "$$\n",
    "\n",
    "It is **equivalent to the L2 norm** for matrices.\n",
    "\n",
    "### **Example: Compute Frobenius Norm**  \n",
    "\n",
    "‚úÖ **Python Code**  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frobenius Norm of A: 5.477225575051661\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "# Define a matrix\n",
    "A = np.array([[1, 2], [3, 4]])\n",
    "\n",
    "# Compute Frobenius Norm\n",
    "frobenius_norm = np.linalg.norm(A, 'fro')\n",
    "\n",
    "print(\"Frobenius Norm of A:\", frobenius_norm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "‚úÖ **Applications:**  \n",
    "- **Regularization in machine learning** (e.g., weight decay in deep learning).  \n",
    "- Used in **low-rank approximation** (important in SVD).  \n",
    "- Measures **stability of transformations** in neural networks.  \n",
    "\n",
    "---\n",
    "\n",
    "# **üîπ Summary of Advanced Topics**  \n",
    "\n",
    "| Concept | Definition | Applications |\n",
    "|---------|------------|--------------|\n",
    "| **Cramer's Rule** | Solves linear systems using determinants | Solving small systems |\n",
    "| **Rank-Nullity Theorem** | Rank + Nullity = # columns | Dimensionality reduction, Kernel methods |\n",
    "| **Moore-Penrose Pseudoinverse** | Generalized inverse for non-square matrices | Regression, Deep Learning, SVD |\n",
    "| **Frobenius Norm** | Matrix L2 norm | Regularization, Stability in ML |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **üîπ Additional Topics to Explore**  \n",
    "\n",
    "### **Jordan Form & Generalized Eigenvectors**  \n",
    "- **Jordan Canonical Form (JCF)** is a generalization of diagonalization.  \n",
    "- Used when a matrix is not **diagonalizable** (e.g., it has defective eigenvalues).  \n",
    "- Helps in **dynamical systems analysis** and **solving differential equations**.  \n",
    "\n",
    "‚úÖ **Applications:**  \n",
    "- Used in **Markov Chains** for analyzing long-term probabilities.  \n",
    "- Important for **control systems and numerical analysis**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Condition Number & Numerical Stability**  \n",
    "- The **condition number** of a matrix measures how sensitive it is to **small changes in input**.  \n",
    "- Defined as:  \n",
    "  $$\n",
    "  \\kappa(A) = \\frac{\\|A\\| \\cdot \\|A^{-1}\\|}{\\text{min eigenvalue} / \\text{max eigenvalue}}\n",
    "  $$\n",
    "- If **$\\kappa(A)$ is large**, small errors in input can lead to **huge errors in output** ‚Üí **unstable system**.  \n",
    "\n",
    "‚úÖ **Applications:**  \n",
    "- Used in **gradient-based optimization algorithms** (SGD, Adam).  \n",
    "- Important in **deep learning** to ensure numerical stability.  \n",
    "\n",
    "‚úÖ **Python Code:**  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Condition Number of A: 14.933034373659263\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "A = np.array([[1, 2], [3, 4]])\n",
    "condition_number = np.linalg.cond(A)\n",
    "\n",
    "print(\"Condition Number of A:\", condition_number)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **Cholesky Decomposition (Efficient Matrix Factorization)**  \n",
    "- Used for **fast inversion** of symmetric, positive-definite matrices.  \n",
    "- Decomposes $ A $ into $ LL^T $ (lower-triangular matrix and its transpose).  \n",
    "\n",
    "‚úÖ **Applications:**  \n",
    "- Used in **Gaussian Processes** and **Kalman Filters**.  \n",
    "- Improves computational efficiency in **machine learning models**.  \n",
    "\n",
    "‚úÖ **Python Code:**  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cholesky Factor L:\n",
      " [[2.         0.        ]\n",
      " [1.         1.41421356]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "A = np.array([[4, 2], [2, 3]])\n",
    "L = np.linalg.cholesky(A)\n",
    "\n",
    "print(\"Cholesky Factor L:\\n\", L)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **Gram-Schmidt Process & QR Factorization**  \n",
    "- Converts a set of vectors into an **orthonormal basis**.  \n",
    "- QR Factorization **decomposes** a matrix $ A $ into:  \n",
    "  $$\n",
    "  A = QR\n",
    "  $$\n",
    "  where:\n",
    "  - $ Q $ is an **orthonormal matrix**.\n",
    "  - $ R $ is an **upper triangular matrix**.  \n",
    "\n",
    "‚úÖ **Applications:**  \n",
    "- Used in **Principal Component Analysis (PCA)**.  \n",
    "- Important in **dimensionality reduction** and **stability in machine learning**.  \n",
    "\n",
    "‚úÖ **Python Code:** \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q Matrix:\n",
      " [[-0.57735027  0.15430335]\n",
      " [-0.57735027 -0.77151675]\n",
      " [-0.57735027  0.6172134 ]]\n",
      "R Matrix:\n",
      " [[-1.73205081 -1.15470054]\n",
      " [ 0.          2.1602469 ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "A = np.array([[1, 1], [1, -1], [1, 2]])\n",
    "Q, R = np.linalg.qr(A)\n",
    "\n",
    "print(\"Q Matrix:\\n\", Q)\n",
    "print(\"R Matrix:\\n\", R)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **Tensor Algebra (Extending Matrices to Higher Dimensions)**  \n",
    "- **Tensors** are multi-dimensional generalizations of matrices.  \n",
    "- Represented as $ \\mathbb{R}^{m \\times n \\times p} $.  \n",
    "- Used in **Deep Learning (TensorFlow, PyTorch)**.  \n",
    "\n",
    "‚úÖ **Applications:**  \n",
    "- Used in **convolutional neural networks (CNNs)** for image processing.  \n",
    "- Essential for **transformer models** (e.g., GPT, BERT).  \n",
    "\n",
    "‚úÖ **Python Code:**  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3D Tensor:\n",
      " tensor([[[1, 2],\n",
      "         [3, 4]],\n",
      "\n",
      "        [[5, 6],\n",
      "         [7, 8]]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "\n",
    "tensor = torch.tensor([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\n",
    "print(\"3D Tensor:\\n\", tensor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "## **üîπ Summary of Missing Topics**  \n",
    "\n",
    "| Concept | Definition | Applications |\n",
    "|---------|------------|--------------|\n",
    "| **Jordan Form** | Generalized diagonalization | Markov Chains, Control Systems |\n",
    "| **Condition Number** | Measures numerical stability | Optimization, Neural Networks |\n",
    "| **Cholesky Decomposition** | Efficient matrix factorization | Gaussian Processes, Fast Inversion |\n",
    "| **Gram-Schmidt & QR** | Converts basis to orthonormal | PCA, Stability in ML |\n",
    "| **Tensor Algebra** | Extends matrices to higher dimensions | Deep Learning, Computer Vision |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
